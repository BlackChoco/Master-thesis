import torch
from torch.utils.data import DataLoader
import numpy as np
import os
import pickle
import pandas as pd
from collections import defaultdict
from tqdm import tqdm
import matplotlib.pyplot as plt
import io
import warnings
from typing import List, Dict, Optional
from Tree_data_model import PostStorage
from peft import get_peft_model, LoraConfig

# å¯¼å…¥è§£è€¦åçš„æ¨¡å—
from cl_base_model import ContrastiveEncoder, load_model_from_modelscope, load_tokenizer_from_modelscope
from cl_dataset import ContrastiveDataset1, ContrastiveDataset2, ContrastiveDataCollator, build_vocab_from_post_storage, preprocess_text
from cl_loss import ContrastiveLoss
from cl_utils import build_pruned_forest

warnings.filterwarnings("ignore", category=UserWarning, message=r"Glyph .* missing from font\(s\) Arial\.")

def load_trained_model_and_tokenizer(checkpoint_path: str):
    """
    ä»checkpointåŠ è½½å®Œæ•´çš„è®­ç»ƒå™¨çŠ¶æ€ï¼Œå¹¶è¿”å›è®­ç»ƒå¥½çš„åŸºç¡€æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚
    """
    if not os.path.exists(checkpoint_path):
        print(f"é”™è¯¯: Checkpoint æ–‡ä»¶ {checkpoint_path} æœªæ‰¾åˆ°ã€‚")
        return None, None, None

    print(f"æ­£åœ¨ä» {checkpoint_path} åŠ è½½checkpoint...")
    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'),weights_only=False) # åŠ è½½åˆ°CPUä»¥é¿å…GPUé—®é¢˜

    model_type = checkpoint['training_model_type']
    model_identifier = checkpoint['training_model_identifier_or_path']
    proj_config = checkpoint['projection_head_config']

    # --- æ–°å¢ï¼šè·å–PEFTé…ç½® ---
    use_peft = checkpoint.get('use_peft', False)
    peft_config = checkpoint.get('peft_config', None)
    # -------------------------
    
    encoder = None
    if model_type == 'textcnn':
        textcnn_conf = checkpoint['textcnn_config']
        vocab_data = checkpoint['vocab']
        encoder = ContrastiveEncoder(
            model_type='textcnn',
            vocab=vocab_data,
            textcnn_config=textcnn_conf,
            projection_hidden_dim=proj_config['hidden_dim'],
            projection_output_dim=proj_config['output_dim'],
            projection_dropout_rate=proj_config['dropout_rate']
        )
    elif model_type == 'modelscope':
        encoder = ContrastiveEncoder(
            model_type='modelscope',
            model_name_or_path=model_identifier,
            projection_hidden_dim=proj_config['hidden_dim'],
            projection_output_dim=proj_config['output_dim'],
            projection_dropout_rate=proj_config['dropout_rate']
        )
        # --- æ–°å¢ï¼šå¦‚æœä½¿ç”¨äº†PEFTï¼Œé‡æ–°åŒ…è£…æ¨¡å‹ ---
        if use_peft:
            print("ğŸ”§ æ£€æµ‹åˆ°PEFTè®­ç»ƒçš„checkpointï¼Œæ­£åœ¨é‡æ–°åº”ç”¨LoRAé…ç½®...")
            lora_config = LoraConfig(**peft_config)
            encoder.base_model = get_peft_model(encoder.base_model, lora_config)
            print("âœ… LoRAé…ç½®å·²é‡æ–°åº”ç”¨ã€‚")
        # ------------------------------------
    else:
        print(f"é”™è¯¯: Checkpointä¸­æœªçŸ¥çš„æ¨¡å‹ç±»å‹ '{model_type}'")
        return None, None, None

    encoder.load_state_dict(checkpoint['contrastive_encoder_state_dict'])
    encoder.eval() # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    print(f"âœ… {model_type.upper()} ContrastiveEncoder åŠ è½½å®Œæˆå¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ã€‚")
    
    # è¿”å›åŸºç¡€æ¨¡å‹å’Œåˆ†è¯å™¨
    return encoder.base_model, encoder.tokenizer, model_type


class DynamicContrastiveTrainer:
    def __init__(self, post_storage: PostStorage,
                 training_model_type: str = 'modelscope',
                 training_model_identifier_or_path: Optional[str] = "google-bert/bert-base-chinese",
                 textcnn_config: Optional[Dict] = None,
                 projection_head_config: Optional[Dict] = None,
                 use_peft: bool = False,  # <--- æ–°å¢å‚æ•°ï¼šæ˜¯å¦ä½¿ç”¨PEFT
                 peft_config: Optional[Dict] = None, # <--- æ–°å¢å‚æ•°ï¼šPEFTé…ç½®
                 pruning_model_path: str = "google-bert/bert-base-chinese",
                 similarity_threshold: float = 0.5,
                 num_negatives: int = 2,
                 batch_size: int = 32,
                 pruning_inference_batch_size: int = 32,
                 base_lr: float = 1e-6,
                 projection_lr: float = 1e-4,
                 use_weighted_loss: bool = False,
                 loss_weights: Optional[Dict[str, float]] = None,
                 adaptive_weighting: bool = False,
                 infonce_mode: str = 'unidirectional',
                 min_subtree_size_ds1: int = 2,
                 max_samples_per_post_ds1: Optional[int] = None,
                 min_subtree_size_ds2: int = 4,
                 max_samples_per_subtree_ds2: Optional[int] = None):

        self.post_storage = post_storage
        self.training_model_type = training_model_type.lower()
        self.training_model_identifier_or_path = training_model_identifier_or_path
        self.textcnn_config = textcnn_config
        self.use_peft = use_peft
        self.peft_config = peft_config
        self.pruning_model_path = pruning_model_path
        self.similarity_threshold = similarity_threshold
        self.num_negatives = num_negatives if infonce_mode != 'in_batch' else 0
        self.batch_size = batch_size
        self.pruning_inference_batch_size = pruning_inference_batch_size
        self.infonce_mode = infonce_mode

        self.min_subtree_size_ds1 = min_subtree_size_ds1
        self.max_samples_per_post_ds1 = max_samples_per_post_ds1
        self.min_subtree_size_ds2 = min_subtree_size_ds2
        self.max_samples_per_subtree_ds2 = max_samples_per_subtree_ds2

        self.use_weighted_loss = use_weighted_loss
        if self.use_weighted_loss:
            if loss_weights is None: self.loss_weights = {'dataset1': 0.5, 'dataset2': 0.5}
            else: self.loss_weights = loss_weights.copy()
            weight_sum = sum(self.loss_weights.values())
            if weight_sum > 0:
                for k_lw in self.loss_weights: self.loss_weights[k_lw] /= weight_sum
            else:
                self.loss_weights = {'dataset1': 0.5, 'dataset2': 0.5}
            self.adaptive_weighting = adaptive_weighting
            self.initial_loss_weights = self.loss_weights.copy()
            print(f"ğŸ¯ å¯ç”¨æŸå¤±åŠ æƒ: {self.loss_weights}, è‡ªé€‚åº”: {self.adaptive_weighting}")
        else:
            self.loss_weights = {'dataset1': 1.0, 'dataset2': 1.0}
            self.adaptive_weighting = False
            print("ğŸ“Š ä½¿ç”¨ç‹¬ç«‹æŸå¤±ï¼ˆæ¯ä¸ªæ•°æ®é›†çš„æŸå¤±ä¹˜ä»¥å…¶æƒé‡ï¼Œé»˜è®¤ä¸º1.0ï¼‰")

        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        _default_proj_config = {'hidden_dim': 512, 'output_dim': 256, 'dropout_rate': 0.1}
        self.projection_config = {**_default_proj_config, **(projection_head_config if projection_head_config is not None else {})}


        self.vocab = None
        if self.training_model_type == 'textcnn':
            print("ğŸ› ï¸ ä½¿ç”¨ TextCNN æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚")
            if self.textcnn_config is None:
                raise ValueError("å½“ training_model_type ä¸º 'textcnn' æ—¶ï¼Œtextcnn_config æ˜¯å¿…éœ€çš„")
            print("ğŸ—ï¸ ä» PostStorage ä¸º TextCNN æ„å»ºè¯æ±‡è¡¨...")
            self.vocab, _ = build_vocab_from_post_storage(self.post_storage, min_freq=self.textcnn_config.get('min_vocab_freq', 1))
            self.contrastive_encoder = ContrastiveEncoder(
                model_type='textcnn',
                vocab=self.vocab,
                textcnn_config=self.textcnn_config,
                projection_hidden_dim=self.projection_config['hidden_dim'],
                projection_output_dim=self.projection_config['output_dim'],
                projection_dropout_rate=self.projection_config['dropout_rate']
            ).to(self.device)
            print(f"   TextCNN æ¨¡å‹åç§° (æ ‡è¯†ç¬¦): {self.training_model_identifier_or_path}")
        elif self.training_model_type == 'modelscope':
            print(f"ğŸ› ï¸ ä½¿ç”¨ ModelScope æ¨¡å‹è¿›è¡Œè®­ç»ƒ: {self.training_model_identifier_or_path}")
            if self.training_model_identifier_or_path is None:
                 raise ValueError("å¯¹äº 'modelscope' æ¨¡å‹ç±»å‹ï¼Œtraining_model_identifier_or_path æ˜¯å¿…éœ€çš„ã€‚")
            self.contrastive_encoder = ContrastiveEncoder(
                model_type='modelscope',
                model_name_or_path=self.training_model_identifier_or_path,
                projection_hidden_dim=self.projection_config['hidden_dim'],
                projection_output_dim=self.projection_config['output_dim'],
                projection_dropout_rate=self.projection_config['dropout_rate']
            ).to(self.device)
             # --- æ–°å¢ï¼šåº”ç”¨PEFT/LoRAçš„é€»è¾‘ ---
            if self.use_peft:
                print("ğŸš€ åº”ç”¨PEFT (LoRA)åˆ°åŸºç¡€æ¨¡å‹...")
                
                # å®šä¹‰é»˜è®¤LoRAé…ç½®ï¼Œå¹¶ä¸ç”¨æˆ·ä¼ å…¥çš„é…ç½®åˆå¹¶
                default_lora_config = {
                    'r': 16,
                    'lora_alpha': 32,
                    'target_modules': ["query", "key", "value"],
                    'lora_dropout': 0.05,
                    'bias': "none",
                }
                final_lora_config_dict = {**default_lora_config, **(self.peft_config or {})}
                
                lora_config = LoraConfig(**final_lora_config_dict)
                
                # å°†åŸºç¡€æ¨¡å‹åŒ…è£…æˆPeftModel
                self.contrastive_encoder.base_model = get_peft_model(self.contrastive_encoder.base_model, lora_config)
                
                print("âœ… LoRAåº”ç”¨å®Œæˆã€‚å¯è®­ç»ƒå‚æ•°è¯¦æƒ…:")
                self.contrastive_encoder.base_model.print_trainable_parameters()
            # --- PEFTé€»è¾‘ç»“æŸ ---
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è®­ç»ƒæ¨¡å‹ç±»å‹: {self.training_model_type}ã€‚è¯·é€‰æ‹© 'modelscope' æˆ– 'textcnn'ã€‚")

        print(f"ğŸ” åˆå§‹åŒ–å‰ªææ¨¡å‹: {self.pruning_model_path}")
        # ä½¿ç”¨ä¸ºModelScopeå®šä¹‰çš„è¾…åŠ©å‡½æ•°åŠ è½½å‰ªææ¨¡å‹
        self.pruning_tokenizer = load_tokenizer_from_modelscope(self.pruning_model_path)
        self.pruning_model = load_model_from_modelscope(
            self.pruning_model_path,
            torch_dtype=torch.float32
        )
        self.pruning_model_on_gpu = False

        self.optimizer = torch.optim.AdamW([
            {'params': self.contrastive_encoder.base_model.parameters(), 'lr': base_lr, 'weight_decay': 1e-6, 'initial_lr': base_lr},
            {'params': self.contrastive_encoder.projection_head.parameters(), 'lr': projection_lr, 'weight_decay': 1e-4, 'initial_lr': projection_lr}
        ])

        self.loss_fn1 = ContrastiveLoss(temperature=0.07, loss_type='infonce', infonce_mode=infonce_mode)
        self.loss_fn2 = ContrastiveLoss(temperature=0.05, loss_type='infonce', infonce_mode=infonce_mode)

        self.training_history = defaultdict(list)
        self.training_history['dataset_sizes'] = {'dataset1': [], 'dataset2': []}
        if self.use_weighted_loss:
            self.training_history['weight_ds1'] = []
            self.training_history['weight_ds2'] = []
        print(f"ğŸš€ DynamicContrastiveTrainer åˆå§‹åŒ–å®Œæˆã€‚è®¾å¤‡: {self.device}")

    def _load_pruning_model_to_gpu(self):
        if not self.pruning_model_on_gpu and self.device.type == 'cuda':
            print("ğŸ”§ å°†å‰ªææ¨¡å‹åŠ è½½åˆ°GPU...")
            self.pruning_model.to(self.device)
            self.pruning_model_on_gpu = True

    def _unload_pruning_model_from_gpu(self):
        if self.pruning_model_on_gpu and self.device.type == 'cuda':
            print("ğŸ”§ ä»GPUå¸è½½å‰ªææ¨¡å‹...")
            self.pruning_model.to('cpu')
            self.pruning_model_on_gpu = False
            torch.cuda.empty_cache()

    def _get_pruning_embeddings(self, texts: List[str]) -> np.ndarray:
        if not texts:
            return np.array([])
        # self._load_pruning_model_to_gpu() # ç”±è°ƒç”¨è€…ç®¡ç†GPUåŠ è½½/å¸è½½
        texts = [str(t) if not isinstance(t, str) else t for t in texts]
        texts = [t if t else "<empty_text_placeholder>" for t in texts] # æ›¿æ¢ç©ºå­—ç¬¦ä¸²

        all_embeddings_list = []
        self.pruning_model.eval()
        with torch.no_grad():
            # ä½¿ç”¨tqdmåŒ…è£¹æ‰¹å¤„ç†å¾ªç¯
            for i in tqdm(range(0, len(texts), self.pruning_inference_batch_size), desc="è®¡ç®—å‰ªæåµŒå…¥", unit="batch"):
                batch_texts = texts[i:i + self.pruning_inference_batch_size]
                if not batch_texts: continue
                try:
                    inputs = self.pruning_tokenizer(
                        batch_texts, padding=True, truncation=True, return_tensors='pt', max_length=512
                    ).to(self.pruning_model.device)
                    outputs = self.pruning_model(**inputs)
                    batch_embeddings = outputs.last_hidden_state.mean(dim=1)
                    if batch_embeddings.dtype in [torch.bfloat16, torch.float16]:
                        batch_embeddings = batch_embeddings.float()
                    all_embeddings_list.append(batch_embeddings.cpu())
                except Exception as e:
                    print(f"è·å–å‰ªæåµŒå…¥æ—¶æ‰¹å¤„ç†é”™è¯¯: {e}. è·³è¿‡æ­¤æ‰¹æ¬¡ã€‚æ‰¹æ¬¡æ–‡æœ¬ç¤ºä¾‹: {batch_texts[0][:50] if batch_texts else 'N/A'}")
                    # æ ¹æ®éœ€è¦ï¼Œå¯ä»¥ä¸ºå¤±è´¥çš„æ‰¹æ¬¡æ·»åŠ å ä½ç¬¦åµŒå…¥ï¼Œæˆ–è€…ç®€å•åœ°è·³è¿‡
                    # ä¾‹å¦‚ï¼Œæ·»åŠ é›¶åµŒå…¥ï¼š
                    # output_dim = self.pruning_model.config.hidden_size # å‡è®¾HFæ¨¡å‹
                    # all_embeddings_list.append(torch.zeros((len(batch_texts), output_dim), dtype=torch.float32).cpu())


        if not all_embeddings_list: return np.array([])
        try:
            all_embeddings_np = torch.cat(all_embeddings_list, dim=0).numpy()
        except RuntimeError as e:
            print(f"è¿æ¥å‰ªæåµŒå…¥æ—¶å‡ºé”™: {e}")
            # å°è¯•æ‰¾å‡ºå“ªä¸ªåµŒå…¥å¯¼è‡´é—®é¢˜
            # for i, emb_tensor in enumerate(all_embeddings_list):
            # print(f"Tensor {i} shape: {emb_tensor.shape}, dtype: {emb_tensor.dtype}")
            return np.array([]) # è¿”å›ç©ºæ•°ç»„ä»¥é¿å…è¿›ä¸€æ­¥é”™è¯¯
        return all_embeddings_np

    def _build_and_log_datasets(self):
        print("ğŸ› ï¸ æ„å»º/é‡å»ºæ•°æ®é›†...")
        print("   æ”¶é›†æ‰€æœ‰è¯„è®ºç”¨äºå‰ªææ¨¡å‹åµŒå…¥...")
        all_comment_texts = []
        comment_nodes_references = []

        for post_id, post_container in self.post_storage.posts.items():
            # Path A: å°è¯•ä» 'comments' å­—å…¸è·å– (å¦‚æœå­˜åœ¨ä¸”è¢«å¡«å……)
            if hasattr(post_container, 'comments') and isinstance(post_container.comments, dict) and post_container.comments:
                # print(f"DEBUG: Post {post_id} - ä½¿ç”¨ 'comments' å­—å…¸è·¯å¾„æ‰¾åˆ° {len(post_container.comments)} æ¡è¯„è®ºã€‚")
                for comment_id, comment_node in post_container.comments.items():
                    content_str = str(comment_node.content) if comment_node.content is not None else ""
                    all_comment_texts.append(content_str)
                    comment_nodes_references.append(comment_node)
            # Path B: å°è¯•ä» 'root' å±æ€§éå† (ä¸ build_vocab_from_post_storage ä¸€è‡´)
            elif hasattr(post_container, 'root') and post_container.root:
                # print(f"DEBUG: Post {post_id} - ä½¿ç”¨ 'root' å±æ€§è·¯å¾„ã€‚")
                queue = [post_container.root] # ä½¿ç”¨ root
                visited_ids_in_post = set()
                while queue:
                    comment_node = queue.pop(0)
                    if comment_node.comment_id in visited_ids_in_post:
                        continue
                    visited_ids_in_post.add(comment_node.comment_id)
                    
                    content_str = str(comment_node.content) if comment_node.content is not None else ""
                    all_comment_texts.append(content_str)
                    comment_nodes_references.append(comment_node)
                    
                    if hasattr(comment_node, 'children') and comment_node.children: # ä½¿ç”¨ children
                        for child_node in comment_node.children: # ä½¿ç”¨ children
                            if child_node:
                                queue.append(child_node)
            # else:
                # print(f"DEBUG: Post {post_id} - æœªæ‰¾åˆ° 'comments' å­—å…¸æˆ– 'root' å±æ€§ã€‚")

        if all_comment_texts:
            # --- æ–°å¢ï¼šåœ¨è®¡ç®—å‰ªæåµŒå…¥å‰ï¼Œå¯¹æ–‡æœ¬è¿›è¡Œé¢„å¤„ç† ---
            print(f"   å¯¹ {len(all_comment_texts)} æ¡è¯„è®ºè¿›è¡Œé¢„å¤„ç†ä»¥ç”¨äºå‰ªæ...")
            preprocessed_texts_for_pruning = [preprocess_text(text) for text in all_comment_texts]

            print(f"   è®¡ç®— {len(preprocessed_texts_for_pruning)} æ¡é¢„å¤„ç†åè¯„è®ºçš„å‰ªæåµŒå…¥...")
            self._load_pruning_model_to_gpu()
            pruning_embeddings_np = self._get_pruning_embeddings(preprocessed_texts_for_pruning)
            self._unload_pruning_model_from_gpu()

            if pruning_embeddings_np.ndim == 2 and pruning_embeddings_np.shape[0] == len(comment_nodes_references):
                for i, comment_node in enumerate(comment_nodes_references):
                    if hasattr(comment_node, 'set_embedding'):
                        comment_node.set_embedding(pruning_embeddings_np[i])
                    else: # åå¤‡æ–¹æ¡ˆ
                        comment_node.embedding = pruning_embeddings_np[i]
                print("   å‰ªæåµŒå…¥å·²ä¸ºPostStorageä¸­çš„æ‰€æœ‰è¯„è®ºè®¾ç½®ã€‚")
            else:
                print(f"   è­¦å‘Š: _get_pruning_embeddings çš„å½¢çŠ¶ä¸åŒ¹é…æˆ–ç»“æœä¸ºç©ºã€‚é¢„æœŸ ({len(comment_nodes_references)}, dim)ï¼Œå¾—åˆ° {pruning_embeddings_np.shape if isinstance(pruning_embeddings_np, np.ndarray) else 'Not an ndarray'}ã€‚è·³è¿‡å‰ªæåµŒå…¥æ›´æ–°ã€‚")
        else:
            print("   åœ¨PostStorageä¸­æœªæ‰¾åˆ°è¯„è®ºè¿›è¡Œå‰ªæåµŒå…¥ã€‚")

        build_pruned_forest(self.post_storage, self.similarity_threshold)

        print("   åˆ›å»º Dataset1...")
        self.dataset1 = ContrastiveDataset1(
            post_storage=self.post_storage,
            similarity_threshold=self.similarity_threshold,
            min_subtree_size=self.min_subtree_size_ds1,
            max_samples_per_post=self.max_samples_per_post_ds1
        )
        print("   åˆ›å»º Dataset2...")
        self.dataset2 = ContrastiveDataset2(
            post_storage=self.post_storage,
            min_subtree_size=self.min_subtree_size_ds2,
            max_samples_per_subtree=self.max_samples_per_subtree_ds2
        )

        ds1_size = len(self.dataset1)
        ds2_size = len(self.dataset2)
        self.training_history['dataset_sizes']['dataset1'].append(ds1_size)
        self.training_history['dataset_sizes']['dataset2'].append(ds2_size)
        print(f"   Dataset1 å¤§å°: {ds1_size}, Dataset2 å¤§å°: {ds2_size}")

        if ds1_size == 0 and ds2_size == 0:
            print("âš ï¸ è­¦å‘Š: ä¸¤ä¸ªæ•°æ®é›†éƒ½ä¸ºç©ºã€‚è®­ç»ƒå¯èƒ½æ— æ³•æœ‰æ•ˆè¿›è¡Œã€‚")

        collator_num_neg = self.num_negatives
        self.collator1 = ContrastiveDataCollator(self.dataset1, num_negatives=collator_num_neg)
        self.collator2 = ContrastiveDataCollator(self.dataset2, num_negatives=collator_num_neg)

        self.train_loader1 = DataLoader(self.dataset1, batch_size=self.batch_size, shuffle=True, collate_fn=self.collator1, num_workers=0, pin_memory=True if self.device.type == 'cuda' else False) if ds1_size > 0 else None
        self.train_loader2 = DataLoader(self.dataset2, batch_size=self.batch_size, shuffle=True, collate_fn=self.collator2, num_workers=0, pin_memory=True if self.device.type == 'cuda' else False) if ds2_size > 0 else None
        print("âœ… æ•°æ®é›†å’Œ DataLoader å·²å‡†å¤‡å°±ç»ªã€‚")

        # --- æ–°å¢ä»£ç ï¼šä¿å­˜æ„å»ºå¥½çš„æ•°æ®é›† ---
        print("ğŸ’¾ æ­£åœ¨ä¿å­˜æ„å»ºå¥½çš„æ•°æ®é›†...")
        save_dir = "cl_dataset"
        os.makedirs(save_dir, exist_ok=True)

        # åˆ›å»ºåŠ¨æ€æ–‡ä»¶å
        sanitized_model_name = self.training_model_identifier_or_path.replace('/', '_')
        similarity_str = str(self.similarity_threshold)
        base_filename = f"{sanitized_model_name}_sim_{similarity_str}"

        # ä¿å­˜ Dataset1
        if ds1_size > 0:
            ds1_filepath = os.path.join(save_dir, f"{base_filename}_dataset1.pkl")
            try:
                with open(ds1_filepath, 'wb') as f:
                    pickle.dump(self.dataset1, f)
                print(f"   -> Dataset1 å·²ä¿å­˜è‡³: {ds1_filepath}")
            except Exception as e:
                print(f"   -> âŒ ä¿å­˜ Dataset1 å¤±è´¥: {e}")
        else:
            print("   -> Dataset1 ä¸ºç©ºï¼Œä¸è¿›è¡Œä¿å­˜ã€‚")

        # ä¿å­˜ Dataset2
        if ds2_size > 0:
            ds2_filepath = os.path.join(save_dir, f"{base_filename}_dataset2.pkl")
            try:
                with open(ds2_filepath, 'wb') as f:
                    pickle.dump(self.dataset2, f)
                print(f"   -> Dataset2 å·²ä¿å­˜è‡³: {ds2_filepath}")
            except Exception as e:
                print(f"   -> âŒ ä¿å­˜ Dataset2 å¤±è´¥: {e}")
        else:
            print("   -> Dataset2 ä¸ºç©ºï¼Œä¸è¿›è¡Œä¿å­˜ã€‚")

    def _process_batch(self, batch: Dict, loss_fn: ContrastiveLoss, dataset_name: str) -> Optional[torch.Tensor]:
        self.optimizer.zero_grad()

        anchor_texts = batch['anchor_texts']
        positive_texts_ds1 = batch.get('positive_texts_ds1', [])
        positive_content_lists_ds2 = batch.get('positive_content_lists_ds2', [])
        negative_texts_flat = batch.get('negative_texts', []) # æ‰å¹³åˆ—è¡¨
        num_negatives_per_anchor = batch.get('num_negatives', 0)

        if not anchor_texts: return None
        loss = None
        processed_anchors_count = 0 # å½“å‰æ‰¹æ¬¡ä¸­å®é™…å¤„ç†çš„é”šç‚¹æ•°é‡

        if dataset_name == 'dataset1':
            valid_indices = [i for i, txt in enumerate(positive_texts_ds1) if txt is not None]
            if not valid_indices: return None

            current_anchor_texts = [anchor_texts[i] for i in valid_indices]
            current_positive_texts = [positive_texts_ds1[i] for i in valid_indices]
            processed_anchors_count = len(current_anchor_texts)
            if processed_anchors_count == 0: return None


            anchor_emb = self.contrastive_encoder(current_anchor_texts)
            positive_emb = self.contrastive_encoder(current_positive_texts)

            neg_emb_reshaped = None
            if self.infonce_mode != 'in_batch' and num_negatives_per_anchor > 0 and negative_texts_flat:
                # ä»æ‰å¹³åˆ—è¡¨ä¸­æå–ä¸ valid_indices å¯¹åº”çš„è´Ÿæ ·æœ¬
                current_negative_texts_for_batch = []
                for original_batch_idx in valid_indices:
                    start_idx_flat = original_batch_idx * num_negatives_per_anchor
                    end_idx_flat = start_idx_flat + num_negatives_per_anchor
                    current_negative_texts_for_batch.extend(negative_texts_flat[start_idx_flat:end_idx_flat])

                if current_negative_texts_for_batch:
                    neg_emb_flat = self.contrastive_encoder(current_negative_texts_for_batch)
                    if neg_emb_flat.nelement() > 0:
                        try:
                            neg_emb_reshaped = neg_emb_flat.view(processed_anchors_count, num_negatives_per_anchor, -1)
                        except RuntimeError as e:
                            print(f"DS1 Reshape error: {e}. Anchors: {processed_anchors_count}, Neg per anchor: {num_negatives_per_anchor}, Flat neg shape: {neg_emb_flat.shape}")
                            return None # or handle differently

            if anchor_emb.nelement() > 0 and positive_emb.nelement() > 0:
                loss = loss_fn(anchor_emb, positive_emb, neg_emb_reshaped if neg_emb_reshaped is not None and neg_emb_reshaped.nelement() > 0 else None)

        elif dataset_name == 'dataset2':
            valid_indices = [i for i, lst in enumerate(positive_content_lists_ds2) if lst is not None and any(s and s.strip() for s in lst)]
            if not valid_indices: return None

            current_anchor_texts = [anchor_texts[i] for i in valid_indices]
            current_positive_lists = [positive_content_lists_ds2[i] for i in valid_indices]
            processed_anchors_count = len(current_anchor_texts)
            if processed_anchors_count == 0: return None

            anchor_emb = self.contrastive_encoder(current_anchor_texts)
            positive_emb_list_for_ds2 = []
            for text_list_for_one_anchor in current_positive_lists:
                valid_texts_in_list = [s for s in text_list_for_one_anchor if s and s.strip()]
                if valid_texts_in_list:
                    # è·å–åŸºç¡€åµŒå…¥ï¼Œç„¶åå¹³å‡ï¼Œç„¶åæŠ•å½±
                    node_base_embeddings = self.contrastive_encoder.get_base_embeddings(valid_texts_in_list)
                    if node_base_embeddings.nelement() > 0:
                        avg_node_base_embedding = node_base_embeddings.mean(dim=0, keepdim=True)
                        projected_avg_emb = self.contrastive_encoder.projection_head(avg_node_base_embedding)
                        positive_emb_list_for_ds2.append(projected_avg_emb)
                    else: # å¦‚æœåˆ—è¡¨ä¸­çš„æ‰€æœ‰æ–‡æœ¬éƒ½æ— æ•ˆ/ç©ºï¼Œåˆ™æ·»åŠ é›¶åµŒå…¥
                        proj_output_dim = self.contrastive_encoder.projection_head[-2].out_features
                        positive_emb_list_for_ds2.append(torch.zeros((1, proj_output_dim), device=self.device, dtype=anchor_emb.dtype))
                else: # å¦‚æœæ•´ä¸ªåˆ—è¡¨ä¸ºç©ºæˆ–ä»…åŒ…å«æ— æ•ˆå­—ç¬¦ä¸²
                    proj_output_dim = self.contrastive_encoder.projection_head[-2].out_features
                    positive_emb_list_for_ds2.append(torch.zeros((1, proj_output_dim), device=self.device, dtype=anchor_emb.dtype))

            if not positive_emb_list_for_ds2 or len(positive_emb_list_for_ds2) != processed_anchors_count:
                 print(f"DS2: positive_emb_list é•¿åº¦ ({len(positive_emb_list_for_ds2)}) ä¸é”šç‚¹æ•°é‡ ({processed_anchors_count}) ä¸åŒ¹é…ã€‚")
                 return None
            positive_emb = torch.cat(positive_emb_list_for_ds2, dim=0)

            neg_emb_reshaped = None
            if self.infonce_mode != 'in_batch' and num_negatives_per_anchor > 0 and negative_texts_flat:
                current_negative_texts_for_batch = []
                for original_batch_idx in valid_indices:
                    start_idx_flat = original_batch_idx * num_negatives_per_anchor
                    end_idx_flat = start_idx_flat + num_negatives_per_anchor
                    current_negative_texts_for_batch.extend(negative_texts_flat[start_idx_flat:end_idx_flat])

                if current_negative_texts_for_batch:
                    neg_emb_flat = self.contrastive_encoder(current_negative_texts_for_batch)
                    if neg_emb_flat.nelement() > 0:
                        try:
                            neg_emb_reshaped = neg_emb_flat.view(processed_anchors_count, num_negatives_per_anchor, -1)
                        except RuntimeError as e:
                            print(f"DS2 Reshape error: {e}. Anchors: {processed_anchors_count}, Neg per anchor: {num_negatives_per_anchor}, Flat neg shape: {neg_emb_flat.shape}")
                            return None


            if anchor_emb.nelement() > 0 and positive_emb.nelement() > 0:
                loss = loss_fn(anchor_emb, positive_emb, neg_emb_reshaped if neg_emb_reshaped is not None and neg_emb_reshaped.nelement() > 0 else None)

        if loss is not None and loss.requires_grad:
            effective_weight = self.loss_weights.get(dataset_name, 1.0) if self.use_weighted_loss else 1.0
            (loss * effective_weight).backward()
            torch.nn.utils.clip_grad_norm_(self.contrastive_encoder.parameters(), max_norm=1.0)
            self.optimizer.step()
            return loss # è¿”å›æœªç¼©æ”¾çš„æŸå¤±ç”¨äºè®°å½•
        return None

    def train(self, num_epochs: int, rebuild_frequency: int, scheduler_patience: int, min_improvement: float):
        self.contrastive_encoder.train()
        best_overall_loss = float('inf')
        patience_counter = 0

        # ç¡®ä¿ä¼˜åŒ–å™¨ä¸­çš„ initial_lr å·²è®¾ç½®
        for grp in self.optimizer.param_groups:
            if 'initial_lr' not in grp:
                grp['initial_lr'] = grp['lr']


        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', factor=0.5,
            patience=scheduler_patience, threshold=min_improvement
        )
        self._build_and_log_datasets() # åˆå§‹æ•°æ®é›†æ„å»º

        for epoch in range(num_epochs):
            print(f"\n--- Epoch {epoch+1}/{num_epochs} ---")
            if epoch > 0 and rebuild_frequency > 0 and epoch % rebuild_frequency == 0:
                print(f"Epoch {epoch+1}: é‡å»ºæ•°æ®é›†...")
                self._build_and_log_datasets()

            self.contrastive_encoder.to(self.device) # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            self.contrastive_encoder.train() # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼

            epoch_losses_ds1 = []
            epoch_losses_ds2 = []

            # Dataset 1 è®­ç»ƒå¾ªç¯
            if self.train_loader1 and len(self.train_loader1) > 0:
                print(f"åœ¨ Dataset1 ä¸Šè®­ç»ƒ (å¤§å°: {len(self.dataset1)} æ ·æœ¬, {len(self.train_loader1)} æ‰¹æ¬¡)")
                progress_bar_ds1 = tqdm(self.train_loader1, desc=f"Epoch {epoch+1} DS1", leave=False, dynamic_ncols=True)
                for batch1 in progress_bar_ds1:
                    loss1_val = self._process_batch(batch1, self.loss_fn1, 'dataset1')
                    if loss1_val is not None:
                        epoch_losses_ds1.append(loss1_val.item())
                        progress_bar_ds1.set_postfix(loss=f"{loss1_val.item():.4f}")
            else:
                print("Dataset1 ä¸ºç©ºæˆ–æœªåŠ è½½ï¼Œè·³è¿‡è®­ç»ƒã€‚")


            # Dataset 2 è®­ç»ƒå¾ªç¯
            if self.train_loader2 and len(self.train_loader2) > 0:
                print(f"åœ¨ Dataset2 ä¸Šè®­ç»ƒ (å¤§å°: {len(self.dataset2)} æ ·æœ¬, {len(self.train_loader2)} æ‰¹æ¬¡)")
                progress_bar_ds2 = tqdm(self.train_loader2, desc=f"Epoch {epoch+1} DS2", leave=False, dynamic_ncols=True)
                for batch2 in progress_bar_ds2:
                    loss2_val = self._process_batch(batch2, self.loss_fn2, 'dataset2')
                    if loss2_val is not None:
                        epoch_losses_ds2.append(loss2_val.item())
                        progress_bar_ds2.set_postfix(loss=f"{loss2_val.item():.4f}")
            else:
                print("Dataset2 ä¸ºç©ºæˆ–æœªåŠ è½½ï¼Œè·³è¿‡è®­ç»ƒã€‚")


            avg_loss_ds1 = np.mean(epoch_losses_ds1) if epoch_losses_ds1 else 0.0
            avg_loss_ds2 = np.mean(epoch_losses_ds2) if epoch_losses_ds2 else 0.0
            current_epoch_combined_loss = 0.0
            w1, w2 = 0.0, 0.0 # åˆå§‹åŒ–æƒé‡

            if self.use_weighted_loss:
                w1_config = self.loss_weights.get('dataset1', 0.5)
                w2_config = self.loss_weights.get('dataset2', 0.5)

                active_datasets = 0
                if epoch_losses_ds1: active_datasets +=1
                if epoch_losses_ds2: active_datasets +=1

                if active_datasets == 0:
                    current_epoch_combined_loss = 0.0
                    w1, w2 = 0.0, 0.0
                elif active_datasets == 1:
                    if epoch_losses_ds1:
                        current_epoch_combined_loss = avg_loss_ds1
                        w1, w2 = 1.0, 0.0
                    else: # epoch_losses_ds2 must be active
                        current_epoch_combined_loss = avg_loss_ds2
                        w1, w2 = 0.0, 1.0
                else: # Both active
                    # Normalize configured weights
                    sum_config_w = w1_config + w2_config
                    if sum_config_w > 0:
                        w1 = w1_config / sum_config_w
                        w2 = w2_config / sum_config_w
                    else:
                        w1, w2 = 0.5, 0.5
                    current_epoch_combined_loss = (w1 * avg_loss_ds1) + (w2 * avg_loss_ds2)
            else: # ä¸ä½¿ç”¨åŠ æƒæŸå¤±ï¼Œç®€å•å¹³å‡æˆ–å–å•ä¸ª
                if epoch_losses_ds1 and epoch_losses_ds2:
                    current_epoch_combined_loss = (avg_loss_ds1 + avg_loss_ds2) / 2.0
                elif epoch_losses_ds1:
                    current_epoch_combined_loss = avg_loss_ds1
                elif epoch_losses_ds2:
                    current_epoch_combined_loss = avg_loss_ds2
                else:
                    current_epoch_combined_loss = 0.0 # ä¸¤ä¸ªæ•°æ®é›†éƒ½æ²¡æœ‰æŸå¤±

            print(f"Epoch {epoch+1} æ€»ç»“: å¹³å‡ DS1 æŸå¤±: {avg_loss_ds1:.4f}, å¹³å‡ DS2 æŸå¤±: {avg_loss_ds2:.4f}, ç»„åˆæŸå¤± (ç”¨äºè°ƒåº¦å™¨): {current_epoch_combined_loss:.4f}")

            self.training_history['epoch'].append(epoch + 1)
            self.training_history['loss_ds1'].append(avg_loss_ds1)
            self.training_history['loss_ds2'].append(avg_loss_ds2)
            self.training_history['combined_loss'].append(current_epoch_combined_loss)
            if self.use_weighted_loss:
                self.training_history['weight_ds1'].append(w1)
                self.training_history['weight_ds2'].append(w2)

            if self.adaptive_weighting and epoch_losses_ds1 and epoch_losses_ds2 and self.use_weighted_loss:
                # ç®€å•çš„åŸºäºæŸå¤±çš„è‡ªé€‚åº”æƒé‡è°ƒæ•´ (æ›´é«˜çº§çš„æ–¹æ³•å­˜åœ¨)
                # æŸå¤±è¶Šå¤§ï¼Œæƒé‡åº”è¯¥è¶Šå¤§ (å‡è®¾ä»»åŠ¡æ˜¯æœ€å°åŒ–æŸå¤±)
                # æˆ–è€…ï¼Œå¦‚æœä»»åŠ¡æ˜¯æœ€å¤§åŒ–æŸä¸ªæŒ‡æ ‡ï¼Œåˆ™æŒ‡æ ‡è¶Šå°ï¼Œæƒé‡è¶Šå¤§
                # è¿™é‡Œæˆ‘ä»¬æœ€å°åŒ–æŸå¤±ï¼Œæ‰€ä»¥æŸå¤±å¤§çš„æ•°æ®é›†åº”è¯¥è·å¾—æ›´å¤šå…³æ³¨ï¼ˆå³æ›´é«˜çš„æƒé‡ï¼‰
                loss_sum_for_weighting = avg_loss_ds1 + avg_loss_ds2
                if loss_sum_for_weighting > 1e-9: # é¿å…é™¤ä»¥é›¶
                    # æƒé‡ä¸æŸå¤±æˆæ­£æ¯”
                    raw_w1_adaptive = avg_loss_ds1 / loss_sum_for_weighting if avg_loss_ds1 > 0 else 0
                    raw_w2_adaptive = avg_loss_ds2 / loss_sum_for_weighting if avg_loss_ds2 > 0 else 0

                    # å¹³æ»‘æ›´æ–°ï¼Œalpha_smooth å†³å®šæ–°è®¡ç®—çš„æƒé‡å å¤šå¤§æ¯”é‡
                    alpha_smooth = 0.3 # 0 è¡¨ç¤ºå®Œå…¨ä½¿ç”¨åˆå§‹æƒé‡, 1 è¡¨ç¤ºå®Œå…¨ä½¿ç”¨å½“å‰è®¡ç®—çš„æƒé‡
                    self.loss_weights['dataset1'] = (1 - alpha_smooth) * self.initial_loss_weights['dataset1'] + alpha_smooth * raw_w1_adaptive

                    self.loss_weights['dataset2'] = (1 - alpha_smooth) * self.initial_loss_weights['dataset2'] + alpha_smooth * raw_w2_adaptive

                    # é‡æ–°å½’ä¸€åŒ–
                    current_sum_adapted_weights = self.loss_weights['dataset1'] + self.loss_weights['dataset2']
                    if current_sum_adapted_weights > 0:
                        self.loss_weights['dataset1'] /= current_sum_adapted_weights
                        self.loss_weights['dataset2'] /= current_sum_adapted_weights
                    else: # å¦‚æœä¸¤ä¸ªæƒé‡éƒ½å˜ä¸ºé›¶ï¼Œåˆ™å›é€€
                        self.loss_weights['dataset1'], self.loss_weights['dataset2'] = self.initial_loss_weights['dataset1'], self.initial_loss_weights['dataset2']
                    print(f"è‡ªé€‚åº”æƒé‡æ›´æ–°: DS1={self.loss_weights['dataset1']:.3f}, DS2={self.loss_weights['dataset2']:.3f}")


            scheduler.step(current_epoch_combined_loss)
            current_lr_base = self.optimizer.param_groups[0]['lr']
            current_lr_proj = self.optimizer.param_groups[1]['lr']
            self.training_history['learning_rate_base'].append(current_lr_base)
            self.training_history['learning_rate_proj'].append(current_lr_proj)
            print(f"å½“å‰å­¦ä¹ ç‡: Base={current_lr_base:.2e}, Projection={current_lr_proj:.2e}")


            if current_epoch_combined_loss < best_overall_loss - min_improvement:
                if epoch_losses_ds1 or epoch_losses_ds2: # ä»…å½“å®é™…å‘ç”Ÿè®­ç»ƒæ—¶æ‰ä¿å­˜
                    best_overall_loss = current_epoch_combined_loss
                    patience_counter = 0
                    print(f"ğŸ‰ å‘ç°æ–°çš„æœ€ä½³æ¨¡å‹! ç»„åˆæŸå¤±: {best_overall_loss:.4f}. ä¿å­˜æ¨¡å‹...")
                    self.save_checkpoint(epoch + 1, best_overall_loss, is_best=True)
                else:
                    print("æ­¤è½®æœªæ‰§è¡Œè®­ç»ƒæ­¥éª¤ã€‚è·³è¿‡æœ€ä½³æ¨¡å‹æ£€æŸ¥ã€‚")
            else:
                patience_counter += 1
                print(f"è€å¿ƒè®¡æ•°: {patience_counter}/{scheduler.patience}")

            if patience_counter > scheduler.patience: # æ³¨æ„ï¼šscheduler.patience æ˜¯ ReduceLROnPlateau çš„å‚æ•°
                print("ğŸ›‘ æ—©åœè§¦å‘ã€‚")
                break
            self.plot_training_progress(save_plot=False, show_plot=False) # ç”Ÿæˆç»˜å›¾æ•°æ®ä»¥å¤‡ä¿å­˜

        print("ğŸ è®­ç»ƒå®Œæˆã€‚")
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼Œæ— è®ºæ˜¯å¦æœ€ä½³
        # self.save_checkpoint(epoch + 1, current_epoch_combined_loss, is_best=False, final_save=True)
        self.plot_training_progress(save_plot=True, show_plot=True) # ä¿å­˜å¹¶æ˜¾ç¤ºæœ€ç»ˆç»˜å›¾


    def save_checkpoint(self, epoch_num, loss_val, is_best=False, final_save=False):
        # åªåœ¨æ‰¾åˆ°æ›´ä¼˜æ¨¡å‹æ—¶æ‰æ‰§è¡Œä¿å­˜æ“ä½œ
        if not is_best:
            return

        state = {
            'epoch': epoch_num,
            'contrastive_encoder_state_dict': self.contrastive_encoder.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_loss': loss_val,
            'training_history': dict(self.training_history), # ä¿å­˜è®­ç»ƒå†å²çš„å‰¯æœ¬
            'training_model_type': self.training_model_type,
            'training_model_identifier_or_path': self.training_model_identifier_or_path,
            'projection_head_config': self.projection_config,
            'pruning_model_path': self.pruning_model_path,
            'similarity_threshold': self.similarity_threshold,
            'num_negatives': self.num_negatives,
            'batch_size': self.batch_size,
            'pruning_inference_batch_size': self.pruning_inference_batch_size,
            'base_lr_initial': self.optimizer.param_groups[0].get('initial_lr'),
            'projection_lr_initial': self.optimizer.param_groups[1].get('initial_lr'),
            'use_weighted_loss': self.use_weighted_loss,
            'loss_weights': self.loss_weights,
            'adaptive_weighting': self.adaptive_weighting,
            'infonce_mode': self.infonce_mode,
            # --- æ–°å¢PEFTçŠ¶æ€ ---
            'use_peft': self.use_peft,
            'peft_config': self.peft_config,
            # --------------------
            'min_subtree_size_ds1': self.min_subtree_size_ds1,
            'max_samples_per_post_ds1': self.max_samples_per_post_ds1,
            'min_subtree_size_ds2': self.min_subtree_size_ds2,
            'max_samples_per_subtree_ds2': self.max_samples_per_subtree_ds2
        }
        if self.training_model_type == 'textcnn':
            state['textcnn_config'] = self.textcnn_config
            state['vocab'] = self.vocab

        # æ ¹æ®æ¨¡å‹æ ‡è¯†ç¬¦åˆ›å»ºä¿å­˜ç›®å½•
        sanitized_model_name = self.training_model_identifier_or_path.replace('/', '_')
        save_dir = os.path.join("model", sanitized_model_name)
        os.makedirs(save_dir, exist_ok=True)

        # ç”ŸæˆæŸå¤±å›¾çš„PNGå­—èŠ‚
        fig_bytes = self.plot_training_progress(save_plot=False, show_plot=False, return_bytes=True)
        if fig_bytes:
            # å°†æŸå¤±å›¾å­—èŠ‚ä¿å­˜åˆ° state ä¸­ï¼ˆå¯é€‰ï¼Œä½†ä¿ç•™äº†åŸæœ‰é€»è¾‘ï¼‰
            state['loss_plot_png'] = fig_bytes
            
            # å°†æŸå¤±å›¾ä¿å­˜ä¸ºç‹¬ç«‹çš„PNGæ–‡ä»¶
            plot_filepath = os.path.join(save_dir, "training_loss_plot.png")
            with open(plot_filepath, 'wb') as f:
                f.write(fig_bytes)
            print(f"ğŸ“Š è®­ç»ƒæŸå¤±å›¾å·²ä¿å­˜è‡³: {plot_filepath}")

        # å®šä¹‰å¹¶ä¿å­˜æœ€ä¼˜æ¨¡å‹çš„checkpointæ–‡ä»¶
        filepath = os.path.join(save_dir, "best_contrastive_model.pth")
        torch.save(state, filepath)
        print(f"âœ… æœ€ä¼˜æ¨¡å‹å·²æ›´æ–°å¹¶ä¿å­˜è‡³: {filepath}")


    def plot_training_progress(self, save_plot=False, show_plot=True, return_bytes=False):
        if not self.training_history['epoch']:
            if return_bytes: return None
            return

        try:
            import matplotlib
            matplotlib.use('Agg') # ç¡®ä¿éäº¤äº’å¼åç«¯
            import matplotlib.pyplot as plt
            plt.style.use('seaborn-v0_8-whitegrid')
        except ImportError:
            print("Matplotlib æœªæ‰¾åˆ°ã€‚è·³è¿‡ç»˜å›¾ç”Ÿæˆã€‚")
            if return_bytes: return None
            return
        except UserWarning: pass # å¿½ç•¥seabornæ ·å¼è­¦å‘Š

        fig, ax1 = plt.subplots(figsize=(14, 8)) # è°ƒæ•´å›¾å½¢å¤§å°
        epochs_data = self.training_history['epoch']

        color_ds1_loss = 'orangered'
        color_ds2_loss = 'forestgreen'
        color_combined_loss = 'mediumblue'

        ax1.set_xlabel('Epoch', fontsize=12)
        ax1.set_ylabel('Loss', color='black', fontsize=12)

        if 'loss_ds1' in self.training_history and any(v is not None and v > 0 for v in self.training_history['loss_ds1']):
            ax1.plot(epochs_data, self.training_history['loss_ds1'], color=color_ds1_loss, linestyle='--', marker='.', markersize=6, label='DS1 Loss (çˆ¶å­)')
        if 'loss_ds2' in self.training_history and any(v is not None and v > 0 for v in self.training_history['loss_ds2']):
            ax1.plot(epochs_data, self.training_history['loss_ds2'], color=color_ds2_loss, linestyle=':', marker='x', markersize=6, label='DS2 Loss (èŠ‚ç‚¹-ä¸­å¿ƒ)')
        if 'combined_loss' in self.training_history and any(v is not None for v in self.training_history['combined_loss']):
            ax1.plot(epochs_data, self.training_history['combined_loss'], color=color_combined_loss, linewidth=2.5, marker='o', markersize=4, label='ç»„åˆæŸå¤± (è°ƒåº¦å™¨)')
        ax1.tick_params(axis='y', labelcolor='black', labelsize=10)
        ax1.tick_params(axis='x', labelsize=10)
        ax1.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)
        ax1.set_yscale('log') # å°è¯•å¯¹æ•°åˆ»åº¦ï¼Œå¦‚æœæŸå¤±èŒƒå›´å¾ˆå¤§

        ax2 = ax1.twinx()
        color_lr_base = 'purple'
        color_lr_proj = 'darkcyan'
        color_w1 = 'coral'
        color_w2 = 'lightgreen'

        ax2.set_ylabel('å­¦ä¹ ç‡ / æƒé‡', color='dimgray', fontsize=12)
        if 'learning_rate_base' in self.training_history and any(v is not None for v in self.training_history['learning_rate_base']):
            ax2.plot(epochs_data, self.training_history['learning_rate_base'], color=color_lr_base, linestyle='-.', marker='s', markersize=4, label='LR (Base)')
        if 'learning_rate_proj' in self.training_history and any(v is not None for v in self.training_history['learning_rate_proj']):
            ax2.plot(epochs_data, self.training_history['learning_rate_proj'], color=color_lr_proj, linestyle='-.', marker='D', markersize=3, label='LR (Proj)')

        if self.use_weighted_loss and 'weight_ds1' in self.training_history and 'weight_ds2' in self.training_history:
            if any(v is not None for v in self.training_history['weight_ds1']):
                 ax2.plot(epochs_data, self.training_history['weight_ds1'], color=color_w1, linestyle=(0, (3, 5, 1, 5)), marker='^', markersize=5, label='æƒé‡ DS1') # (0, (3, 5, 1, 5)) is dashdotdotted
            if any(v is not None for v in self.training_history['weight_ds2']):
                 ax2.plot(epochs_data, self.training_history['weight_ds2'], color=color_w2, linestyle=(0, (3, 5, 1, 5)), marker='v', markersize=5, label='æƒé‡ DS2')

        ax2.tick_params(axis='y', labelcolor='dimgray', labelsize=10)
        ax2.set_ylim(bottom=0) # å­¦ä¹ ç‡å’Œæƒé‡éè´Ÿ
        # ax2.set_yscale('log') # å¦‚æœå­¦ä¹ ç‡å˜åŒ–èŒƒå›´ä¹Ÿå¾ˆå¤§

        handles1, labels1 = ax1.get_legend_handles_labels()
        handles2, labels2 = ax2.get_legend_handles_labels()
        # å°†å›¾ä¾‹æ”¾åœ¨å›¾è¡¨ä¸‹æ–¹
        fig.legend(handles1 + handles2, labels1 + labels2, loc='lower center', ncol=4, bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=True, fontsize=9)

        model_name_for_title = self.training_model_identifier_or_path.split('/')[-1]
        plt.title(f'è®­ç»ƒè¿›åº¦: {model_name_for_title} ({self.training_model_type.upper()})', fontsize=15, pad=25)
        fig.tight_layout(rect=[0, 0.08, 1, 0.95]) # è°ƒæ•´å¸ƒå±€ä¸ºå›¾ä¾‹ç•™å‡ºç©ºé—´

        plot_bytes_val = None
        if return_bytes or save_plot:
            buf = io.BytesIO()
            plt.savefig(buf, format='png', dpi=150, bbox_inches='tight') # bbox_inches='tight' ç¡®ä¿æ‰€æœ‰å†…å®¹éƒ½ä¿å­˜
            buf.seek(0)
            plot_bytes_val = buf.getvalue()
            buf.close()

        # if save_plot and plot_bytes_val:
        #     plot_filename = f"training_progress_{self.training_model_identifier_or_path.replace('/', '_')}_{self.training_model_type}.png"
        #     with open(plot_filename, 'wb') as f:
        #         f.write(plot_bytes_val)
        #     print(f"ğŸ“ˆ è®­ç»ƒè¿›åº¦å›¾å·²ä¿å­˜åˆ° {plot_filename}")

        # if show_plot:
        #     # ç”±äºä½¿ç”¨äº† 'Agg' åç«¯ï¼Œplt.show() ä¸ä¼šæ˜¾ç¤ºä»»ä½•å†…å®¹ã€‚
        #     # ç”¨æˆ·éœ€è¦æ‰“å¼€ä¿å­˜çš„PNGæ–‡ä»¶æ¥æŸ¥çœ‹å›¾åƒã€‚
        #     print("ç»˜å›¾å·²ç”Ÿæˆã€‚å¦‚æœ save_plot=Trueï¼Œè¯·æ£€æŸ¥ä¿å­˜çš„PNGæ–‡ä»¶ã€‚")

        plt.close(fig) # å…³é—­å›¾å½¢ä»¥é‡Šæ”¾å†…å­˜

        if return_bytes:
            return plot_bytes_val
        return None

def build_pruned_forest(post_storage: PostStorage, similarity_threshold: float):
    """
    åŸºäºç›¸ä¼¼åº¦é˜ˆå€¼æ„å»ºå‰ªæåçš„æ£®æ—
    """
    print("ğŸ”„ æ„å»ºå‰ªææ£®æ—...")
    post_storage.forests.clear()
    pruning_results = post_storage.prune_all_posts_by_similarity(
        similarity_threshold=similarity_threshold, show_progress=True
    )
    print(f"âœ… æ£®æ—æ„å»ºå®Œæˆ: {len(pruning_results)} ä¸ªå¸–å­")
    return pruning_results

def fine_tune_contrastive_model(
    model: ContrastiveEncoder,
    train_loader: DataLoader,
    optimizer: torch.optim.Optimizer,
    loss_fn: ContrastiveLoss,
    device: torch.device,
    num_epochs: int = 3,
    scheduler_patience: int = 2,
    min_improvement: float = 1e-5
):
    """
    å¯¹æ¯”æ¨¡å‹å¾®è°ƒ
    """
    model.train()
    best_loss = float('inf')
    patience_counter = 0
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=scheduler_patience, threshold=min_improvement
    )

    for epoch in range(num_epochs):
        epoch_loss = 0.0
        batches_processed = 0

        progress_bar = tqdm(train_loader, desc=f"å¾®è°ƒå¯¹æ¯”æ¨¡å‹ (Epoch {epoch+1}/{num_epochs})", leave=False)
        for batch in progress_bar:
            optimizer.zero_grad()
            
            anchor_texts = batch['anchor_texts']
            positive_texts_ds1 = batch['positive_texts_ds1']
            negative_texts = batch['negative_texts']
            num_negatives = batch['num_negatives']

            # è¿‡æ»¤æ‰ positive_texts_ds1 ä¸­çš„ None (æ¥è‡ªDataset2çš„å ä½ç¬¦)
            valid_indices_ds1 = [i for i, txt in enumerate(positive_texts_ds1) if txt is not None]
            
            if valid_indices_ds1:
                anchor_texts_ds1 = [anchor_texts[i] for i in valid_indices_ds1]
                positive_texts_ds1_f = [positive_texts_ds1[i] for i in valid_indices_ds1]

                if anchor_texts_ds1: # ç¡®ä¿åˆ—è¡¨ä¸ä¸ºç©º
                    anchor_emb = model(anchor_texts_ds1)
                    positive_emb_ds1 = model(positive_texts_ds1_f)
                    
                    # é‡æ„ä¸è¿‡æ»¤åæ ·æœ¬å¯¹åº”çš„è´Ÿæ ·æœ¬
                    neg_emb = None
                    if negative_texts and num_negatives > 0:
                        # ä»æ‰å¹³åˆ—è¡¨ä¸­æå–ä¸ valid_indices_ds1 å¯¹åº”çš„è´Ÿæ ·æœ¬
                        current_batch_neg_texts = []
                        original_batch_size_collator = len(anchor_texts) # collatorå¤„ç†å‰çš„batchå¤§å°
                        for orig_idx in valid_indices_ds1:
                            start = orig_idx * num_negatives
                            end = start + num_negatives
                            current_batch_neg_texts.extend(negative_texts[start:end])
                        
                        if current_batch_neg_texts:
                            neg_emb_flat = model(current_batch_neg_texts)
                            neg_emb = neg_emb_flat.view(len(anchor_texts_ds1), num_negatives, -1)
                    
                    if neg_emb is not None and neg_emb.nelement() > 0:
                        loss = loss_fn(anchor_emb, positive_emb_ds1, neg_emb)
                    else:
                        loss = loss_fn(anchor_emb, positive_emb_ds1)

                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    optimizer.step()

                    epoch_loss += loss.item()
                    batches_processed += 1

            progress_bar.set_postfix(loss=epoch_loss / batches_processed if batches_processed > 0 else epoch_loss)

        avg_loss = epoch_loss / batches_processed if batches_processed > 0 else 0.0
        scheduler.step(avg_loss)

        if avg_loss < best_loss - min_improvement:
            best_loss = avg_loss
            patience_counter = 0
            print(f"ğŸ’¾ ä¿å­˜å½“å‰æœ€ä½³æ¨¡å‹ï¼ŒæŸå¤±: {best_loss:.4f}")
            # å¯ä»¥é€‰æ‹©ä¿å­˜æ¨¡å‹
            # torch.save(model.state_dict(), "best_contrastive_model.pth")
        else:
            patience_counter += 1
            print(f"â³ ç­‰å¾…æ›´ä¼˜æ¨¡å‹ï¼Œå½“å‰è®¡æ•°å™¨: {patience_counter}/{scheduler_patience}")

        if patience_counter >= scheduler_patience:
            print("ğŸ›‘ æ—©åœè§¦å‘ï¼Œåœæ­¢è®­ç»ƒ")

def main_training_pipeline():
    print("ğŸš€ å¼€å§‹è®­ç»ƒæµç¨‹...")
    # 1. å‡†å¤‡æ•°æ®
    try:
        comment_df = pd.read_csv('data/cl_data/train_comments_filtered.csv', encoding='utf-8')
        post_df = pd.read_csv('data/cl_data/train_posts_filtered.csv', encoding='utf-8')
    except FileNotFoundError:
        print("é”™è¯¯: comments_data.csv æˆ– contents_data.csv æœªæ‰¾åˆ°ã€‚è¯·ç¡®ä¿æ•°æ®æ–‡ä»¶å­˜åœ¨ã€‚")
        return      
    

    # ç¡®ä¿ note_id å’Œ comment_id æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œä»¥é¿å…åç»­é—®é¢˜
    comment_df['note_id'] = comment_df['note_id'].astype(str)
    comment_df['comment_id'] = comment_df['comment_id'].astype(str)
    comment_df['parent_comment_id'] = comment_df['parent_comment_id'].astype(str)
    post_df['note_id'] = post_df['note_id'].astype(str)


    storage = PostStorage()
    # ç¡®ä¿å¸–å­å†…å®¹æ˜¯å­—ç¬¦ä¸²ï¼Œå¦‚æœ title ä¸å­˜åœ¨ï¼Œå°è¯• contentï¼Œå¦‚æœéƒ½ä¸ºç©ºï¼Œåˆ™ä¸ºç©ºå­—ç¬¦ä¸²
    for _, row in post_df.iterrows():
        post_content = str(row.get('title', '')) or str(row.get('content', '')) # ä¿è¯æ˜¯å­—ç¬¦ä¸²
        storage.add_post(post_id=str(row['note_id']), post_content=post_content)

    for _, row in comment_df.iterrows():
        post_id_str = str(row['note_id'])
        comment_id_str = str(row['comment_id'])
        content_str = str(row.get('content', '')) # ç¡®ä¿å†…å®¹æ˜¯å­—ç¬¦ä¸²
        parent_id_str = str(row['parent_comment_id']) if str(row['parent_comment_id']) != '0' else post_id_str
        
        try:
            storage.add_comment_to_post(post_id_str, comment_id_str, content_str, parent_id_str)
        except Exception as e:
            print(f"æ’å…¥è¯„è®ºå¤±è´¥: {e}, å¸–å­ID: {post_id_str}, è¯„è®ºID: {comment_id_str}")

    # 2. é€‰æ‹©è®­ç»ƒæ¨¡å‹ç±»å‹å¹¶é…ç½®è®­ç»ƒå™¨
    common_trainer_params = {
        'post_storage': storage,
        'pruning_model_path': "google-bert/bert-base-chinese", # 
        'similarity_threshold': 0.95, # è°ƒæ•´é˜ˆå€¼
        'num_negatives': 8,      # å¢åŠ è´Ÿæ ·æœ¬æ•°é‡
        'batch_size': 8,        # è°ƒæ•´æ‰¹é‡å¤§å°
        'pruning_inference_batch_size': 16, # <--- ä¸ºå‰ªææ¨¡å‹æ¨æ–­è®¾ç½®ä¸€ä¸ªåˆç†çš„æ‰¹å¤§å°
        'base_lr': 5e-6,         # è°ƒæ•´å­¦ä¹ ç‡
        'projection_lr': 5e-5,
        'use_weighted_loss': True,
        'loss_weights': {'dataset1': 1, 'dataset2': 0}, # è°ƒæ•´æƒé‡
        'adaptive_weighting': False, # å¯ç”¨è‡ªé€‚åº”æƒé‡
        'infonce_mode': 'bidirectional', # åŒå‘å¯¹æ¯”
        'projection_head_config': {'hidden_dim': 768, 'output_dim': 384, 'dropout_rate': 0.15}, # è°ƒæ•´æŠ•å½±å¤´
        'min_subtree_size_ds1': 2, 'max_samples_per_post_ds1': None,
        'min_subtree_size_ds2': 100000, 'max_samples_per_subtree_ds2': None,

        # --- æ–°å¢PEFTé…ç½® ---
        'use_peft': True,  # è®¾ç½®ä¸º True æ¥å¯ç”¨ LoRA
        'peft_config': {
            'r': 8,              # LoRAçš„ç§©ï¼Œè¶Šå°å‚æ•°è¶Šå°‘ï¼Œå¸¸ç”¨8, 16, 32
            'lora_alpha': 16,    # LoRAçš„ç¼©æ”¾å› å­ï¼Œé€šå¸¸æ˜¯rçš„ä¸¤å€
            'target_modules': ["query", "key", "value"], # å¯¹æ³¨æ„åŠ›çš„Q,K,Våº”ç”¨
            'lora_dropout': 0.1, # LoRAå±‚çš„dropoutç‡
            'bias': "none",      # "none", "all", "lora_only"
    }}

    # ğŸ¯ é€‰é¡¹ 1: ModelScope æ¨¡å‹
    print("\n--- é…ç½® ModelScope æ¨¡å‹è®­ç»ƒ ---")
    trainer = DynamicContrastiveTrainer(
        training_model_type='modelscope',
        # ä½¿ç”¨å¦ä¸€ä¸ªModelScopeæ¨¡å‹ä½œä¸ºè®­ç»ƒç›®æ ‡
        training_model_identifier_or_path="google-bert/bert-base-chinese",
        **common_trainer_params
    )

    # # ğŸ¯ é€‰é¡¹ 2: è‡ªå®šä¹‰ TextCNN
    # print("\n--- é…ç½® TextCNN è®­ç»ƒ ---")
    # textcnn_specific_config = {
    #     'embedding_dim': 300,       
    #     'num_filters': 128,         
    #     'filter_sizes': [2, 3, 4],  
    #     'model_dropout_rate': 0.1,  
    #     'max_seq_length': 200,      # TextCNNåˆ†è¯å™¨çš„æœ€å¤§åºåˆ—é•¿åº¦
    #     'textcnn_output_dim': 768,  # TextCNNè¾“å‡ºç»´åº¦ (ä¸æŠ•å½±å¤´è¾“å‡ºåŒ¹é…æˆ–ä½œä¸ºå…¶è¾“å…¥)
    #     'min_vocab_freq': 1         # è¯æ±‡è¡¨æœ€å°è¯é¢‘
    # }
    
    # ç¡®ä¿ TextCNN çš„è¾“å‡ºç»´åº¦ä¸æŠ•å½±å¤´çš„è¾“å…¥ç»´åº¦åŒ¹é…
    # common_trainer_params['projection_head_config']['hidden_dim'] å¯ä»¥åŸºäº textcnn_output_dim
    # æˆ–è€… textcnn_output_dim ç›´æ¥ä½œä¸ºæŠ•å½±å¤´çš„è¾“å…¥
    # è¿™é‡Œå‡è®¾ textcnn_output_dim æ˜¯æŠ•å½±å¤´çš„è¾“å…¥ï¼Œæ‰€ä»¥ base_dim ä¼šæ˜¯ textcnn_output_dim

    # trainer = DynamicContrastiveTrainer(
    #     training_model_type='textcnn',
    #     training_model_identifier_or_path="model/my_custom_textcnn_v4_no_pruning_paircl", # è‡ªå®šä¹‰æ¨¡å‹æ ‡è¯†ç¬¦
    #     textcnn_config=textcnn_specific_config,
    #     **common_trainer_params
    # )
    
    # 3. å¼€å§‹è®­ç»ƒ
    print("\n--- å¼€å§‹è®­ç»ƒ ---")
    trainer.train(
        num_epochs=1, # ä¸ºäº†å¿«é€Ÿæµ‹è¯•ï¼Œå‡å°‘äº†epochï¼ŒåŸä¸º100
        rebuild_frequency=2,  # ä¸ºäº†å¿«é€Ÿæµ‹è¯•ï¼Œå‡å°‘äº†é¢‘ç‡ï¼ŒåŸä¸º200
        scheduler_patience=7, # åŸä¸º2
        min_improvement=1e-5
    )
    
    print("ğŸ‰ è®­ç»ƒæµç¨‹å®Œæˆ!")
    print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹å’Œè®­ç»ƒçŠ¶æ€å·²ä¿å­˜ã€‚è®­ç»ƒåçš„åŸºç¡€æ¨¡å‹éƒ¨åˆ†ä½äº 'trained_{trainer.training_model_type}_embedding_model' ç›®å½•ä¸­ã€‚")


if __name__ == "__main__":
    main_training_pipeline()






