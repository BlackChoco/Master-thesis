import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
import pandas as pd
import json
import os
from tqdm import tqdm
from Tree_data_model import PostStorage
from cl_base_model import ContrastiveEncoder, load_model_from_modelscope, load_tokenizer_from_modelscope
from cl_loss import ContrastiveLoss
from peft import get_peft_model, LoraConfig


def build_pruned_forest(post_storage: PostStorage, similarity_threshold: float):
    """
    åŸºäºç›¸ä¼¼åº¦é˜ˆå€¼æ„å»ºå‰ªæåçš„æ£®æ—
    """
    print("ğŸ”„ æ„å»ºå‰ªææ£®æ—...")
    post_storage.forests.clear()
    pruning_results = post_storage.prune_all_posts_by_similarity(
        similarity_threshold=similarity_threshold, show_progress=True
    )
    print(f"âœ… æ£®æ—æ„å»ºå®Œæˆ: {len(pruning_results)} ä¸ªå¸–å­")
    return pruning_results


def fine_tune_contrastive_model(
    model: ContrastiveEncoder,
    train_loader: DataLoader,
    optimizer: torch.optim.Optimizer,
    loss_fn: ContrastiveLoss,
    device: torch.device,
    num_epochs: int = 3,
    scheduler_patience: int = 2,
    min_improvement: float = 1e-5
):
    """
    å¯¹æ¯”æ¨¡å‹å¾®è°ƒ
    """
    model.train()
    best_loss = float('inf')
    patience_counter = 0
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=scheduler_patience, threshold=min_improvement
    )

    for epoch in range(num_epochs):
        epoch_loss = 0.0
        batches_processed = 0

        progress_bar = tqdm(train_loader, desc=f"å¾®è°ƒå¯¹æ¯”æ¨¡å‹ (Epoch {epoch+1}/{num_epochs})", leave=False)
        for batch in progress_bar:
            optimizer.zero_grad()
            
            anchor_texts = batch['anchor_texts']
            positive_texts_ds1 = batch['positive_texts_ds1']
            negative_texts = batch['negative_texts']
            num_negatives = batch['num_negatives']

            # è¿‡æ»¤æ‰ positive_texts_ds1 ä¸­çš„ None (æ¥è‡ªDataset2çš„å ä½ç¬¦)
            valid_indices_ds1 = [i for i, txt in enumerate(positive_texts_ds1) if txt is not None]
            
            if valid_indices_ds1:
                anchor_texts_ds1 = [anchor_texts[i] for i in valid_indices_ds1]
                positive_texts_ds1_f = [positive_texts_ds1[i] for i in valid_indices_ds1]

                if anchor_texts_ds1:
                    anchor_emb = model(anchor_texts_ds1)
                    positive_emb_ds1 = model(positive_texts_ds1_f)
                    
                    # é‡æ„ä¸è¿‡æ»¤åæ ·æœ¬å¯¹åº”çš„è´Ÿæ ·æœ¬
                    neg_emb = None
                    if negative_texts and num_negatives > 0:
                        current_batch_neg_texts = []
                        for orig_idx in valid_indices_ds1:
                            start = orig_idx * num_negatives
                            end = start + num_negatives
                            current_batch_neg_texts.extend(negative_texts[start:end])
                        
                        if current_batch_neg_texts:
                            neg_emb_flat = model(current_batch_neg_texts)
                            neg_emb = neg_emb_flat.view(len(anchor_texts_ds1), num_negatives, -1)
                    
                    if neg_emb is not None and neg_emb.nelement() > 0:
                        loss = loss_fn(anchor_emb, positive_emb_ds1, neg_emb)
                    else:
                        loss = loss_fn(anchor_emb, positive_emb_ds1)

                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    optimizer.step()

                    epoch_loss += loss.item()
                    batches_processed += 1

            progress_bar.set_postfix(loss=epoch_loss / batches_processed if batches_processed > 0 else epoch_loss)

        avg_loss = epoch_loss / batches_processed if batches_processed > 0 else 0.0
        scheduler.step(avg_loss)

        if avg_loss < best_loss - min_improvement:
            best_loss = avg_loss
            patience_counter = 0
            print(f"ğŸ’¾ ä¿å­˜å½“å‰æœ€ä½³æ¨¡å‹ï¼ŒæŸå¤±: {best_loss:.4f}")
        else:
            patience_counter += 1
            print(f"â³ ç­‰å¾…æ›´ä¼˜æ¨¡å‹ï¼Œå½“å‰è®¡æ•°å™¨: {patience_counter}/{scheduler_patience}")

        if patience_counter >= scheduler_patience:
            print("ğŸ›‘ æ—©åœè§¦å‘ï¼Œåœæ­¢è®­ç»ƒ")
            break


def load_trained_model_and_tokenizer(checkpoint_path: str):
    """
    ä»checkpointåŠ è½½å®Œæ•´çš„è®­ç»ƒå™¨çŠ¶æ€ï¼Œå¹¶è¿”å›è®­ç»ƒå¥½çš„åŸºç¡€æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚
    """
    if not os.path.exists(checkpoint_path):
        print(f"é”™è¯¯: Checkpoint æ–‡ä»¶ {checkpoint_path} æœªæ‰¾åˆ°ã€‚")
        return None, None, None

    print(f"æ­£åœ¨ä» {checkpoint_path} åŠ è½½checkpoint...")
    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'), weights_only=False)

    model_type = checkpoint['training_model_type']
    model_identifier = checkpoint['training_model_identifier_or_path']
    proj_config = checkpoint['projection_head_config']

    # è·å–PEFTé…ç½®
    use_peft = checkpoint.get('use_peft', False)
    peft_config = checkpoint.get('peft_config', None)
    
    encoder = None
    if model_type == 'textcnn':
        textcnn_conf = checkpoint['textcnn_config']
        vocab_data = checkpoint['vocab']
        encoder = ContrastiveEncoder(
            model_type='textcnn',
            vocab=vocab_data,
            textcnn_config=textcnn_conf,
            projection_hidden_dim=proj_config['hidden_dim'],
            projection_output_dim=proj_config['output_dim'],
            projection_dropout_rate=proj_config['dropout_rate']
        )
    elif model_type == 'modelscope':
        encoder = ContrastiveEncoder(
            model_type='modelscope',
            model_name_or_path=model_identifier,
            projection_hidden_dim=proj_config['hidden_dim'],
            projection_output_dim=proj_config['output_dim'],
            projection_dropout_rate=proj_config['dropout_rate']
        )
        # å¦‚æœä½¿ç”¨äº†PEFTï¼Œé‡æ–°åŒ…è£…æ¨¡å‹
        if use_peft:
            print("ğŸ”§ æ£€æµ‹åˆ°PEFTè®­ç»ƒçš„checkpointï¼Œæ­£åœ¨é‡æ–°åº”ç”¨LoRAé…ç½®...")
            lora_config = LoraConfig(**peft_config)
            encoder.base_model = get_peft_model(encoder.base_model, lora_config)
            print("âœ… LoRAé…ç½®å·²é‡æ–°åº”ç”¨ã€‚")
    else:
        print(f"é”™è¯¯: Checkpointä¸­æœªçŸ¥çš„æ¨¡å‹ç±»å‹ '{model_type}'")
        return None, None, None

    encoder.load_state_dict(checkpoint['contrastive_encoder_state_dict'])
    encoder.eval()
    print(f"âœ… {model_type.upper()} ContrastiveEncoder åŠ è½½å®Œæˆå¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ã€‚")
    
    return encoder.base_model, encoder.tokenizer, model_type

