import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
import os
import json
import random
import itertools
import hashlib
import copy
from tqdm import tqdm
from torch.optim import AdamW
from transformers import get_linear_schedule_with_warmup
# --- å…³é”®ä¿®æ”¹: å¯¼å…¥ AutoModel å’Œ AutoTokenizer ---
from modelscope import AutoModel, AutoTokenizer
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report
from sklearn.model_selection import train_test_split # ç”¨äºä»è®­ç»ƒé›†ä¸­åˆ’åˆ†éªŒè¯é›†
from peft import get_peft_model, LoraConfig


# ä»è§£è€¦åçš„æ¨¡å—å¯¼å…¥å¿…è¦çš„ç±»
from cl_base_model import ContrastiveEncoder, TextCNNModel, TextCNNTokenizer

# --- 2. ç¼–ç å™¨ç¼“å­˜ç›¸å…³ç±» ---

class EncodedDataset(Dataset):
    """ä½¿ç”¨é¢„ç¼–ç ç‰¹å¾çš„æ•°æ®é›†ï¼Œç”¨äºå†»ç»“ç¼–ç å™¨çš„åœºæ™¯"""
    def __init__(self, encoded_features: torch.Tensor, labels: list, label_to_id: dict):
        self.encoded_features = encoded_features
        self.labels = labels
        self.label_to_id = label_to_id

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            'encoded_features': self.encoded_features[idx],
            'labels': torch.tensor(self.label_to_id[self.labels[idx]], dtype=torch.long)
        }

def generate_cache_key(model_name: str, texts: list, data_fraction: float = 1.0, max_len: int = 256, round_num: int = None) -> str:
    """ä¸ºæ–‡æœ¬åˆ—è¡¨å’Œæ¨¡å‹ç”Ÿæˆå”¯ä¸€çš„ç¼“å­˜é”®ï¼ŒåŒ…å«è½®æ¬¡ä¿¡æ¯"""
    # ä½¿ç”¨æ¨¡å‹åç§°ã€æ–‡æœ¬å†…å®¹çš„å“ˆå¸Œå€¼ã€æ•°æ®æ¯”ä¾‹ç”Ÿæˆç¼“å­˜é”®
    texts_str = ''.join(texts[:100])  # åªç”¨å‰100ä¸ªæ–‡æœ¬è®¡ç®—å“ˆå¸Œï¼Œé¿å…è¿‡é•¿
    content_hash = hashlib.md5(f"{model_name}_{texts_str}_{len(texts)}_{data_fraction}_{max_len}".encode()).hexdigest()[:8]
    # åŒ…å«è½®æ¬¡ä¿¡æ¯å’Œç®€çŸ­hash
    if round_num:
        return f"{model_name}_round{round_num}_frac{data_fraction}_{content_hash}"
    return f"{model_name}_frac{data_fraction}_{content_hash}"

def encode_texts_with_model(texts: list, encoder, tokenizer, device, batch_size: int = 64, max_len: int = 256):
    """ä½¿ç”¨ç¼–ç å™¨æ‰¹é‡ç¼–ç æ–‡æœ¬"""
    encoder.eval()
    encoder = encoder.to(device)  # ç¡®ä¿ç¼–ç å™¨åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
    all_features = []

    print(f" æ­£åœ¨ç¼–ç  {len(texts)} ä¸ªæ–‡æœ¬...")

    # åˆ›å»ºä¸´æ—¶æ•°æ®é›†ç”¨äºæ‰¹é‡ç¼–ç 
    temp_dataset = SupervisedTextDataset(
        texts, [0] * len(texts),  # ä¸´æ—¶æ ‡ç­¾ï¼Œä¸ä¼šä½¿ç”¨
        tokenizer, {0: 0}, max_len
    )
    temp_loader = DataLoader(temp_dataset, batch_size=batch_size, shuffle=False)

    with torch.no_grad():
        for batch in tqdm(temp_loader, desc="ç¼–ç ä¸­"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)

            # è·å–ç¼–ç å™¨è¾“å‡º
            base_output = encoder(input_ids=input_ids, attention_mask=attention_mask)

            # æå–ç‰¹å¾ï¼ˆä¸SupervisedModel.forwardä¸­çš„é€»è¾‘ç›¸åŒï¼‰
            if isinstance(base_output, dict):
                if 'last_hidden_state' in base_output:
                    features = base_output['last_hidden_state'][:, 0, :]
                elif 'hidden_states' in base_output:
                    features = base_output['hidden_states'][:, 0, :]
                elif 'pooler_output' in base_output and base_output['pooler_output'] is not None:
                    features = base_output['pooler_output']
                else:
                    raise KeyError(f"åœ¨æ¨¡å‹è¾“å‡ºå­—å…¸ä¸­æ‰¾ä¸åˆ°å¯ç”¨çš„ç‰¹å¾é”®ã€‚å¯ç”¨é”®: {base_output.keys()}")
            elif hasattr(base_output, 'pooler_output') and base_output.pooler_output is not None:
                features = base_output.pooler_output
            elif hasattr(base_output, 'last_hidden_state'):
                features = base_output.last_hidden_state[:, 0, :]
            else:
                features = base_output

            # ç¡®ä¿ç‰¹å¾æ˜¯float32
            if features.dtype != torch.float32:
                features = features.to(torch.float32)

            all_features.append(features.cpu())

    return torch.cat(all_features, dim=0)

def load_or_create_cache(cache_key: str, cache_dir: str, texts: list, encoder, tokenizer, device, config):
    """åŠ è½½æˆ–åˆ›å»ºç¼–ç ç¼“å­˜"""
    cache_path = os.path.join(cache_dir, f"{cache_key}.pt")

    if os.path.exists(cache_path):
        print(f" ä»ç¼“å­˜åŠ è½½ç¼–ç ç‰¹å¾: {cache_path}")
        return torch.load(cache_path, map_location='cpu')
    else:
        print(f" ç¼“å­˜ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»ºæ–°ç¼“å­˜...")
        os.makedirs(cache_dir, exist_ok=True)

        encoded_features = encode_texts_with_model(
            texts, encoder, tokenizer, device,
            batch_size=config['optimization']['cache_batch_size']
        )

        # ä¿å­˜ç¼“å­˜
        torch.save(encoded_features, cache_path)
        print(f" ç¼–ç ç‰¹å¾å·²ç¼“å­˜åˆ°: {cache_path}")

        return encoded_features

def save_best_model_for_seed(model, model_state, hyperparams, best_val_f1, test_metrics, experiment_output_dir):
    """ä¸ºæ¯ä¸ªç§å­ä¿å­˜æœ€ä¼˜æ¨¡å‹"""
    model_name = hyperparams['model_name'].replace('/', '_').replace('-', '_')

    #  ä¿®æ”¹ï¼šåœ¨å®éªŒç›®å½•ä¸‹åˆ›å»ºsaved_modelså­ç›®å½•
    models_dir = os.path.join(experiment_output_dir, 'saved_models')
    save_dir = os.path.join(models_dir, f"{model_name}_frac{hyperparams['data_fraction']}_seed{hyperparams['seed']}")
    os.makedirs(save_dir, exist_ok=True)

    # ä¿å­˜æ¨¡å‹æƒé‡å’Œå…ƒä¿¡æ¯
    model_path = os.path.join(save_dir, 'best_model.pth')
    torch.save({
        'model_state_dict': model_state,
        'hyperparameters': hyperparams,
        'best_val_f1': best_val_f1,
        'test_metrics': test_metrics,
        'model_architecture': type(model).__name__
    }, model_path)

    print(f" ç§å­{hyperparams['seed']}çš„æœ€ä¼˜æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")

    return {
        'model_path': model_path,
        'save_dir': save_dir
    }

class CachedSupervisedModel(nn.Module):
    """ä½¿ç”¨ç¼“å­˜ç‰¹å¾çš„ç›‘ç£æ¨¡å‹ï¼Œç”¨äºå†»ç»“ç¼–ç å™¨çš„åœºæ™¯"""
    def __init__(self, encoder_output_dim: int, num_labels: int, classifier_type: str = 'linear', mlp_hidden_neurons: int = 384):
        super().__init__()

        print(f"ç¼“å­˜æ¨¡å¼åˆ†ç±»å™¨æ¥æ”¶çš„ç‰¹å¾ç»´åº¦: {encoder_output_dim}")

        if classifier_type == 'linear':
            self.classifier = nn.Linear(encoder_output_dim, num_labels)
        elif classifier_type == 'mlp':
            # ä¸¤å±‚MLPåˆ†ç±»å™¨ï¼šæŒ‡å®šä¸­é—´å±‚ç¥ç»å…ƒæ•°é‡
            self.classifier = nn.Sequential(
                nn.Linear(encoder_output_dim, mlp_hidden_neurons),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(mlp_hidden_neurons, num_labels)
            )
            print(f"ç¼“å­˜æ¨¡å¼MLPåˆ†ç±»å™¨ç»“æ„: 2å±‚ï¼Œç»´åº¦å˜åŒ–: {encoder_output_dim} -> {mlp_hidden_neurons} -> {num_labels}")
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ†ç±»å™¨ç±»å‹: {classifier_type}ã€‚è¯·é€‰æ‹© 'linear' æˆ– 'mlp'ã€‚")

    def forward(self, encoded_features):
        return self.classifier(encoded_features)

# --- 3. ä¿®æ”¹åçš„è®­ç»ƒå’Œè¯„ä¼°å‡½æ•° ---

def train_epoch_cached(model, data_loader, loss_fn, optimizer, device, scheduler):
    """ç¼“å­˜æ¨¡å¼çš„è®­ç»ƒè½®æ¬¡"""
    model.train()
    total_loss = 0
    for batch in tqdm(data_loader, desc="è®­ç»ƒä¸­", leave=False):
        encoded_features = batch['encoded_features'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()
        outputs = model(encoded_features)
        loss = loss_fn(outputs, labels)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()
        total_loss += loss.item()
    return total_loss / len(data_loader)

def evaluate_model_cached(model, data_loader, loss_fn, device, id_to_label: dict):
    """ç¼“å­˜æ¨¡å¼çš„æ¨¡å‹è¯„ä¼°"""
    model.eval()
    total_loss = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for batch in tqdm(data_loader, desc="è¯„ä¼°ä¸­", leave=False):
            encoded_features = batch['encoded_features'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(encoded_features)
            loss = loss_fn(outputs, labels)
            total_loss += loss.item()

            preds = torch.argmax(outputs, dim=1).cpu().numpy()
            all_preds.extend(preds)
            all_labels.extend(labels.cpu().numpy())

    # è®¡ç®—æŒ‡æ ‡
    avg_loss = total_loss / len(data_loader)
    accuracy = accuracy_score(all_labels, all_preds)

    # MacroæŒ‡æ ‡ï¼šå„ç±»åˆ«ç®€å•å¹³å‡
    f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)
    precision_macro = precision_score(all_labels, all_preds, average='macro', zero_division=0)
    recall_macro = recall_score(all_labels, all_preds, average='macro', zero_division=0)

    # MicroæŒ‡æ ‡ï¼šä»…è®¡ç®—F1ç”¨äºéªŒè¯
    f1_micro = f1_score(all_labels, all_preds, average='micro', zero_division=0)

    class_ids = sorted(id_to_label.keys())
    target_names = [f"class_{id_to_label[cid]}" for cid in class_ids]
    report_dict = classification_report(
        all_labels,
        all_preds,
        labels=class_ids,
        target_names=target_names,
        output_dict=True,
        zero_division=0
    )

    per_class_metrics = {}
    for name in target_names:
        if name in report_dict:
            metrics = report_dict[name].copy()
            metrics.pop('support', None)
            per_class_metrics[name] = metrics

    return {
        # æ ¸å¿ƒæŒ‡æ ‡ï¼šé€‚åˆå‡è¡¡æ•°æ®é›†çš„5ä¸ªå…³é”®æŒ‡æ ‡
        "accuracy": accuracy,                # æ•´ä½“å‡†ç¡®ç‡
        "precision": precision_macro,        # Macroç²¾ç¡®ç‡
        "recall": recall_macro,             # Macroå¬å›ç‡
        "f1_score": f1_macro,               # Macro F1åˆ†æ•°
        "f1_micro": f1_micro,               # Micro F1ï¼ˆéªŒè¯ç”¨ï¼Œåº”ç­‰äºaccuracyï¼‰
        # è¾…åŠ©ä¿¡æ¯
        "loss": avg_loss,
        "per_class_metrics": per_class_metrics
    }

# --- 3. æ•°æ®é›†å’Œæ¨¡å‹å®šä¹‰ ---

class SupervisedTextDataset(Dataset):
    """ç”¨äºæœ‰ç›‘ç£æ–‡æœ¬åˆ†ç±»çš„PyTorchæ•°æ®é›†ã€‚"""
    def __init__(self, texts: list, labels: list, tokenizer, label_to_id: dict, max_len: int = 256):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.label_to_id = label_to_id

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        
        # ä½¿ç”¨ä¸é¢„è®­ç»ƒæ—¶ç›¸åŒçš„åˆ†è¯å™¨
        # æ£€æŸ¥åˆ†è¯å™¨ç±»å‹ï¼Œå› ä¸º TextCNNTokenizer å’Œ HF Tokenizer çš„å‚æ•°ä¸åŒ
        if isinstance(self.tokenizer, TextCNNTokenizer):
            encoding = self.tokenizer(
                text,
                padding=True,
                truncation=True,
                return_tensors='pt',
                max_length=self.max_len
            )
        else: # å‡è®¾æ˜¯ HuggingFace Tokenizer
            encoding = self.tokenizer(
                text,
                add_special_tokens=True,
                max_length=self.max_len,
                return_token_type_ids=False,
                padding='max_length',
                truncation=True,
                return_attention_mask=True,
                return_tensors='pt',
            )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(self.label_to_id[label], dtype=torch.long)
        }

class SupervisedModel(nn.Module):
    """
    åŒ…è£…é¢„è®­ç»ƒçš„ç¼–ç å™¨å’Œä¸€ä¸ªåˆ†ç±»å¤´ã€‚
    """
    def __init__(self, base_encoder: nn.Module, num_labels: int, classifier_type: str = 'linear', mlp_hidden_neurons: int = 384):
        super().__init__()
        self.base_encoder = base_encoder

        # ä»åŸºç¡€ç¼–ç å™¨è·å–éšè—å±‚ç»´åº¦
        if hasattr(base_encoder, 'base_dim'): # é€‚ç”¨äºæˆ‘ä»¬è‡ªå®šä¹‰çš„TextCNNModel
            hidden_size = base_encoder.base_dim
        elif hasattr(base_encoder.config, 'hidden_size'): # é€‚ç”¨äºHuggingFace/ModelScopeæ¨¡å‹
            hidden_size = base_encoder.config.hidden_size
        else:
            # å°è¯•ä»æ¨¡å‹è¾“å‡ºè·å–ç»´åº¦ï¼ˆå¦‚æœæ¨¡å‹å·²åŠ è½½å‚æ•°ï¼‰
            try:
                # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿè¾“å…¥æ¥æ¨æ–­ç»´åº¦
                dummy_input = {'input_ids': torch.randint(0, 100, (1, 10)), 'attention_mask': torch.ones(1, 10)}
                dummy_output = base_encoder(**dummy_input)

                # è°ƒè¯•ï¼šæ‰“å°è¾“å‡ºçš„æ‰€æœ‰é”®
                print(f"è°ƒè¯•: æ¨¡å‹è¾“å‡ºé”®: {dummy_output.keys()}")

                # ModelScope æ¨¡å‹é€šå¸¸è¿”å›å­—å…¸ï¼Œå°è¯•ä¸åŒçš„é”®
                if 'last_hidden_state' in dummy_output:
                    hidden_size = dummy_output['last_hidden_state'].shape[-1]
                elif 'hidden_states' in dummy_output:
                    hidden_size = dummy_output['hidden_states'].shape[-1]
                else:
                    raise KeyError("åœ¨æ¨¡å‹è¾“å‡ºä¸­æ‰¾ä¸åˆ° 'last_hidden_state' æˆ– 'hidden_states'ã€‚")

            except Exception as e:
                raise ValueError(f"æ— æ³•è‡ªåŠ¨ç¡®å®šåŸºç¡€ç¼–ç å™¨çš„è¾“å‡ºç»´åº¦: {e}")

        print(f"åˆ†ç±»å™¨æ¥æ”¶çš„éšè—å±‚ç»´åº¦: {hidden_size}")

        if classifier_type == 'linear':
            self.classifier = nn.Linear(hidden_size, num_labels)
        elif classifier_type == 'mlp':
            # ä¸¤å±‚MLPåˆ†ç±»å™¨ï¼šæŒ‡å®šä¸­é—´å±‚ç¥ç»å…ƒæ•°é‡
            self.classifier = nn.Sequential(
                nn.Linear(hidden_size, mlp_hidden_neurons),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(mlp_hidden_neurons, num_labels)
            )
            print(f"MLPåˆ†ç±»å™¨ç»“æ„: 2å±‚ï¼Œç»´åº¦å˜åŒ–: {hidden_size} -> {mlp_hidden_neurons} -> {num_labels}")
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ†ç±»å™¨ç±»å‹: {classifier_type}ã€‚è¯·é€‰æ‹© 'linear' æˆ– 'mlp'ã€‚")

    def forward(self, input_ids, attention_mask):
        # åŸºç¡€ç¼–ç å™¨è¾“å‡º
        # ModelScope æ¨¡å‹éœ€è¦å…³é”®å­—å‚æ•°
        base_output = self.base_encoder(input_ids=input_ids, attention_mask=attention_mask)
        
        # æ ¹æ®åŸºç¡€ç¼–ç å™¨çš„è¾“å‡ºç±»å‹æå–ç‰¹å¾
        # ModelScope æ¨¡å‹é€šå¸¸è¿”å›ä¸€ä¸ªå­—å…¸
        if isinstance(base_output, dict):
            if 'last_hidden_state' in base_output:
                # [CLS] token representation
                features = base_output['last_hidden_state'][:, 0, :]
            elif 'hidden_states' in base_output:
                # æœ‰äº›æ¨¡å‹å¯èƒ½åªè¿”å› hidden_states
                features = base_output['hidden_states'][:, 0, :]
            elif 'pooler_output' in base_output and base_output['pooler_output'] is not None:
                features = base_output['pooler_output']
            else:
                # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼ŒæŠ›å‡ºé”™è¯¯
                raise KeyError(f"åœ¨æ¨¡å‹è¾“å‡ºå­—å…¸ä¸­æ‰¾ä¸åˆ°å¯ç”¨çš„ç‰¹å¾é”®ã€‚å¯ç”¨é”®: {base_output.keys()}")
        elif hasattr(base_output, 'pooler_output') and base_output.pooler_output is not None:
            features = base_output.pooler_output
        elif hasattr(base_output, 'last_hidden_state'):
            features = base_output.last_hidden_state[:, 0, :]
        else:
            features = base_output

        # --- ä¿®æ­£ï¼šç¡®ä¿ features æ˜¯ float32 ---
        if features.dtype != torch.float32:
            features = features.to(torch.float32)
        # --------------------------------------

        logits = self.classifier(features)
        return logits

    def freeze_encoder(self):
        """å†»ç»“åŸºç¡€ç¼–ç å™¨çš„æ‰€æœ‰å‚æ•°ã€‚"""
        print(" æ­£åœ¨å†»ç»“åŸºç¡€ç¼–ç å™¨çš„å‚æ•°...")
        for param in self.base_encoder.parameters():
            param.requires_grad = False
        print(" åŸºç¡€ç¼–ç å™¨å·²å†»ç»“ã€‚")

    def unfreeze_encoder(self):
        """è§£å†»åŸºç¡€ç¼–ç å™¨çš„æ‰€æœ‰å‚æ•°ã€‚"""
        print(" æ­£åœ¨è§£å†»åŸºç¡€ç¼–ç å™¨çš„å‚æ•°...")
        for param in self.base_encoder.parameters():
            param.requires_grad = True
        print(" åŸºç¡€ç¼–ç å™¨å·²è§£å†»ã€‚")


# --- 2. è¾…åŠ©å‡½æ•° ---

def set_seed(seed_value=42):
    """ä¸ºæ‰€æœ‰ç›¸å…³åº“è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å¯å¤ç°æ€§ã€‚"""
    random.seed(seed_value)
    np.random.seed(seed_value)
    torch.manual_seed(seed_value)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed_value)

def load_data(train_path, test_path):
    """åŠ è½½è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ã€‚"""
    try:
        df_train = pd.read_csv(train_path)
        df_test = pd.read_csv(test_path)
        print(f"åŠ è½½æ•°æ®: è®­ç»ƒé›† {len(df_train)} è¡Œ, æµ‹è¯•é›† {len(df_test)} è¡Œã€‚")
        return df_train, df_test
    except FileNotFoundError as e:
        print(f"é”™è¯¯: æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ° - {e}")
        return None, None

def load_pretrained_encoder(checkpoint_path: str):
    """
    ä»å¯¹æ¯”å­¦ä¹ é˜¶æ®µçš„checkpointåŠ è½½åŸºç¡€æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œæˆ–ç›´æ¥ä»ModelScope HubåŠ è½½ã€‚
    """
    if os.path.exists(checkpoint_path):
        print(f"æ­£åœ¨ä»æœ¬åœ°checkpoint {checkpoint_path} åŠ è½½...")
        checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'), weights_only=False)

        model_type = checkpoint.get('training_model_type', 'hf')
        model_identifier = checkpoint.get('training_model_identifier_or_path')
        proj_config = checkpoint.get('projection_head_config')
        use_peft = checkpoint.get('use_peft', False)
        peft_config = checkpoint.get('peft_config', None)

        temp_encoder = None
        if model_type == 'textcnn':
            textcnn_conf = checkpoint['textcnn_config']
            vocab_data = checkpoint['vocab']
            temp_encoder = ContrastiveEncoder(
                model_type='textcnn', vocab=vocab_data, textcnn_config=textcnn_conf,
                projection_hidden_dim=proj_config['hidden_dim'],
                projection_output_dim=proj_config['output_dim'],
                projection_dropout_rate=proj_config['dropout_rate']
            )
        elif model_type in ['hf', 'modelscope']:
            temp_encoder = ContrastiveEncoder(
                model_type='modelscope', model_name_or_path=model_identifier,
                projection_hidden_dim=proj_config['hidden_dim'],
                projection_output_dim=proj_config['output_dim'],
                projection_dropout_rate=proj_config['dropout_rate']
            )
            # --- LoRA/PEFTæ¨¡å‹ç‰¹æ®Šå¤„ç† ---
            if use_peft and peft_config is not None:
                print("æ£€æµ‹åˆ°PEFT/LoRAæ¨¡å‹ï¼Œæ­£åœ¨åº”ç”¨LoRAç»“æ„...")
                lora_config = LoraConfig(**peft_config)
                temp_encoder.base_model = get_peft_model(temp_encoder.base_model, lora_config)
                print("LoRAç»“æ„å·²åº”ç”¨ã€‚")
        else:
            print(f"é”™è¯¯: Checkpointä¸­æœªçŸ¥çš„æ¨¡å‹ç±»å‹ '{model_type}'")
            return None, None, None

        # åŠ è½½æƒé‡æ—¶å…è®¸strict=Falseï¼Œå…¼å®¹LoRAæƒé‡
        temp_encoder.load_state_dict(checkpoint['contrastive_encoder_state_dict'], strict=False)
        print(f" Checkpoint åŠ è½½æˆåŠŸã€‚æ¨¡å‹ç±»å‹: {model_type.upper()}")

        return temp_encoder.base_model, temp_encoder.tokenizer, model_type
    
    # å¦‚æœä¸æ˜¯æœ¬åœ°æ–‡ä»¶ï¼Œåˆ™å°è¯•ä» ModelScope Hub åŠ è½½
    else:
        print(f"æœªæ‰¾åˆ°æœ¬åœ°æ–‡ä»¶ï¼Œå°è¯•ä» ModelScope Hub åŠ è½½æ¨¡å‹: {checkpoint_path}")
        try:
            # --- å…³é”®ä¿®æ”¹: ä½¿ç”¨ AutoModel å’Œ AutoTokenizer ---
            base_model = AutoModel.from_pretrained(checkpoint_path, trust_remote_code=True)
            tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)
            print(f" æˆåŠŸä» ModelScope Hub åŠ è½½åŸºç¡€æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚")
            return base_model, tokenizer, 'ms' # è¿”å› 'ms' ä½œä¸ºæ¨¡å‹ç±»å‹
        except Exception as e:
            print(f"é”™è¯¯: æ— æ³•ä» ModelScope Hub åŠ è½½æ¨¡å‹ '{checkpoint_path}': {e}")
            return None, None, None


# --- 3. è®­ç»ƒå’Œè¯„ä¼°é€»è¾‘ ---

def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler):
    """æ‰§è¡Œä¸€ä¸ªè®­ç»ƒè½®æ¬¡ã€‚"""
    model.train()
    total_loss = 0
    for batch in tqdm(data_loader, desc="è®­ç»ƒä¸­", leave=False):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        loss = loss_fn(outputs, labels)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()
        total_loss += loss.item()
    return total_loss / len(data_loader)

def evaluate_model(model, data_loader, loss_fn, device, id_to_label: dict):
    """åœ¨éªŒè¯é›†æˆ–æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹ã€‚"""
    model.eval()
    total_loss = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for batch in tqdm(data_loader, desc="è¯„ä¼°ä¸­", leave=False):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            loss = loss_fn(outputs, labels)
            total_loss += loss.item()

            preds = torch.argmax(outputs, dim=1).cpu().numpy()
            all_preds.extend(preds)
            all_labels.extend(labels.cpu().numpy())

    # --- è®¡ç®—æ•´ä½“æŒ‡æ ‡ ---
    avg_loss = total_loss / len(data_loader)
    accuracy = accuracy_score(all_labels, all_preds)

    # MacroæŒ‡æ ‡ï¼šå„ç±»åˆ«ç®€å•å¹³å‡
    f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)
    precision_macro = precision_score(all_labels, all_preds, average='macro', zero_division=0)
    recall_macro = recall_score(all_labels, all_preds, average='macro', zero_division=0)

    # MicroæŒ‡æ ‡ï¼šä»…è®¡ç®—F1ç”¨äºéªŒè¯
    f1_micro = f1_score(all_labels, all_preds, average='micro', zero_division=0)

    # --- è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡ï¼ˆä¿ç•™ç”¨äºè¯¦ç»†åˆ†æï¼‰ ---
    class_ids = sorted(id_to_label.keys())
    target_names = [f"class_{id_to_label[cid]}" for cid in class_ids]
    report_dict = classification_report(
        all_labels,
        all_preds,
        labels=class_ids,
        target_names=target_names,
        output_dict=True,
        zero_division=0
    )
    per_class_metrics = {}
    for name in target_names:
        if name in report_dict:
            metrics = report_dict[name].copy()
            metrics.pop('support', None)
            per_class_metrics[name] = metrics

    return {
        # æ ¸å¿ƒæŒ‡æ ‡ï¼šé€‚åˆå‡è¡¡æ•°æ®é›†çš„5ä¸ªå…³é”®æŒ‡æ ‡
        "accuracy": accuracy,                # æ•´ä½“å‡†ç¡®ç‡
        "precision": precision_macro,        # Macroç²¾ç¡®ç‡
        "recall": recall_macro,             # Macroå¬å›ç‡
        "f1_score": f1_macro,               # Macro F1åˆ†æ•°
        "f1_micro": f1_micro,               # Micro F1ï¼ˆéªŒè¯ç”¨ï¼Œåº”ç­‰äºaccuracyï¼‰
        # è¾…åŠ©ä¿¡æ¯
        "loss": avg_loss,
        "per_class_metrics": per_class_metrics
    }

# --- 4. è¶…å‚æ•°æœç´¢é…ç½® ---

# ç»Ÿä¸€çš„å®éªŒé…ç½®å­—å…¸
CONFIG = {
    # å®éªŒå…ƒä¿¡æ¯
    'experiment_meta': {
        'description': 'bert_linear_experiment',  # å®éªŒæè¿°æ ‡è¯†ç¬¦
        'experiment_name': 'bertåœ¨ä¸åŒæ•°æ®æ¯”ä¾‹ä¸‹çº¿æ€§è¯„ä¼°ç»“æœ',   # å®éªŒçš„ä¸­æ–‡åç§°
        'purpose': 'è·‘robertaå’Œbertåœ¨çº¿æ€§è¯„ä¼°ä¸‹çš„baseline',  # å®éªŒç›®çš„
        'notes': 'å¯¹é½å®éªŒé…ç½®',  # å®éªŒå¤‡æ³¨
    },

    # æ•°æ®é…ç½®
    'data': {
        'train_data_path': 'data/sup_train_data/balanced_trainset.csv',
        'test_data_path': 'data/sup_train_data/balanced_testset.csv',
        'validation_split': 0.2,  # éªŒè¯é›†æ¯”ä¾‹ï¼ˆä»…åœ¨ä½¿ç”¨æ¯”ä¾‹åˆ†å‰²æ—¶æœ‰æ•ˆï¼‰
        'excluded_labels': [],   # è¦è¿‡æ»¤çš„æ ‡ç­¾
        # æ–°å¢ï¼šå›ºå®šæ•°é‡åˆ†å‰²é…ç½®
        'use_fixed_split': True,    # æ˜¯å¦ä½¿ç”¨å›ºå®šæ•°é‡åˆ†å‰²è€Œéæ¯”ä¾‹åˆ†å‰²
        'train_samples_per_label': 500,  # æ¯ä¸ªæ ‡ç­¾çš„è®­ç»ƒæ ·æœ¬æ•°ï¼ˆä»è®­ç»ƒé›†æ–‡ä»¶é‡‡æ ·ï¼‰
        'val_samples_per_label': 200,    # æ¯ä¸ªæ ‡ç­¾çš„éªŒè¯æ ·æœ¬æ•°ï¼ˆä»éªŒè¯æ•°æ®æºé‡‡æ ·ï¼‰
        'test_samples_per_label': None,  # æ¯ä¸ªæ ‡ç­¾çš„æµ‹è¯•æ ·æœ¬æ•°ï¼ˆä»æµ‹è¯•æ•°æ®æºé‡‡æ ·ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨æ—§é€»è¾‘ï¼šéªŒè¯é›†ä»è®­ç»ƒé›†åˆ†å‰²ï¼Œæµ‹è¯•é›†ä½¿ç”¨å…¨éƒ¨ï¼‰
        'split_random_seed': 42,  # æ•°æ®åˆ’åˆ†çš„éšæœºç§å­ï¼ˆæ§åˆ¶å“ªäº›æ ·æœ¬è¢«é€‰ä¸­ï¼‰
        # æ–°å¢ï¼šæ–°é‡‡æ ·ç­–ç•¥é…ç½®ï¼ˆtest_samples_per_labelä¸ä¸ºNoneæ—¶ç”Ÿæ•ˆï¼‰
        'use_test_for_val_and_test': False,  # æ˜¯å¦ä»æµ‹è¯•é›†æ–‡ä»¶ä¸­åŒæ—¶é‡‡æ ·éªŒè¯é›†å’Œæµ‹è¯•é›†ï¼ˆä¸æ”¾å›é‡‡æ ·ï¼‰
    },

    # æ¨¡å‹é…ç½®
    'models': {
        # 'lora_bert_base_chinese_cl': 'model/google-bert_bert-base-chinese/best_contrastive_model.pth',
        # 'TextCNN_CL_bert': 'model/my_custom_textcnn_v3_bert_pruning_paircl/best_contrastive_model.pth',
        # 'RoBERTa_base_chinese': 'iic/nlp_roberta_backbone_base_std',
        'Bert_base_chinese': 'google-bert/bert-base-chinese',
        # '0.75_round1_0.1_cl_bert' : 'iter_model/frac0.1_round1/best_model.pth'
    },

    # è¶…å‚æ•°æœç´¢ç©ºé—´
    'hyperparameters': {
        'epochs': [10,30,50],                    # è®­ç»ƒè½®æ•°      [50,100]
        'batch_size':[16,32,64] ,              # æ‰¹æ¬¡å¤§å°         [16,32,64,128]
        'learning_rate': [1e-1,1e-2,1e-3], # å­¦ä¹ ç‡       [1e-3,1e-4]
        'data_fractions': [1.0],  # æ•°æ®ä½¿ç”¨æ¯”ä¾‹       [1.0, 0.5, 0.2, 0.1, 0.05, 0.02]
        'seeds': [42],             # éšæœºç§å­ [123, 456, 789, 101, 202, 303, 404, 505, 606]
        'classifier_types': ['linear'], # åˆ†ç±»å™¨ç±»å‹
        'mlp_hidden_neurons': [512],  # MLPéšè—å±‚ç¥ç»å…ƒæ•°é‡
        'freeze_encoder': [True],     # æ˜¯å¦å†»ç»“ç¼–ç å™¨
    },

    # å®éªŒæ§åˆ¶
    'experiment': {
        'base_output_dir': 'sup_result_hyperparams',  # åŸºç¡€è¾“å‡ºç›®å½•
        'save_individual_results': True,
        'aggregate_results': True,
        'save_experiment_info': True,  # ä¿å­˜å®éªŒä¿¡æ¯
    },

    # æ€§èƒ½ä¼˜åŒ–
    'optimization': {
        'use_encoder_cache': True,       # å†»ç»“ç¼–ç å™¨æ—¶æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        'cache_dir': 'encoder_cache',    # ç¼“å­˜ç›®å½•
        'cache_batch_size': 64,          # ç¼“å­˜æ—¶çš„æ‰¹æ¬¡å¤§å°
    }
}

def generate_hyperparameter_combinations(config):
    """ç”Ÿæˆæ‰€æœ‰è¶…å‚æ•°ç»„åˆ"""
    hyperparams = config['hyperparameters']

    # åˆ›å»ºè¶…å‚æ•°ç»„åˆ
    combinations = []
    for (model_name, checkpoint_path), epochs, batch_size, lr, fraction, seed, classifier_type, mlp_hidden_neurons, freeze in itertools.product(
        config['models'].items(),
        hyperparams['epochs'],
        hyperparams['batch_size'],
        hyperparams['learning_rate'],
        hyperparams['data_fractions'],
        hyperparams['seeds'],
        hyperparams['classifier_types'],
        hyperparams['mlp_hidden_neurons'],
        hyperparams['freeze_encoder']
    ):
        # åªæœ‰å½“classifier_type='mlp'æ—¶ï¼Œmlp_hidden_neuronså‚æ•°æ‰æœ‰æ„ä¹‰
        # å½“classifier_type='linear'æ—¶ï¼Œè·³è¿‡ä¸åŒç¥ç»å…ƒæ•°é‡çš„ç»„åˆ
        if classifier_type == 'linear' and mlp_hidden_neurons != hyperparams['mlp_hidden_neurons'][0]:
            continue

        combinations.append({
            'model_name': model_name,
            'checkpoint_path': checkpoint_path,
            'epochs': epochs,
            'batch_size': batch_size,
            'learning_rate': lr,
            'data_fraction': fraction,
            'seed': seed,
            'classifier_type': classifier_type,
            'mlp_hidden_neurons': mlp_hidden_neurons,
            'freeze_encoder': freeze,
        })

    return combinations

def run_single_experiment(config, hyperparams, experiment_output_dir=None, round_num=None):
    """è¿è¡Œå•æ¬¡å®éªŒ"""

    print(f"\n--- å®éªŒé…ç½® ---")
    print(f"æ¨¡å‹: {hyperparams['model_name']}")
    print(f"åˆ†ç±»å™¨: {hyperparams['classifier_type']}")
    if hyperparams['classifier_type'] == 'mlp':
        print(f"MLPéšè—å±‚ç¥ç»å…ƒ: {hyperparams['mlp_hidden_neurons']}")
    print(f"å†»ç»“ç¼–ç å™¨: {hyperparams['freeze_encoder']}")
    print(f"æ•°æ®æ¯”ä¾‹: {hyperparams['data_fraction']*100}%")
    print(f"å­¦ä¹ ç‡: {hyperparams['learning_rate']}")
    print(f"æ‰¹æ¬¡å¤§å°: {hyperparams['batch_size']}")
    print(f"è®­ç»ƒè½®æ•°: {hyperparams['epochs']}")
    print(f"éšæœºç§å­: {hyperparams['seed']}")
    if round_num:
        print(f"å®éªŒè½®æ¬¡: ç¬¬{round_num}è½®")

    # ç¼“å­˜ä¼˜åŒ–æç¤º
    use_cache = config['optimization']['use_encoder_cache'] and hyperparams['freeze_encoder']
    if use_cache:
        print(" ç¼“å­˜ä¼˜åŒ–: å¯ç”¨")
    else:
        print(" ç¼“å­˜ä¼˜åŒ–: ç¦ç”¨ (ç¼–ç å™¨æœªå†»ç»“æˆ–ç¼“å­˜åŠŸèƒ½å…³é—­)")

    # åŠ è½½æ•°æ®
    df_train_full, df_test = load_data(config['data']['train_data_path'], config['data']['test_data_path'])
    if df_train_full is None:
        return None

    # è¿‡æ»¤æ ‡ç­¾
    for label in config['data']['excluded_labels']:
        df_train_full = df_train_full[df_train_full['label'] != label].reset_index(drop=True)
        df_test = df_test[df_test['label'] != label].reset_index(drop=True)

    #  å…³é”®ä¿®æ”¹ï¼šæ ¹æ®é…ç½®é€‰æ‹©æ•°æ®åˆ†å‰²æ–¹å¼
    # âœ… æ–°å¢ï¼šæ£€æµ‹æ˜¯å¦ä½¿ç”¨æ–°é‡‡æ ·ç­–ç•¥ï¼ˆä»æµ‹è¯•é›†ä¸­åŒæ—¶é‡‡æ ·éªŒè¯é›†å’Œæµ‹è¯•é›†ï¼‰
    use_new_sampling = config['data'].get('use_test_for_val_and_test', False) and \
                       config['data'].get('test_samples_per_label') is not None

    if use_new_sampling:
        # ğŸ†• æ–°é‡‡æ ·ç­–ç•¥ï¼šè®­ç»ƒé›†ä»è®­ç»ƒæ–‡ä»¶é‡‡æ ·ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†ä»æµ‹è¯•æ–‡ä»¶é‡‡æ ·ï¼ˆä¸æ”¾å›ï¼‰
        print(f" ä½¿ç”¨æ–°é‡‡æ ·ç­–ç•¥ï¼š")
        print(f"   - è®­ç»ƒé›†ï¼šä»è®­ç»ƒæ–‡ä»¶é‡‡æ ·")
        print(f"   - éªŒè¯é›† + æµ‹è¯•é›†ï¼šä»æµ‹è¯•æ–‡ä»¶é‡‡æ ·ï¼ˆä¸æ”¾å›ï¼‰")

        # ä»è®­ç»ƒé›†æ–‡ä»¶æŒ‰æ ‡ç­¾é‡‡æ ·è®­ç»ƒé›†
        df_train_list = []
        for label in df_train_full['label'].unique():
            label_data = df_train_full[df_train_full['label'] == label].reset_index(drop=True)

            required = config['data']['train_samples_per_label']
            if len(label_data) < required:
                print(f"  è­¦å‘Š: æ ‡ç­¾ '{label}' è®­ç»ƒæ•°æ®åªæœ‰ {len(label_data)} æ¡ï¼Œéœ€è¦ {required} æ¡")
                print(f"   å°†ä½¿ç”¨æ‰€æœ‰å¯ç”¨æ•°æ®")
                label_train = label_data
            else:
                split_seed = config['data'].get('split_random_seed', 42)
                label_data_shuffled = label_data.sample(n=len(label_data), random_state=split_seed).reset_index(drop=True)
                label_train = label_data_shuffled[:required]

            df_train_list.append(label_train)
            print(f"   æ ‡ç­¾ '{label}': {len(label_train)} è®­ç»ƒæ ·æœ¬")

        df_train_prelim = pd.concat(df_train_list, ignore_index=True)

        # ä»æµ‹è¯•é›†æ–‡ä»¶æŒ‰æ ‡ç­¾é‡‡æ ·éªŒè¯é›†å’Œæµ‹è¯•é›†ï¼ˆä¸æ”¾å›ï¼‰
        df_val_list = []
        df_test_sampled_list = []

        val_count = config['data']['val_samples_per_label']
        test_count = config['data']['test_samples_per_label']

        for label in df_test['label'].unique():
            label_data = df_test[df_test['label'] == label].reset_index(drop=True)

            required_total = val_count + test_count
            if len(label_data) < required_total:
                print(f"  è­¦å‘Š: æ ‡ç­¾ '{label}' æµ‹è¯•æ•°æ®åªæœ‰ {len(label_data)} æ¡ï¼Œéœ€è¦ {required_total} æ¡")
                print(f"   å°†æŒ‰æ¯”ä¾‹åˆ†é…éªŒè¯é›†å’Œæµ‹è¯•é›†")
                # æŒ‰æ¯”ä¾‹åˆ†é…
                split_seed = config['data'].get('split_random_seed', 42)
                val_ratio = val_count / required_total
                label_val, label_test = train_test_split(
                    label_data,
                    test_size=(1 - val_ratio),
                    random_state=split_seed
                )
            else:
                # ä½¿ç”¨å¯é…ç½®çš„éšæœºç§å­è¿›è¡Œé‡‡æ ·ï¼ˆä¸æ”¾å›ï¼‰
                split_seed = config['data'].get('split_random_seed', 42)
                label_data_shuffled = label_data.sample(n=len(label_data), random_state=split_seed).reset_index(drop=True)

                # ä¸æ”¾å›é‡‡æ ·ï¼šå‰Nä¸ªä½œä¸ºéªŒè¯é›†ï¼ŒåMä¸ªä½œä¸ºæµ‹è¯•é›†
                label_val = label_data_shuffled[:val_count]
                label_test = label_data_shuffled[val_count:val_count + test_count]

            df_val_list.append(label_val)
            df_test_sampled_list.append(label_test)
            print(f"   æ ‡ç­¾ '{label}': {len(label_val)} éªŒè¯, {len(label_test)} æµ‹è¯•ï¼ˆä»æµ‹è¯•æ–‡ä»¶é‡‡æ ·ï¼‰")

        df_val = pd.concat(df_val_list, ignore_index=True)
        df_test = pd.concat(df_test_sampled_list, ignore_index=True)  # âœ… è¦†ç›–åŸæµ‹è¯•é›†

        print(f" æ–°é‡‡æ ·ç­–ç•¥å®Œæˆ: è®­ç»ƒé›† {len(df_train_prelim)} æ¡, éªŒè¯é›† {len(df_val)} æ¡, æµ‹è¯•é›† {len(df_test)} æ¡")

    elif config['data']['use_fixed_split']:
        # ğŸ”„ æ—§é€»è¾‘ï¼šå›ºå®šæ•°é‡åˆ†å‰²ï¼ˆä»è®­ç»ƒé›†ä¸­åˆ†å‰²éªŒè¯é›†ï¼Œæµ‹è¯•é›†ä½¿ç”¨å…¨éƒ¨ï¼‰
        print(f" ä½¿ç”¨å›ºå®šæ•°é‡åˆ†å‰²ï¼ˆæ—§ç­–ç•¥ï¼‰: æ¯ä¸ªæ ‡ç­¾ {config['data']['train_samples_per_label']} è®­ç»ƒ + {config['data']['val_samples_per_label']} éªŒè¯...")

        # æŒ‰æ ‡ç­¾åˆ†ç»„å¹¶å›ºå®šé‡‡æ ·
        df_train_list = []
        df_val_list = []

        for label in df_train_full['label'].unique():
            label_data = df_train_full[df_train_full['label'] == label].reset_index(drop=True)

            # æ£€æŸ¥æ¯ä¸ªæ ‡ç­¾çš„æ•°æ®é‡æ˜¯å¦è¶³å¤Ÿ
            required_total = config['data']['train_samples_per_label'] + config['data']['val_samples_per_label']
            if len(label_data) < required_total:
                print(f"  è­¦å‘Š: æ ‡ç­¾ '{label}' åªæœ‰ {len(label_data)} æ¡æ•°æ®ï¼Œéœ€è¦ {required_total} æ¡")
                print(f"   å°†ä½¿ç”¨æ‰€æœ‰å¯ç”¨æ•°æ®ï¼ŒæŒ‰åŸæ¯”ä¾‹åˆ†å‰²...")
                # å¦‚æœæ•°æ®ä¸å¤Ÿï¼ŒæŒ‰åŸæ¯”ä¾‹åˆ†å‰²
                split_seed = config['data'].get('split_random_seed', 42)
                label_train, label_val = train_test_split(
                    label_data,
                    test_size=config['data']['validation_split'],
                    random_state=split_seed
                )
            else:
                # ä½¿ç”¨å¯é…ç½®çš„éšæœºç§å­è¿›è¡Œé‡‡æ ·
                split_seed = config['data'].get('split_random_seed', 42)
                label_data_shuffled = label_data.sample(n=len(label_data), random_state=split_seed).reset_index(drop=True)

                # æŒ‰å›ºå®šæ•°é‡åˆ†å‰²ï¼šè®­ç»ƒé›†ä»å‰é¢å–ï¼ŒéªŒè¯é›†ä»åé¢å–
                train_count = config['data']['train_samples_per_label']
                val_count = config['data']['val_samples_per_label']

                label_train = label_data_shuffled[:train_count]  # å‰Nä¸ªä½œä¸ºè®­ç»ƒé›†
                label_val = label_data_shuffled[-val_count:]     # åMä¸ªä½œä¸ºéªŒè¯é›†

            df_train_list.append(label_train)
            df_val_list.append(label_val)
            print(f"   æ ‡ç­¾ '{label}': {len(label_train)} è®­ç»ƒ, {len(label_val)} éªŒè¯")

        df_train_prelim = pd.concat(df_train_list, ignore_index=True)
        df_val = pd.concat(df_val_list, ignore_index=True)

        print(f" å›ºå®šæ•°é‡åˆ†å‰²å®Œæˆ: è®­ç»ƒé›† {len(df_train_prelim)} æ¡, éªŒè¯é›† {len(df_val)} æ¡")
    else:
        # ğŸ“Š æ¯”ä¾‹åˆ†å‰²ï¼ˆæœ€åŸå§‹çš„é€»è¾‘ï¼‰
        print(" ä½¿ç”¨æ¯”ä¾‹åˆ†å‰²ï¼Œç¡®ä¿å®éªŒé—´æ•°æ®ä¸€è‡´æ€§...")
        split_seed = config['data'].get('split_random_seed', 42)
        df_train_prelim, df_val = train_test_split(
            df_train_full,
            test_size=config['data']['validation_split'],
            random_state=split_seed,  # ä½¿ç”¨å¯é…ç½®çš„ç§å­
            stratify=df_train_full['label']
        )

    # æ•°æ®é‡‡æ ·ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if hyperparams['data_fraction'] < 1.0:
        df_train = df_train_prelim.groupby('label', group_keys=False).apply(
            lambda x: x.sample(frac=hyperparams['data_fraction'], random_state=hyperparams['seed'])
        ).reset_index(drop=True)
    else:
        df_train = df_train_prelim.reset_index(drop=True)

    #  å…³é”®ä¿®æ”¹ï¼šåœ¨æ•°æ®åˆ†å‰²åè®¾ç½®å®éªŒéšæœºç§å­
    print(f" è®¾ç½®å®éªŒéšæœºç§å­ {hyperparams['seed']} (å½±å“æ¨¡å‹åˆå§‹åŒ–å’Œè®­ç»ƒè¿‡ç¨‹)...")
    set_seed(hyperparams['seed'])

    # ç”Ÿæˆæ ‡ç­¾æ˜ å°„
    unique_labels = sorted(df_train_full['label'].unique())
    label_to_id = {label: i for i, label in enumerate(unique_labels)}
    id_to_label = {i: label for label, i in label_to_id.items()}
    num_labels = len(unique_labels)

    # åŠ è½½é¢„è®­ç»ƒç¼–ç å™¨
    base_encoder, tokenizer, _ = load_pretrained_encoder(hyperparams['checkpoint_path'])
    if base_encoder is None:
        return None

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # ç¡®ä¿ç¼–ç å™¨åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
    base_encoder = base_encoder.to(device)

    # æ ¹æ®æ˜¯å¦ä½¿ç”¨ç¼“å­˜é€‰æ‹©ä¸åŒçš„å¤„ç†è·¯å¾„
    if use_cache:
        print("\n ä½¿ç”¨ç¼“å­˜ä¼˜åŒ–æ¨¡å¼...")

        # ä¸ºæ•°æ®é›†ç”Ÿæˆç¼“å­˜ - ä¿å­˜åœ¨å®éªŒç›®å½•ä¸‹
        if experiment_output_dir:
            cache_dir = os.path.join(experiment_output_dir, 'encoder_cache')
        else:
            cache_dir = config['optimization']['cache_dir']

        #  ä¿®å¤ï¼šåŸºäºå®Œæ•´æ•°æ®é›†ç”Ÿæˆç¼“å­˜é”®ï¼ŒåŒ…å«è½®æ¬¡ä¿¡æ¯
        # âœ… æ–°å¢ï¼šæ ¹æ®é‡‡æ ·ç­–ç•¥ç”Ÿæˆä¸åŒçš„ç¼“å­˜é”®
        if use_new_sampling:
            # æ–°ç­–ç•¥ï¼šè®­ç»ƒé›†ä»è®­ç»ƒæ–‡ä»¶ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†ä»æµ‹è¯•æ–‡ä»¶ï¼ˆå·²é‡‡æ ·ï¼‰
            train_cache_key = generate_cache_key(hyperparams['model_name'], df_train_prelim['content'].tolist(), 1.0, 256, round_num)
            val_cache_key = generate_cache_key(hyperparams['model_name'], df_val['content'].tolist(), 1.0, 256, round_num)  # éªŒè¯é›†å·²é‡‡æ ·
            test_cache_key = generate_cache_key(hyperparams['model_name'], df_test['content'].tolist(), 1.0, 256, round_num)  # æµ‹è¯•é›†å·²é‡‡æ ·
        else:
            # æ—§ç­–ç•¥ï¼šè®­ç»ƒé›†å’ŒéªŒè¯é›†ä»è®­ç»ƒæ–‡ä»¶ï¼Œæµ‹è¯•é›†ä½¿ç”¨å®Œæ•´æµ‹è¯•æ–‡ä»¶
            train_cache_key = generate_cache_key(hyperparams['model_name'], df_train_prelim['content'].tolist(), 1.0, 256, round_num)  # åŸºäºå®Œæ•´è®­ç»ƒé›†
            val_cache_key = generate_cache_key(hyperparams['model_name'], df_val['content'].tolist(), 1.0, 256, round_num)  # éªŒè¯é›†å·²åˆ†å‰²
            test_cache_key = generate_cache_key(hyperparams['model_name'], df_test['content'].tolist(), 1.0, 256, round_num)  # æµ‹è¯•é›†ä½¿ç”¨å…¨éƒ¨

        # åŠ è½½æˆ–åˆ›å»ºç¼“å­˜ï¼ˆåŸºäºå®Œæ•´æ•°æ®é›†ï¼‰
        train_features_full = load_or_create_cache(train_cache_key, cache_dir, df_train_prelim['content'].tolist(),
                                                  base_encoder, tokenizer, device, config)
        val_features = load_or_create_cache(val_cache_key, cache_dir, df_val['content'].tolist(),
                                          base_encoder, tokenizer, device, config)
        test_features = load_or_create_cache(test_cache_key, cache_dir, df_test['content'].tolist(),
                                           base_encoder, tokenizer, device, config)

        #  å…³é”®ä¿®å¤ï¼šæ ¹æ®é‡‡æ ·åçš„è®­ç»ƒé›†é€‰æ‹©å¯¹åº”çš„ç‰¹å¾
        if hyperparams['data_fraction'] < 1.0:
            # è·å–é‡‡æ ·åè®­ç»ƒé›†åœ¨åŸå§‹è®­ç»ƒé›†ä¸­çš„ç´¢å¼•
            train_indices = df_train_prelim.index[df_train_prelim['content'].isin(df_train['content'])].tolist()
            train_features = train_features_full[train_indices]
            print(f" ä»å®Œæ•´è®­ç»ƒç‰¹å¾({train_features_full.shape[0]})ä¸­é€‰æ‹©é‡‡æ ·ç‰¹å¾({train_features.shape[0]})")
        else:
            train_features = train_features_full

        # åˆ›å»ºç¼“å­˜æ•°æ®é›†
        train_dataset = EncodedDataset(train_features, df_train['label'].tolist(), label_to_id)
        val_dataset = EncodedDataset(val_features, df_val['label'].tolist(), label_to_id)
        test_dataset = EncodedDataset(test_features, df_test['label'].tolist(), label_to_id)

        train_loader = DataLoader(train_dataset, batch_size=hyperparams['batch_size'], shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=hyperparams['batch_size'])
        test_loader = DataLoader(test_dataset, batch_size=hyperparams['batch_size'])

        # æ„å»ºç¼“å­˜æ¨¡å¼çš„ç›‘ç£æ¨¡å‹
        encoder_output_dim = train_features.shape[1]
        model = CachedSupervisedModel(
            encoder_output_dim=encoder_output_dim,
            num_labels=num_labels,
            classifier_type=hyperparams['classifier_type'],
            mlp_hidden_neurons=hyperparams['mlp_hidden_neurons'] if hyperparams['classifier_type'] == 'mlp' else 384
        ).to(device)

        # è®¾ç½®ä¼˜åŒ–å™¨
        optimizer = AdamW(model.parameters(), lr=hyperparams['learning_rate'])

        # å‡†å¤‡è®­ç»ƒ
        loss_fn = nn.CrossEntropyLoss()
        total_steps = len(train_loader) * hyperparams['epochs']
        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)

        best_val_f1 = -1
        best_model_state = None
        best_val_metrics = None
        best_train_loss = None
        best_epoch = None
        last_train_loss = None

        # è®­ç»ƒå¾ªç¯ï¼ˆç¼“å­˜æ¨¡å¼ï¼‰
        for epoch in range(hyperparams['epochs']):
            print(f"  Epoch {epoch + 1}/{hyperparams['epochs']}")
            train_loss = train_epoch_cached(model, train_loader, loss_fn, optimizer, device, scheduler)
            last_train_loss = train_loss
            val_metrics = evaluate_model_cached(model, val_loader, loss_fn, device, id_to_label)
            print(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f} | éªŒè¯F1: {val_metrics['f1_score']:.4f} | éªŒè¯å‡†ç¡®ç‡: {val_metrics['accuracy']:.4f}")

            if val_metrics['f1_score'] > best_val_f1:
                best_val_f1 = val_metrics['f1_score']
                best_model_state = model.state_dict()
                best_val_metrics = copy.deepcopy(val_metrics)
                best_train_loss = train_loss
                best_epoch = epoch + 1
                print(f"   æ–°çš„æœ€ä½³éªŒè¯F1åˆ†æ•°: {best_val_f1:.4f}")

        # âœ… åªåœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸Šè¯„ä¼°ï¼Œä¸è§¦åŠæµ‹è¯•é›†
        if best_model_state:
            model.load_state_dict(best_model_state)
        if best_val_metrics is None:
            best_val_metrics = evaluate_model_cached(model, val_loader, loss_fn, device, id_to_label)
        print(" ä½¿ç”¨æœ€ä½³æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œè¯„ä¼°...")
        train_metrics = evaluate_model_cached(model, train_loader, loss_fn, device, id_to_label)
        if best_train_loss is None:
            best_train_loss = last_train_loss
        if best_epoch is None:
            best_epoch = hyperparams['epochs']

        # âœ… æš‚ä¸ä¿å­˜æ¨¡å‹ï¼Œç­‰å¾…å…¨å±€æœ€ä¼˜é€‰æ‹©åå†ä¿å­˜
        # è¿™æ ·å¯ä»¥èŠ‚çœç£ç›˜ç©ºé—´ï¼Œé¿å…ä¿å­˜æ‰€æœ‰éæœ€ä¼˜è¶…å‚æ•°ç»„åˆçš„æ¨¡å‹

    else:
        print("\n ä½¿ç”¨æ ‡å‡†æ¨¡å¼...")

        # æ ‡å‡†æ¨¡å¼ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        train_dataset = SupervisedTextDataset(df_train['content'].tolist(), df_train['label'].tolist(), tokenizer, label_to_id)
        val_dataset = SupervisedTextDataset(df_val['content'].tolist(), df_val['label'].tolist(), tokenizer, label_to_id)
        test_dataset = SupervisedTextDataset(df_test['content'].tolist(), df_test['label'].tolist(), tokenizer, label_to_id)

        train_loader = DataLoader(train_dataset, batch_size=hyperparams['batch_size'], shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=hyperparams['batch_size'])
        test_loader = DataLoader(test_dataset, batch_size=hyperparams['batch_size'])

        # æ„å»ºç›‘ç£æ¨¡å‹
        model = SupervisedModel(
            base_encoder=base_encoder,
            num_labels=num_labels,
            classifier_type=hyperparams['classifier_type'],
            mlp_hidden_neurons=hyperparams['mlp_hidden_neurons'] if hyperparams['classifier_type'] == 'mlp' else 384
        ).to(device)

        # è®¾ç½®ä¼˜åŒ–å™¨
        if hyperparams['freeze_encoder']:
            model.freeze_encoder()
            optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=hyperparams['learning_rate'])
        else:
            model.unfreeze_encoder()
            optimizer = AdamW(model.parameters(), lr=hyperparams['learning_rate'])

        # å‡†å¤‡è®­ç»ƒ
        loss_fn = nn.CrossEntropyLoss()
        total_steps = len(train_loader) * hyperparams['epochs']
        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)

        best_val_f1 = -1
        best_model_state = None
        best_val_metrics = None
        best_train_loss = None
        best_epoch = None
        last_train_loss = None

        # è®­ç»ƒå¾ªç¯
        for epoch in range(hyperparams['epochs']):
            print(f"  Epoch {epoch + 1}/{hyperparams['epochs']}")
            train_loss = train_epoch(model, train_loader, loss_fn, optimizer, device, scheduler)
            last_train_loss = train_loss
            val_metrics = evaluate_model(model, val_loader, loss_fn, device, id_to_label)
            print(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f} | éªŒè¯F1: {val_metrics['f1_score']:.4f} | éªŒè¯å‡†ç¡®ç‡: {val_metrics['accuracy']:.4f}")

            if val_metrics['f1_score'] > best_val_f1:
                best_val_f1 = val_metrics['f1_score']
                best_model_state = model.state_dict()
                best_val_metrics = copy.deepcopy(val_metrics)
                best_train_loss = train_loss
                best_epoch = epoch + 1
                print(f"   æ–°çš„æœ€ä½³éªŒè¯F1åˆ†æ•°: {best_val_f1:.4f}")

        # âœ… åªåœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸Šè¯„ä¼°ï¼Œä¸è§¦åŠæµ‹è¯•é›†
        if best_model_state:
            model.load_state_dict(best_model_state)
        if best_val_metrics is None:
            best_val_metrics = evaluate_model(model, val_loader, loss_fn, device, id_to_label)
        print(" ä½¿ç”¨æœ€ä½³æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œè¯„ä¼°...")
        train_metrics = evaluate_model(model, train_loader, loss_fn, device, id_to_label)
        if best_train_loss is None:
            best_train_loss = last_train_loss
        if best_epoch is None:
            best_epoch = hyperparams['epochs']

        # âœ… æš‚ä¸ä¿å­˜æ¨¡å‹ï¼Œç­‰å¾…å…¨å±€æœ€ä¼˜é€‰æ‹©åå†ä¿å­˜
        # è¿™æ ·å¯ä»¥èŠ‚çœç£ç›˜ç©ºé—´ï¼Œé¿å…ä¿å­˜æ‰€æœ‰éæœ€ä¼˜è¶…å‚æ•°ç»„åˆçš„æ¨¡å‹

    print(f"  éªŒè¯é›†æœ€ä½³ç»“æœ (Epoch {best_epoch}):")
    print(f"    éªŒè¯é›†F1: {best_val_f1:.4f}")
    print(f"    éªŒè¯é›†å‡†ç¡®ç‡: {best_val_metrics['accuracy']:.4f}")
    print(f"  âš ï¸  æµ‹è¯•é›†è¯„ä¼°å°†åœ¨é€‰å‡ºå…¨å±€æœ€ä¼˜è¶…å‚æ•°åè¿›è¡Œ")
    print(f"  âš ï¸  æ¨¡å‹å°†åœ¨é€‰å‡ºå…¨å±€æœ€ä¼˜åä¿å­˜")

    # æ·»åŠ è¶…å‚æ•°ä¿¡æ¯åˆ°ç»“æœä¸­ï¼ˆä¸å«æµ‹è¯•é›†æŒ‡æ ‡ï¼Œä¸å«model_save_pathï¼‰
    result = {
        'hyperparameters': hyperparams,
        'metrics': None,  # âœ… æµ‹è¯•é›†æŒ‡æ ‡æš‚æ—¶ä¸ºNone
        'metrics_train': train_metrics,
        'metrics_val': best_val_metrics,
        'best_val_f1': best_val_f1,
        'best_epoch': best_epoch,
        'train_loss_at_best': best_train_loss,
        'used_cache': use_cache,
        'model_save_path': None,  # âœ… æš‚æ—¶æ²¡æœ‰ä¿å­˜è·¯å¾„
        'model_state_dict': best_model_state,  # âœ… ä¿å­˜state_dictä¾›åç»­ä¿å­˜å’Œæµ‹è¯•é›†è¯„ä¼°
        'test_loader': test_loader,  # âœ… ä¿å­˜test_loaderä¾›åç»­è¯„ä¼°
        'loss_fn': loss_fn,  # âœ… ä¿å­˜loss_fn
        'device': device,  # âœ… ä¿å­˜device
        'id_to_label': id_to_label,  # âœ… ä¿å­˜id_to_label
        'use_cache': use_cache  # âœ… æ ‡è®°æ˜¯å¦ä½¿ç”¨ç¼“å­˜æ¨¡å¼
    }

    return result

def extract_best_results_by_model_and_fraction(results):
    """æå–æ¯ä¸ªæ¨¡å‹åœ¨æ¯ä¸ªæ•°æ®æ¯”ä¾‹ä¸‹çš„æœ€ä¼˜ç»“æœï¼ˆåŸºäºéªŒè¯é›†F1ï¼‰"""
    best_results = {}

    for result in results:
        if result is None:
            continue

        hyperparams = result['hyperparameters']
        # âœ… æ”¹ä¸ºåŸºäºéªŒè¯é›†F1é€‰æ‹©ï¼Œä¸è¿­ä»£æ¥å£ä¿æŒä¸€è‡´
        val_f1 = result.get('best_val_f1', float('-inf'))

        model_name = hyperparams['model_name']
        data_fraction = hyperparams['data_fraction']

        key = (model_name, data_fraction)

        if key not in best_results or val_f1 > best_results[key].get('best_val_f1', float('-inf')):
            best_results[key] = result

    return best_results

def generate_model_comparison_analysis(best_results):
    """ç”Ÿæˆæ¨¡å‹å¯¹æ¯”åˆ†æ"""
    # æŒ‰æ¨¡å‹å’Œæ•°æ®æ¯”ä¾‹ç»„ç»‡ç»“æœ
    comparison_data = {}
    model_names = set()
    data_fractions = set()

    for (model_name, data_fraction), result in best_results.items():
        model_names.add(model_name)
        data_fractions.add(data_fraction)

        if model_name not in comparison_data:
            comparison_data[model_name] = {}

        # âœ… å®‰å…¨åœ°å¤„ç†å¯èƒ½ä¸ºNoneçš„æµ‹è¯•é›†æŒ‡æ ‡
        test_metrics = result.get('metrics')
        comparison_data[model_name][data_fraction] = {
            'metrics': test_metrics,
            'per_class_metrics': test_metrics.get('per_class_metrics', {}) if test_metrics else {},
            'hyperparams': result['hyperparameters'],
            'best_val_f1': result['best_val_f1']
        }

    model_names = sorted(model_names)
    data_fractions = sorted(data_fractions, reverse=True)

    # ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼æ•°æ®
    comparison_table = {
        'models': model_names,
        'data_fractions': data_fractions,
        'results': {}
    }

    for fraction in data_fractions:
        comparison_table['results'][fraction] = {}
        for model in model_names:
            if model in comparison_data and fraction in comparison_data[model]:
                model_result = comparison_data[model][fraction]
                test_metrics = model_result['metrics']

                # âœ… åªæœ‰å½“æµ‹è¯•é›†æŒ‡æ ‡å­˜åœ¨æ—¶æ‰æ·»åŠ 
                if test_metrics:
                    comparison_table['results'][fraction][model] = {
                        'accuracy': round(test_metrics['accuracy'], 4),
                        'f1_score': round(test_metrics['f1_score'], 4),
                        'precision': round(test_metrics['precision'], 4),
                        'recall': round(test_metrics['recall'], 4),
                        'f1_micro': round(test_metrics.get('f1_micro', 0), 4),
                        'per_class_metrics': test_metrics.get('per_class_metrics', {}),
                        'best_hyperparams': {
                            'learning_rate': model_result['hyperparams']['learning_rate'],
                            'batch_size': model_result['hyperparams']['batch_size'],
                            'epochs': model_result['hyperparams']['epochs'],
                            'seed': model_result['hyperparams']['seed']
                        }
                    }
                else:
                    # æµ‹è¯•é›†æŒ‡æ ‡å°šæœªè¯„ä¼°
                    comparison_table['results'][fraction][model] = {
                        'note': 'æµ‹è¯•é›†æŒ‡æ ‡å°šæœªè¯„ä¼°',
                        'validation_f1': model_result['best_val_f1'],
                        'best_hyperparams': {
                            'learning_rate': model_result['hyperparams']['learning_rate'],
                            'batch_size': model_result['hyperparams']['batch_size'],
                            'epochs': model_result['hyperparams']['epochs'],
                            'seed': model_result['hyperparams']['seed']
                        }
                    }
            else:
                comparison_table['results'][fraction][model] = None

    return comparison_table, comparison_data

def group_results_by_model_fraction(results):
    """æŒ‰(æ¨¡å‹,æ•°æ®æ¯”ä¾‹)åˆ†ç»„ä¿å­˜æ‰€æœ‰ç§å­çš„ç»“æœ"""
    grouped = {}

    for result in results:
        if result is None:
            continue

        hyperparams = result['hyperparameters']
        key = f"{hyperparams['model_name']}_frac{hyperparams['data_fraction']}"

        if key not in grouped:
            grouped[key] = []

        # âœ… å®‰å…¨åœ°å¤„ç†å¯èƒ½ä¸ºNoneçš„æµ‹è¯•é›†æŒ‡æ ‡å’Œæ¨¡å‹è·¯å¾„
        test_metrics = result.get('metrics')
        model_path = result.get('model_save_path')
        grouped[key].append({
            'seed': hyperparams['seed'],
            'test_f1': test_metrics['f1_score'] if test_metrics else None,
            'test_accuracy': test_metrics['accuracy'] if test_metrics else None,
            'test_precision': test_metrics['precision'] if test_metrics else None,
            'test_recall': test_metrics['recall'] if test_metrics else None,
            'test_f1_micro': test_metrics.get('f1_micro', 0) if test_metrics else None,
            'test_metrics': test_metrics,
            'train_metrics': result.get('metrics_train'),
            'val_metrics': result.get('metrics_val'),
            'per_class_metrics': test_metrics.get('per_class_metrics', {}) if test_metrics else {},
            'val_f1': result['best_val_f1'],
            'best_epoch': result.get('best_epoch'),
            'train_loss_at_best': result.get('train_loss_at_best'),
            'model_path': model_path,  # âœ… åªæœ‰å…¨å±€æœ€ä¼˜æ¨¡å‹æœ‰è·¯å¾„ï¼Œå…¶ä½™ä¸ºNone
            'hyperparameters': hyperparams
        })

    return grouped

def save_experiment_results(results, config, output_dir=None):
    """ä¿å­˜å®éªŒç»“æœ - é‡æ–°ç»„ç»‡çš„æ–‡ä»¶å¤¹ç»“æ„"""
    # åˆ›å»ºå®éªŒç‰¹å®šçš„è¾“å‡ºç›®å½•
    if output_dir:
        # ä½¿ç”¨ä¼ å…¥çš„output_dirï¼ˆç”¨äºiterative_mainï¼‰
        output_dir = output_dir
    else:
        # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤ç›®å½•ï¼ˆç”¨äºç‹¬ç«‹è¿è¡Œï¼‰
        experiment_id = config['experiment_meta']['description']
        output_dir = os.path.join(config['experiment']['base_output_dir'], experiment_id)

    os.makedirs(output_dir, exist_ok=True)

    # åˆ›å»ºä¸¤ä¸ªå­æ–‡ä»¶å¤¹
    best_results_dir = os.path.join(output_dir, 'best_results_comparison')
    detailed_results_dir = os.path.join(output_dir, 'detailed_results')
    os.makedirs(best_results_dir, exist_ok=True)
    os.makedirs(detailed_results_dir, exist_ok=True)

    # ä¿å­˜å®éªŒä¿¡æ¯åˆ°ä¸»ç›®å½•
    if config['experiment']['save_experiment_info']:
        experiment_info = {
            'experiment_meta': config['experiment_meta'],
            'data_config': config['data'],
            'models': config['models'],
            'hyperparameters': config['hyperparameters'],
            'total_experiments': len(results),
            'successful_experiments': sum(1 for r in results if r is not None),
            'timestamp': pd.Timestamp.now().isoformat()
        }

        info_filepath = os.path.join(output_dir, 'experiment_info.json')
        with open(info_filepath, 'w', encoding='utf-8') as f:
            json.dump(experiment_info, f, ensure_ascii=False, indent=4)
        print(f" å®éªŒä¿¡æ¯å·²ä¿å­˜åˆ°: {info_filepath}")

    # ä¿å­˜è¯¦ç»†ç»“æœåˆ° detailed_results æ–‡ä»¶å¤¹
    if config['experiment']['save_individual_results']:
        print(" ä¿å­˜è¯¦ç»†å®éªŒç»“æœ...")
        for i, result in enumerate(results):
            if result is None:
                continue
            hyperparams = result['hyperparameters']
            filename = (
                f"{hyperparams['model_name']}_{hyperparams['classifier_type']}"
            )
            if hyperparams['classifier_type'] == 'mlp':
                filename += f"_{hyperparams['mlp_hidden_neurons']}neurons"
            filename += (
                f"_freeze{hyperparams['freeze_encoder']}_"
                f"ep{hyperparams['epochs']}_bs{hyperparams['batch_size']}_"
                f"lr{hyperparams['learning_rate']}_"
                f"frac{int(hyperparams['data_fraction']*100)}_"
                f"seed{hyperparams['seed']}.json"
            )
            filepath = os.path.join(detailed_results_dir, filename)

            # âœ… åˆ›å»ºJSONå¯åºåˆ—åŒ–çš„å‰¯æœ¬ï¼ˆç§»é™¤state_dictç­‰ä¸å¯åºåˆ—åŒ–å¯¹è±¡ï¼‰
            result_for_json = {
                'hyperparameters': result['hyperparameters'],
                'metrics': result.get('metrics'),
                'metrics_train': result.get('metrics_train'),
                'metrics_val': result.get('metrics_val'),
                'best_val_f1': result.get('best_val_f1'),
                'best_epoch': result.get('best_epoch'),
                'train_loss_at_best': result.get('train_loss_at_best'),
                'used_cache': result.get('used_cache'),
                'model_save_path': result.get('model_save_path'),
                # ä¸ä¿å­˜: model_state_dict, test_loader, loss_fn, device, id_to_label
            }

            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(result_for_json, f, ensure_ascii=False, indent=4)

        print(f" è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {detailed_results_dir}")

    #  æ–°å¢: ä¿å­˜æ‰€æœ‰ç§å­çš„å®Œæ•´ç»“æœ
    print(" ä¿å­˜æ‰€æœ‰ç§å­çš„å®Œæ•´ç»“æœ...")
    all_seeds_results = group_results_by_model_fraction(results)

    all_seeds_path = os.path.join(output_dir, 'all_seeds_results.json')
    with open(all_seeds_path, 'w', encoding='utf-8') as f:
        json.dump(all_seeds_results, f, ensure_ascii=False, indent=4)
    print(f" æ‰€æœ‰ç§å­ç»“æœå·²ä¿å­˜åˆ°: {all_seeds_path}")

    # æå–æœ€ä¼˜ç»“æœå¹¶ç”Ÿæˆå¯¹æ¯”åˆ†æ
    print(" æå–æœ€ä¼˜ç»“æœå¹¶ç”Ÿæˆå¯¹æ¯”åˆ†æ...")
    best_results = extract_best_results_by_model_and_fraction(results)
    comparison_table, comparison_data = generate_model_comparison_analysis(best_results)

    # ä¿å­˜æ¨¡å‹å¯¹æ¯”ç»“æœåˆ° best_results_comparison æ–‡ä»¶å¤¹
    model_comparison_path = os.path.join(best_results_dir, 'model_comparison_by_data_fraction.json')
    with open(model_comparison_path, 'w', encoding='utf-8') as f:
        json.dump(comparison_table, f, ensure_ascii=False, indent=4)
    print(f" æ¨¡å‹å¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: {model_comparison_path}")

    # ä¿å­˜æœ€ä¼˜è¶…å‚æ•°é…ç½®
    best_hyperparams = {}
    for (model_name, data_fraction), result in best_results.items():
        if model_name not in best_hyperparams:
            best_hyperparams[model_name] = {}

        # âœ… å®‰å…¨åœ°å¤„ç†å¯èƒ½ä¸ºNoneçš„æµ‹è¯•é›†æŒ‡æ ‡
        test_metrics = result.get('metrics')
        if test_metrics:
            best_hyperparams[model_name][data_fraction] = {
                'hyperparameters': result['hyperparameters'],
                'performance': {
                    'f1_score': test_metrics['f1_score'],
                    'accuracy': test_metrics['accuracy'],
                    'precision': test_metrics['precision'],
                    'recall': test_metrics['recall'],
                    'f1_micro': test_metrics.get('f1_micro', 0)
                },
                'per_class_metrics': test_metrics.get('per_class_metrics', {}),
                'validation_f1': result['best_val_f1']
            }
        else:
            # å¦‚æœæµ‹è¯•é›†æŒ‡æ ‡ä¸å¯ç”¨ï¼Œåªä¿å­˜éªŒè¯é›†ä¿¡æ¯
            best_hyperparams[model_name][data_fraction] = {
                'hyperparameters': result['hyperparameters'],
                'performance': None,  # æµ‹è¯•é›†æŒ‡æ ‡å°šæœªè¯„ä¼°
                'validation_f1': result['best_val_f1']
            }

    best_hyperparams_path = os.path.join(best_results_dir, 'best_hyperparams_by_model.json')
    with open(best_hyperparams_path, 'w', encoding='utf-8') as f:
        json.dump(best_hyperparams, f, ensure_ascii=False, indent=4)
    print(f"  æœ€ä¼˜è¶…å‚æ•°å·²ä¿å­˜åˆ°: {best_hyperparams_path}")

    # ç”Ÿæˆæ€§èƒ½åˆ†ææŠ¥å‘Š
    performance_analysis = {
        'summary': {
            'total_model_data_fraction_combinations': len(best_results),
            'models_tested': list(set(model for model, _ in best_results.keys())),
            'data_fractions_tested': sorted(list(set(fraction for _, fraction in best_results.keys())), reverse=True)
        },
        'best_overall_performance': {},
        'performance_trends': {}
    }

    # âœ… åªæœ‰å½“æ‰€æœ‰ç»“æœéƒ½æœ‰æµ‹è¯•é›†æŒ‡æ ‡æ—¶æ‰ç”Ÿæˆæ€§èƒ½åˆ†æ
    all_have_test_metrics = all(r.get('metrics') is not None for r in best_results.values())

    if all_have_test_metrics:
        # æ‰¾å‡ºæ¯ä¸ªæŒ‡æ ‡çš„å…¨å±€æœ€ä¼˜ç»“æœ
        for metric in ['accuracy', 'f1_score', 'precision', 'recall']:
            best_result = max(best_results.values(), key=lambda x: x['metrics'][metric])
            performance_analysis['best_overall_performance'][metric] = {
                'value': best_result['metrics'][metric],
                'model': best_result['hyperparameters']['model_name'],
                'data_fraction': best_result['hyperparameters']['data_fraction'],
                'hyperparameters': best_result['hyperparameters'],
                'per_class_metrics': best_result['metrics'].get('per_class_metrics', {})
            }

        # åˆ†ææ€§èƒ½è¶‹åŠ¿
        for model_name in performance_analysis['summary']['models_tested']:
            model_results = [(fraction, result) for (model, fraction), result in best_results.items() if model == model_name]
            model_results.sort(key=lambda x: x[0], reverse=True)  # æŒ‰æ•°æ®æ¯”ä¾‹é™åºæ’åˆ—

            performance_analysis['performance_trends'][model_name] = {
                'f1_scores_by_fraction': [(fraction, result['metrics']['f1_score']) for fraction, result in model_results],
                'accuracy_by_fraction': [(fraction, result['metrics']['accuracy']) for fraction, result in model_results]
            }
    else:
        performance_analysis['note'] = 'æµ‹è¯•é›†æŒ‡æ ‡å°šæœªå…¨éƒ¨è¯„ä¼°ï¼Œæ€§èƒ½åˆ†æåŸºäºéªŒè¯é›†'
        # åŸºäºéªŒè¯é›†ç”Ÿæˆç®€åŒ–çš„åˆ†æ
        best_val_result = max(best_results.values(), key=lambda x: x['best_val_f1'])
        performance_analysis['best_validation_performance'] = {
            'validation_f1': best_val_result['best_val_f1'],
            'model': best_val_result['hyperparameters']['model_name'],
            'data_fraction': best_val_result['hyperparameters']['data_fraction']
        }

    performance_analysis_path = os.path.join(best_results_dir, 'performance_analysis.json')
    with open(performance_analysis_path, 'w', encoding='utf-8') as f:
        json.dump(performance_analysis, f, ensure_ascii=False, indent=4)
    print(f" æ€§èƒ½åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {performance_analysis_path}")

    # ä¼ ç»Ÿçš„èšåˆç»“æœåˆ†æ - ä¿å­˜åˆ°è¯¦ç»†ç»“æœæ–‡ä»¶å¤¹
    # âš ï¸  æ³¨æ„ï¼šæ­¤èšåˆåˆ†æåªåŒ…å«æœ‰æµ‹è¯•é›†æŒ‡æ ‡çš„ç»“æœï¼ˆå³å…¨å±€æœ€ä¼˜çš„è¶…å‚æ•°ç»„åˆï¼‰
    if config['experiment']['aggregate_results']:
        print(" ç”Ÿæˆä¼ ç»Ÿèšåˆåˆ†æ...")
        aggregate_results = {}
        skipped_count = 0  # è®°å½•è·³è¿‡çš„ç»“æœæ•°é‡

        for result in results:
            if result is None:
                continue

            hyperparams = result['hyperparameters']
            metrics = result.get('metrics')  # âœ… ä½¿ç”¨ get() é¿å… KeyError

            # âœ… è·³è¿‡æ²¡æœ‰æµ‹è¯•é›†æŒ‡æ ‡çš„ç»“æœï¼ˆåªæœ‰å…¨å±€æœ€ä¼˜æœ‰æµ‹è¯•é›†æŒ‡æ ‡ï¼‰
            if metrics is None:
                skipped_count += 1
                continue

            # åˆ›å»ºé…ç½®é”®
            config_key = (
                hyperparams['model_name'],
                hyperparams['classifier_type'],
                hyperparams['mlp_hidden_neurons'] if hyperparams['classifier_type'] == 'mlp' else 0,
                hyperparams['freeze_encoder'],
                hyperparams['epochs'],
                hyperparams['batch_size'],
                hyperparams['learning_rate'],
                hyperparams['data_fraction']
            )

            if config_key not in aggregate_results:
                aggregate_results[config_key] = []

            aggregate_results[config_key].append({
                'seed': hyperparams['seed'],
                'accuracy': metrics['accuracy'],
                'f1_score': metrics['f1_score'],
                'precision': metrics['precision'],
                'recall': metrics['recall'],
                'f1_micro': metrics.get('f1_micro', 0)
                # ç§»é™¤ per_class_metrics ä»¥é¿å… pandas mean() é”™è¯¯
            })

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        summary_results = {
            'experiment_meta': config['experiment_meta'],  # æ·»åŠ å®éªŒå…ƒä¿¡æ¯
            'results': {}
        }

        for config_key, runs in aggregate_results.items():
            if len(runs) > 0:
                df_runs = pd.DataFrame(runs)
                summary_results['results'][str(config_key)] = {
                    'config': {
                        'model_name': config_key[0],
                        'classifier_type': config_key[1],
                        'mlp_hidden_neurons': config_key[2],
                        'freeze_encoder': config_key[3],
                        'epochs': config_key[4],
                        'batch_size': config_key[5],
                        'learning_rate': config_key[6],
                        'data_fraction': config_key[7]
                    },
                    'mean_metrics': df_runs.mean().to_dict(),
                    'std_metrics': df_runs.std().to_dict(),
                    'num_runs': len(runs),
                    'all_runs': runs
                }

        # ä¿å­˜èšåˆç»“æœåˆ°è¯¦ç»†ç»“æœæ–‡ä»¶å¤¹
        summary_filepath = os.path.join(detailed_results_dir, 'hyperparameter_search_summary.json')
        with open(summary_filepath, 'w', encoding='utf-8') as f:
            json.dump(summary_results, f, ensure_ascii=False, indent=4)

        if skipped_count > 0:
            print(f" ä¼ ç»Ÿèšåˆç»“æœå·²ä¿å­˜åˆ°: {summary_filepath}")
            print(f"   (æ³¨æ„: è·³è¿‡äº† {skipped_count} ä¸ªæ²¡æœ‰æµ‹è¯•é›†æŒ‡æ ‡çš„ç»“æœï¼ŒåªåŒ…å«å…¨å±€æœ€ä¼˜çš„é…ç½®)")
        else:
            print(f" ä¼ ç»Ÿèšåˆç»“æœå·²ä¿å­˜åˆ°: {summary_filepath}")

    print(f"\n æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
    print(f"    æœ€ä¼˜ç»“æœå¯¹æ¯”: {best_results_dir}")
    print(f"    è¯¦ç»†å®éªŒç»“æœ: {detailed_results_dir}")

    return output_dir


# --- 6. ä¸»å‡½æ•° ---

def run_supervised_training_interface(encoder_path: str, config: dict, output_dir: str, round_num: int = None) -> dict:
    """
    æ ‡å‡†åŒ–æ¥å£ï¼šè¿è¡Œç›‘ç£å­¦ä¹ è¶…å‚æ•°æœç´¢

    Args:
        encoder_path: ç¼–ç å™¨æ¨¡å‹è·¯å¾„
        config: ç›‘ç£å­¦ä¹ é…ç½®å­—å…¸
        output_dir: è¾“å‡ºç›®å½•

    Returns:
        åŒ…å«æœ€ä½³æ¨¡å‹è·¯å¾„ã€å®éªŒç›®å½•åŠå…³é”®æŒ‡æ ‡çš„å­—å…¸
    """
    import os
    import copy

    print(f" ç›‘ç£å­¦ä¹ æ¥å£è°ƒç”¨")
    print(f"   ç¼–ç å™¨: {encoder_path}")
    print(f"   è¾“å‡ºç›®å½•: {output_dir}")
    if round_num:
        print(f"   å®éªŒè½®æ¬¡: ç¬¬{round_num}è½®")

    global CONFIG
    original_config = CONFIG

    try:
        # å¤åˆ¶å…¨å±€CONFIGå¹¶ä¿®æ”¹
        config_copy = copy.deepcopy(CONFIG)

        # æ›´æ–°é…ç½®
        config_copy['experiment_meta']['description'] = 'iterative_supervised'
        config_copy['experiment']['base_output_dir'] = output_dir

        # ä½¿ç”¨æä¾›çš„ç¼–ç å™¨è·¯å¾„
        config_copy['models'] = {
            'iterative_encoder': encoder_path
        }

        # æ›´æ–°æ•°æ®é…ç½®
        if 'train_data_path' in config:
            config_copy['data']['train_data_path'] = config['train_data_path']
        if 'test_data_path' in config:
            config_copy['data']['test_data_path'] = config['test_data_path']

        # âœ… æ–°å¢ï¼šå›ºå®šåˆ†å‰²ç›¸å…³å‚æ•°
        if 'use_fixed_split' in config:
            config_copy['data']['use_fixed_split'] = config['use_fixed_split']
        if 'train_samples_per_label' in config:
            config_copy['data']['train_samples_per_label'] = config['train_samples_per_label']
        if 'val_samples_per_label' in config:
            config_copy['data']['val_samples_per_label'] = config['val_samples_per_label']
        if 'test_samples_per_label' in config:
            config_copy['data']['test_samples_per_label'] = config['test_samples_per_label']
        if 'use_test_for_val_and_test' in config:
            config_copy['data']['use_test_for_val_and_test'] = config['use_test_for_val_and_test']
        if 'split_random_seed' in config:
            config_copy['data']['split_random_seed'] = config['split_random_seed']

        # æ›´æ–°è¶…å‚æ•°
        if 'data_fractions' in config:
            config_copy['hyperparameters']['data_fractions'] = config['data_fractions']
        if 'epochs' in config:
            config_copy['hyperparameters']['epochs'] = config['epochs']
        if 'batch_size' in config:
            config_copy['hyperparameters']['batch_size'] = config['batch_size']
        if 'learning_rate' in config:
            # Convert learning_rate to float if it's a string (from YAML)
            lr = config['learning_rate']
            if isinstance(lr, list):
                config_copy['hyperparameters']['learning_rate'] = [float(x) for x in lr]
            else:
                config_copy['hyperparameters']['learning_rate'] = [float(lr)]
        # âœ… å•ç§å­æ¨¡å¼ï¼ˆè¿­ä»£è®­ç»ƒä¸“ç”¨ï¼‰
        if 'seeds' in config:
            config_copy['hyperparameters']['seeds'] = config['seeds']
        else:
            # é»˜è®¤ä½¿ç”¨å•ç§å­
            config_copy['hyperparameters']['seeds'] = [42]
            print(f"   [è¿­ä»£æ¨¡å¼] ä½¿ç”¨å•ç§å­: 42")

        if 'classifier_types' in config:
            config_copy['hyperparameters']['classifier_types'] = config['classifier_types']
        if 'mlp_hidden_neurons' in config:
            config_copy['hyperparameters']['mlp_hidden_neurons'] = config['mlp_hidden_neurons']
        if 'freeze_encoder' in config:
            config_copy['hyperparameters']['freeze_encoder'] = config['freeze_encoder']

        # ä½¿ç”¨æ›´æ–°åçš„é…ç½®è¿è¡Œå®éªŒ
        original_config = CONFIG
        CONFIG = config_copy

        # ç”Ÿæˆè¶…å‚æ•°ç»„åˆå¹¶è¿è¡Œ
        combinations = generate_hyperparameter_combinations(CONFIG)
        print(f" ç”Ÿæˆäº† {len(combinations)} ä¸ªè¶…å‚æ•°ç»„åˆ")

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)

        all_results = []
        for i, hyperparams in enumerate(combinations):
            print(f"è¿è¡Œç»„åˆ {i+1}/{len(combinations)}")
            result = run_single_experiment(CONFIG, hyperparams, output_dir, round_num)
            all_results.append(result)

        # âœ… å…³é”®ä¿®æ”¹ï¼šåŸºäºéªŒè¯é›†F1é€‰æ‹©æœ€ä¼˜æ¨¡å‹ï¼ˆè€Œéæµ‹è¯•é›†F1å‡å€¼ï¼‰
        print(f"\n   [è¿­ä»£æ¨¡å¼] åŸºäºéªŒè¯é›†F1é€‰æ‹©æœ€ä¼˜æ¨¡å‹...")
        best_run = None
        best_val_f1 = float('-inf')

        for run in all_results:
            if run is None:
                continue
            val_f1 = run.get('best_val_f1', float('-inf'))
            if val_f1 > best_val_f1:
                best_val_f1 = val_f1
                best_run = run

        if not best_run:
            raise RuntimeError("æœªè·å¾—æœ‰æ•ˆçš„ç›‘ç£å­¦ä¹ ç»“æœä¾›è¿­ä»£æµç¨‹ä½¿ç”¨")

        # âœ… æ£€æŸ¥æ˜¯å¦è·³è¿‡æµ‹è¯•é›†è¯„ä¼°ï¼ˆGrid Search æ¨¡å¼ï¼‰
        skip_test_eval = config.get('skip_test_eval', False)

        if skip_test_eval:
            print(f"\n   [Grid Search æ¨¡å¼] è·³è¿‡æµ‹è¯•é›†è¯„ä¼°")
            print(f"   æœ€ä½³è¶…å‚æ•°: epoch={best_run['hyperparameters']['epochs']}, "
                  f"lr={best_run['hyperparameters']['learning_rate']}, "
                  f"bs={best_run['hyperparameters']['batch_size']}, "
                  f"frac={best_run['hyperparameters']['data_fraction']}")
            print(f"   éªŒè¯é›†F1: {best_val_f1:.4f}")

            # è¿”å›ç»“æœï¼ˆä¸åŒ…å«æµ‹è¯•é›†æŒ‡æ ‡ï¼‰
            return {
                'best_model_path': None,  # Grid Search ä¸ä¿å­˜æ¨¡å‹
                'hyperparameters': best_run['hyperparameters'],
                'best_val_f1': best_val_f1,
                'best_epoch': best_run.get('best_epoch'),
                'train_loss_at_best': best_run.get('train_loss_at_best'),
                'metrics': {
                    'train': best_run.get('metrics_train', {}),
                    'dev': best_run.get('metrics_val', {}),  # âœ… ä½¿ç”¨ metrics_val
                    'test': {}  # ç©ºå­—å…¸ï¼Œè¡¨ç¤ºæœªè¯„ä¼°
                },
                'used_cache': best_run.get('use_cache', False)
            }

        # âœ… æ ‡å‡†æ¨¡å¼ï¼šå¯¹æœ€ä¼˜æ¨¡å‹è¯„ä¼°æµ‹è¯•é›†
        print(f"\n   [æµ‹è¯•é›†è¯„ä¼°] åªå¯¹æœ€ä¼˜è¶…å‚æ•°é…ç½®è¯„ä¼°æµ‹è¯•é›†...")
        print(f"   æœ€ä½³è¶…å‚æ•°: epoch={best_run['hyperparameters']['epochs']}, "
              f"lr={best_run['hyperparameters']['learning_rate']}, "
              f"bs={best_run['hyperparameters']['batch_size']}, "
              f"frac={best_run['hyperparameters']['data_fraction']}")
        print(f"   éªŒè¯é›†F1: {best_val_f1:.4f}")

        # åŠ è½½æœ€ä½³æ¨¡å‹å¹¶åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
        device = best_run['device']
        test_loader = best_run['test_loader']
        loss_fn = best_run['loss_fn']
        id_to_label = best_run['id_to_label']
        use_cache = best_run['use_cache']

        # é‡å»ºæ¨¡å‹ç»“æ„
        if use_cache:
            # ç¼“å­˜æ¨¡å¼ - éœ€è¦æ¨æ–­ç¼–ç å™¨è¾“å‡ºç»´åº¦
            # æ ¹æ®åˆ†ç±»å™¨ç±»å‹é€‰æ‹©æ­£ç¡®çš„æƒé‡é”®å
            classifier_type = best_run['hyperparameters']['classifier_type']
            state_dict = best_run['model_state_dict']

            if classifier_type == 'linear':
                # Linearåˆ†ç±»å™¨: classifier.weight çš„shapeæ˜¯ [num_labels, encoder_output_dim]
                encoder_output_dim = state_dict['classifier.weight'].shape[1]
            elif classifier_type == 'mlp':
                # MLPåˆ†ç±»å™¨: classifier.0.weight çš„shapeæ˜¯ [mlp_hidden_neurons, encoder_output_dim]
                encoder_output_dim = state_dict['classifier.0.weight'].shape[1]
            else:
                raise ValueError(f"æœªçŸ¥çš„åˆ†ç±»å™¨ç±»å‹: {classifier_type}")

            test_model = CachedSupervisedModel(
                encoder_output_dim=encoder_output_dim,
                num_labels=len(id_to_label),
                classifier_type=classifier_type,
                mlp_hidden_neurons=best_run['hyperparameters'].get('mlp_hidden_neurons', 384)
            ).to(device)
        else:
            # æ ‡å‡†æ¨¡å¼ - éœ€è¦é‡æ–°åŠ è½½ç¼–ç å™¨
            base_encoder, _, _ = load_pretrained_encoder(best_run['hyperparameters']['checkpoint_path'])
            test_model = SupervisedModel(
                base_encoder=base_encoder,
                num_labels=len(id_to_label),
                classifier_type=best_run['hyperparameters']['classifier_type'],
                mlp_hidden_neurons=best_run['hyperparameters'].get('mlp_hidden_neurons', 384)
            ).to(device)

        # åŠ è½½æœ€ä½³æƒé‡
        test_model.load_state_dict(best_run['model_state_dict'])

        # è¯„ä¼°æµ‹è¯•é›†
        if use_cache:
            test_metrics = evaluate_model_cached(test_model, test_loader, loss_fn, device, id_to_label)
        else:
            test_metrics = evaluate_model(test_model, test_loader, loss_fn, device, id_to_label)

        print(f"\n   [æµ‹è¯•é›†ç»“æœ]:")
        print(f"    å‡†ç¡®ç‡: {test_metrics['accuracy']:.4f}")
        print(f"    Macro F1: {test_metrics['f1_score']:.4f}")
        print(f"    Macro ç²¾ç¡®ç‡: {test_metrics['precision']:.4f}")
        print(f"    Macro å¬å›ç‡: {test_metrics['recall']:.4f}")

        # æ›´æ–°best_runçš„æµ‹è¯•é›†æŒ‡æ ‡
        best_run['metrics'] = test_metrics

        # âœ… åªä¿å­˜æœ€ä¼˜æ¨¡å‹ä¸€æ¬¡ï¼ˆåŒ…å«å®Œæ•´çš„æµ‹è¯•é›†æŒ‡æ ‡ï¼‰
        print(f"\n   [ä¿å­˜æœ€ä¼˜æ¨¡å‹] ä¿å­˜å…¨å±€æœ€ä¼˜è¶…å‚æ•°çš„æ¨¡å‹...")
        model_save_info = save_best_model_for_seed(
            test_model, best_run['model_state_dict'],
            best_run['hyperparameters'], best_val_f1,
            test_metrics, output_dir
        )
        best_model_path = model_save_info['model_path']
        best_run['model_save_path'] = best_model_path  # âœ… æ›´æ–°ä¿å­˜è·¯å¾„

        print(f"   æœ€ä¼˜æ¨¡å‹å·²ä¿å­˜åˆ°: {best_model_path}")

        # ä¿å­˜ç»“æœ - ç°åœ¨all_resultsä¸­çš„best_runåŒ…å«äº†æµ‹è¯•é›†æŒ‡æ ‡
        experiment_dir = save_experiment_results(all_results, CONFIG, output_dir)

        # âœ… è¿”å›å®Œæ•´çš„train/dev/testæŒ‡æ ‡
        interface_payload = {
            'experiment_dir': experiment_dir,
            'best_model_path': best_model_path,
            'metrics': {
                'train': best_run.get('metrics_train'),
                'dev': best_run.get('metrics_val'),
                'test': test_metrics
            },
            'best_epoch': best_run.get('best_epoch'),
            'train_loss_at_best': best_run.get('train_loss_at_best'),
            'hyperparameters': best_run.get('hyperparameters'),
            'used_cache': best_run.get('used_cache', False),
            'best_val_f1': best_val_f1  # âœ… æ–°å¢éªŒè¯é›†F1
        }

        print(f"\n   ç›‘ç£å­¦ä¹ å®Œæˆï¼Œç»“æœä¿å­˜åœ¨: {experiment_dir}")
        print(f"   æœ€ä½³æ¨¡å‹: {best_model_path}")

        return interface_payload

    except Exception as e:
        print(f" ç›‘ç£å­¦ä¹ å¤±è´¥: {e}")
        raise
    finally:
        CONFIG = original_config


if __name__ == '__main__':
    print(" å¼€å§‹è¶…å‚æ•°æœç´¢å®éªŒ...")
    print(f" å®éªŒåç§°: {CONFIG['experiment_meta']['experiment_name']}")
    print(f" å®éªŒç›®çš„: {CONFIG['experiment_meta']['purpose']}")
    print(f" å®éªŒå¤‡æ³¨: {CONFIG['experiment_meta']['notes']}")

    # ç”Ÿæˆæ‰€æœ‰è¶…å‚æ•°ç»„åˆ
    combinations = generate_hyperparameter_combinations(CONFIG)
    print(f" æ€»å…±éœ€è¦è¿è¡Œ {len(combinations)} ä¸ªå®éªŒé…ç½®")

    #  æ–°å¢ï¼šæå‰åˆ›å»ºå®éªŒç›®å½•
    experiment_id = CONFIG['experiment_meta']['description']
    experiment_output_dir = os.path.join(CONFIG['experiment']['base_output_dir'], experiment_id)
    os.makedirs(experiment_output_dir, exist_ok=True)
    print(f" å®éªŒè¾“å‡ºç›®å½•: {experiment_output_dir}")

    # è¿è¡Œæ‰€æœ‰å®éªŒ
    all_results = []
    for i, hyperparams in enumerate(combinations):
        print(f"\n{'='*80}")
        print(f"å®éªŒè¿›åº¦: {i+1}/{len(combinations)}")
        print(f"{'='*80}")

        # ä¸»å‡½æ•°è¿è¡Œæ—¶ä¸ä¼ round_numï¼Œåªåœ¨iterative_mainè°ƒç”¨æ—¶ä¼ 
        result = run_single_experiment(CONFIG, hyperparams, experiment_output_dir, None)
        all_results.append(result)

        # å¯é€‰ï¼šæ¯å®Œæˆå‡ ä¸ªå®éªŒå°±ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ
        if (i + 1) % 10 == 0:
            print(f"\n ä¿å­˜ä¸­é—´ç»“æœ... (å·²å®Œæˆ {i+1}/{len(combinations)} ä¸ªå®éªŒ)")
            # æ³¨æ„ï¼šä¸­é—´ç»“æœä¿å­˜æ—¶ï¼Œæµ‹è¯•é›†æŒ‡æ ‡ä¸ºNone
            experiment_dir = save_experiment_results(all_results, CONFIG)

    # âœ… é€‰æ‹©å…¨å±€æœ€ä¼˜æ¨¡å‹å¹¶è¯„ä¼°æµ‹è¯•é›†
    print(f"\n{'='*80}")
    print(" é€‰æ‹©å…¨å±€æœ€ä¼˜æ¨¡å‹...")
    print(f"{'='*80}")

    best_run = None
    best_val_f1 = float('-inf')

    for run in all_results:
        if run is None:
            continue
        val_f1 = run.get('best_val_f1', float('-inf'))
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            best_run = run

    if best_run:
        print(f"\n âœ… æ‰¾åˆ°å…¨å±€æœ€ä¼˜æ¨¡å‹:")
        print(f"    éªŒè¯é›†F1: {best_val_f1:.4f}")
        print(f"    æœ€ä½³è¶…å‚æ•°: epoch={best_run['hyperparameters']['epochs']}, "
              f"lr={best_run['hyperparameters']['learning_rate']}, "
              f"bs={best_run['hyperparameters']['batch_size']}, "
              f"frac={best_run['hyperparameters']['data_fraction']}, "
              f"seed={best_run['hyperparameters']['seed']}")

        # é‡å»ºæ¨¡å‹å¹¶è¯„ä¼°æµ‹è¯•é›†
        device = best_run['device']
        test_loader = best_run['test_loader']
        loss_fn = best_run['loss_fn']
        id_to_label = best_run['id_to_label']
        use_cache = best_run['use_cache']

        print(f"\n é‡å»ºæ¨¡å‹å¹¶è¯„ä¼°æµ‹è¯•é›†...")
        if use_cache:
            # ç¼“å­˜æ¨¡å¼ - éœ€è¦æ¨æ–­ç¼–ç å™¨è¾“å‡ºç»´åº¦
            # æ ¹æ®åˆ†ç±»å™¨ç±»å‹é€‰æ‹©æ­£ç¡®çš„æƒé‡é”®å
            classifier_type = best_run['hyperparameters']['classifier_type']
            state_dict = best_run['model_state_dict']

            if classifier_type == 'linear':
                # Linearåˆ†ç±»å™¨: classifier.weight çš„shapeæ˜¯ [num_labels, encoder_output_dim]
                encoder_output_dim = state_dict['classifier.weight'].shape[1]
            elif classifier_type == 'mlp':
                # MLPåˆ†ç±»å™¨: classifier.0.weight çš„shapeæ˜¯ [mlp_hidden_neurons, encoder_output_dim]
                encoder_output_dim = state_dict['classifier.0.weight'].shape[1]
            else:
                raise ValueError(f"æœªçŸ¥çš„åˆ†ç±»å™¨ç±»å‹: {classifier_type}")

            test_model = CachedSupervisedModel(
                encoder_output_dim=encoder_output_dim,
                num_labels=len(id_to_label),
                classifier_type=classifier_type,
                mlp_hidden_neurons=best_run['hyperparameters'].get('mlp_hidden_neurons', 384)
            ).to(device)
        else:
            base_encoder, _, _ = load_pretrained_encoder(best_run['hyperparameters']['checkpoint_path'])
            test_model = SupervisedModel(
                base_encoder=base_encoder,
                num_labels=len(id_to_label),
                classifier_type=best_run['hyperparameters']['classifier_type'],
                mlp_hidden_neurons=best_run['hyperparameters'].get('mlp_hidden_neurons', 384)
            ).to(device)

        test_model.load_state_dict(best_run['model_state_dict'])

        # è¯„ä¼°æµ‹è¯•é›†
        if use_cache:
            test_metrics = evaluate_model_cached(test_model, test_loader, loss_fn, device, id_to_label)
        else:
            test_metrics = evaluate_model(test_model, test_loader, loss_fn, device, id_to_label)

        print(f"\n âœ… æµ‹è¯•é›†ç»“æœ:")
        print(f"    å‡†ç¡®ç‡: {test_metrics['accuracy']:.4f}")
        print(f"    Macro F1: {test_metrics['f1_score']:.4f}")
        print(f"    Macro ç²¾ç¡®ç‡: {test_metrics['precision']:.4f}")
        print(f"    Macro å¬å›ç‡: {test_metrics['recall']:.4f}")

        # æ›´æ–°best_runçš„æµ‹è¯•é›†æŒ‡æ ‡
        best_run['metrics'] = test_metrics

        # ä¿å­˜æœ€ä¼˜æ¨¡å‹
        print(f"\n ä¿å­˜å…¨å±€æœ€ä¼˜æ¨¡å‹...")
        model_save_info = save_best_model_for_seed(
            test_model, best_run['model_state_dict'],
            best_run['hyperparameters'], best_val_f1,
            test_metrics, experiment_output_dir
        )
        best_run['model_save_path'] = model_save_info['model_path']
        print(f" æœ€ä¼˜æ¨¡å‹å·²ä¿å­˜åˆ°: {model_save_info['model_path']}")
    else:
        print(f"\n âš ï¸  æœªæ‰¾åˆ°æœ‰æ•ˆçš„å®éªŒç»“æœ")

    # ä¿å­˜æœ€ç»ˆç»“æœï¼ˆåŒ…å«æµ‹è¯•é›†æŒ‡æ ‡ï¼‰
    print(f"\n ä¿å­˜æœ€ç»ˆå®éªŒç»“æœ...")
    experiment_dir = save_experiment_results(all_results, CONFIG)

    print(f"\n æ‰€æœ‰è¶…å‚æ•°æœç´¢å®éªŒå·²å®Œæˆï¼")
    print(f" ç»“æœä¿å­˜åœ¨: {experiment_dir}")
    print(f" å®éªŒæè¿°: {CONFIG['experiment_meta']['description']}")
    if best_run and best_run.get('model_save_path'):
        print(f" æœ€ä¼˜æ¨¡å‹: {best_run['model_save_path']}")

# --- å…¶ä»–å®éªŒé…ç½®ç¤ºä¾‹ ---

# ä½ å¯ä»¥å¤åˆ¶ä»¥ä¸‹é…ç½®ç¤ºä¾‹ï¼Œä¿®æ”¹experiment_metaéƒ¨åˆ†ï¼Œè¿›è¡Œä¸åŒçš„å®éªŒå¯¹æ¯”

EXPERIMENT_CONFIGS = {
    "learning_rate_comparison": {
        'experiment_meta': {
            'description': 'learning_rate_comparison',
            'experiment_name': 'å­¦ä¹ ç‡å¯¹æ¯”å®éªŒ',
            'purpose': 'æµ‹è¯•ä¸åŒå­¦ä¹ ç‡å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“',
            'notes': 'å›ºå®šå…¶ä»–å‚æ•°ï¼Œå¯¹æ¯”1e-3, 2e-3, 5e-3ä¸‰ç§å­¦ä¹ ç‡',
        },
        'hyperparameters': {
            'epochs': [20],
            'batch_size': [32],
            'learning_rate': [1e-3, 2e-3, 5e-3],
            'data_fractions': [1.0],
            'seeds': [42, 123, 456],
            'classifier_types': ['linear'],
            'mlp_layers': [1],  # linearåˆ†ç±»å™¨æ—¶MLPå±‚æ•°æ— æ„ä¹‰
            'freeze_encoder': [True],
        },
    },

    "data_efficiency": {
        'experiment_meta': {
            'description': 'data_efficiency',
            'experiment_name': 'æ•°æ®æ•ˆç‡åˆ†æ',
            'purpose': 'åˆ†ææ¨¡å‹åœ¨ä¸åŒæ•°æ®é‡ä¸‹çš„å­¦ä¹ æ•ˆç‡',
            'notes': 'å›ºå®šæœ€ä¼˜å‚æ•°ï¼Œæµ‹è¯•æ•°æ®ç¨€ç¼ºåœºæ™¯ä¸‹çš„æ€§èƒ½è¡°å‡',
        },
        'hyperparameters': {
            'epochs': [50],
            'batch_size': [32],
            'learning_rate': [1e-3],
            'data_fractions': [1.0, 0.8, 0.6, 0.4, 0.2, 0.1, 0.05, 0.01],
            'seeds': [42, 123, 456, 789, 101],
            'classifier_types': ['linear', 'mlp'],
            'mlp_layers': [1, 2, 3],  # æµ‹è¯•ä¸åŒMLPå±‚æ•°
            'freeze_encoder': [True],
        },
    },

    "architecture_comparison": {
        'experiment_meta': {
            'description': 'architecture_comparison',
            'experiment_name': 'æ¶æ„å¯¹æ¯”å®éªŒ',
            'purpose': 'å¯¹æ¯”Linear Probeå’ŒMLPåˆ†ç±»å™¨çš„æ€§èƒ½å·®å¼‚',
            'notes': 'åœ¨ç›¸åŒæ¡ä»¶ä¸‹æµ‹è¯•ä¸¤ç§åˆ†ç±»å™¨æ¶æ„',
        },
        'hyperparameters': {
            'epochs': [30],
            'batch_size': [16, 32],
            'learning_rate': [1e-3, 2e-3],
            'data_fractions': [1.0, 0.5, 0.2],
            'seeds': [42, 123, 456],
            'classifier_types': ['linear', 'mlp'],
            'mlp_layers': [1, 2, 3],  # å¯¹æ¯”ä¸åŒMLPå±‚æ•°çš„æ•ˆæœ
            'freeze_encoder': [True, False],
        },
    }
}

# ä½¿ç”¨æ–¹æ³•ï¼š
# 1. å°†ä¸Šè¿°ä»»æ„é…ç½®å¤åˆ¶åˆ°ä¸»CONFIGä¸­
# 2. ä¿®æ”¹experiment_metaå­—æ®µæ¥æè¿°ä½ çš„å®éªŒ
# 3. è¿è¡Œè„šæœ¬å³å¯
