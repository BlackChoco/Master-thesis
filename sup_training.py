import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
import os
import json
import random
import itertools
import hashlib
from tqdm import tqdm
from torch.optim import AdamW
from transformers import get_linear_schedule_with_warmup
# --- å…³é”®ä¿®æ”¹: å¯¼å…¥ AutoModel å’Œ AutoTokenizer ---
from modelscope import AutoModel, AutoTokenizer
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report
from sklearn.model_selection import train_test_split # ç”¨äºä»è®­ç»ƒé›†ä¸­åˆ’åˆ†éªŒè¯é›†
from peft import get_peft_model, LoraConfig


# ä»è§£è€¦åçš„æ¨¡å—å¯¼å…¥å¿…è¦çš„ç±»
from cl_base_model import ContrastiveEncoder, TextCNNModel, TextCNNTokenizer

# --- 2. ç¼–ç å™¨ç¼“å­˜ç›¸å…³ç±» ---

class EncodedDataset(Dataset):
    """ä½¿ç”¨é¢„ç¼–ç ç‰¹å¾çš„æ•°æ®é›†ï¼Œç”¨äºå†»ç»“ç¼–ç å™¨çš„åœºæ™¯"""
    def __init__(self, encoded_features: torch.Tensor, labels: list, label_to_id: dict):
        self.encoded_features = encoded_features
        self.labels = labels
        self.label_to_id = label_to_id

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            'encoded_features': self.encoded_features[idx],
            'labels': torch.tensor(self.label_to_id[self.labels[idx]], dtype=torch.long)
        }

def generate_cache_key(model_name: str, texts: list, data_fraction: float = 1.0, max_len: int = 256) -> str:
    """ä¸ºæ–‡æœ¬åˆ—è¡¨å’Œæ¨¡å‹ç”Ÿæˆå”¯ä¸€çš„ç¼“å­˜é”®"""
    # ä½¿ç”¨æ¨¡å‹åç§°ã€æ–‡æœ¬å†…å®¹çš„å“ˆå¸Œå€¼ã€æ•°æ®æ¯”ä¾‹ç”Ÿæˆç¼“å­˜é”®
    texts_str = ''.join(texts[:100])  # åªç”¨å‰100ä¸ªæ–‡æœ¬è®¡ç®—å“ˆå¸Œï¼Œé¿å…è¿‡é•¿
    content_hash = hashlib.md5(f"{model_name}_{texts_str}_{len(texts)}_{data_fraction}_{max_len}".encode()).hexdigest()
    return f"{model_name}_{content_hash}"

def encode_texts_with_model(texts: list, encoder, tokenizer, device, batch_size: int = 64, max_len: int = 256):
    """ä½¿ç”¨ç¼–ç å™¨æ‰¹é‡ç¼–ç æ–‡æœ¬"""
    encoder.eval()
    encoder = encoder.to(device)  # ç¡®ä¿ç¼–ç å™¨åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
    all_features = []

    print(f"ğŸ“¦ æ­£åœ¨ç¼–ç  {len(texts)} ä¸ªæ–‡æœ¬...")

    # åˆ›å»ºä¸´æ—¶æ•°æ®é›†ç”¨äºæ‰¹é‡ç¼–ç 
    temp_dataset = SupervisedTextDataset(
        texts, [0] * len(texts),  # ä¸´æ—¶æ ‡ç­¾ï¼Œä¸ä¼šä½¿ç”¨
        tokenizer, {0: 0}, max_len
    )
    temp_loader = DataLoader(temp_dataset, batch_size=batch_size, shuffle=False)

    with torch.no_grad():
        for batch in tqdm(temp_loader, desc="ç¼–ç ä¸­"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)

            # è·å–ç¼–ç å™¨è¾“å‡º
            base_output = encoder(input_ids=input_ids, attention_mask=attention_mask)

            # æå–ç‰¹å¾ï¼ˆä¸SupervisedModel.forwardä¸­çš„é€»è¾‘ç›¸åŒï¼‰
            if isinstance(base_output, dict):
                if 'last_hidden_state' in base_output:
                    features = base_output['last_hidden_state'][:, 0, :]
                elif 'hidden_states' in base_output:
                    features = base_output['hidden_states'][:, 0, :]
                elif 'pooler_output' in base_output and base_output['pooler_output'] is not None:
                    features = base_output['pooler_output']
                else:
                    raise KeyError(f"åœ¨æ¨¡å‹è¾“å‡ºå­—å…¸ä¸­æ‰¾ä¸åˆ°å¯ç”¨çš„ç‰¹å¾é”®ã€‚å¯ç”¨é”®: {base_output.keys()}")
            elif hasattr(base_output, 'pooler_output') and base_output.pooler_output is not None:
                features = base_output.pooler_output
            elif hasattr(base_output, 'last_hidden_state'):
                features = base_output.last_hidden_state[:, 0, :]
            else:
                features = base_output

            # ç¡®ä¿ç‰¹å¾æ˜¯float32
            if features.dtype != torch.float32:
                features = features.to(torch.float32)

            all_features.append(features.cpu())

    return torch.cat(all_features, dim=0)

def load_or_create_cache(cache_key: str, cache_dir: str, texts: list, encoder, tokenizer, device, config):
    """åŠ è½½æˆ–åˆ›å»ºç¼–ç ç¼“å­˜"""
    cache_path = os.path.join(cache_dir, f"{cache_key}.pt")

    if os.path.exists(cache_path):
        print(f"ğŸ’¾ ä»ç¼“å­˜åŠ è½½ç¼–ç ç‰¹å¾: {cache_path}")
        return torch.load(cache_path, map_location='cpu')
    else:
        print(f"ğŸ”„ ç¼“å­˜ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»ºæ–°ç¼“å­˜...")
        os.makedirs(cache_dir, exist_ok=True)

        encoded_features = encode_texts_with_model(
            texts, encoder, tokenizer, device,
            batch_size=config['optimization']['cache_batch_size']
        )

        # ä¿å­˜ç¼“å­˜
        torch.save(encoded_features, cache_path)
        print(f"ğŸ’¾ ç¼–ç ç‰¹å¾å·²ç¼“å­˜åˆ°: {cache_path}")

        return encoded_features

class CachedSupervisedModel(nn.Module):
    """ä½¿ç”¨ç¼“å­˜ç‰¹å¾çš„ç›‘ç£æ¨¡å‹ï¼Œç”¨äºå†»ç»“ç¼–ç å™¨çš„åœºæ™¯"""
    def __init__(self, encoder_output_dim: int, num_labels: int, classifier_type: str = 'linear', mlp_hidden_neurons: int = 384):
        super().__init__()

        print(f"ç¼“å­˜æ¨¡å¼åˆ†ç±»å™¨æ¥æ”¶çš„ç‰¹å¾ç»´åº¦: {encoder_output_dim}")

        if classifier_type == 'linear':
            self.classifier = nn.Linear(encoder_output_dim, num_labels)
        elif classifier_type == 'mlp':
            # ä¸¤å±‚MLPåˆ†ç±»å™¨ï¼šæŒ‡å®šä¸­é—´å±‚ç¥ç»å…ƒæ•°é‡
            self.classifier = nn.Sequential(
                nn.Linear(encoder_output_dim, mlp_hidden_neurons),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(mlp_hidden_neurons, num_labels)
            )
            print(f"ç¼“å­˜æ¨¡å¼MLPåˆ†ç±»å™¨ç»“æ„: 2å±‚ï¼Œç»´åº¦å˜åŒ–: {encoder_output_dim} -> {mlp_hidden_neurons} -> {num_labels}")
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ†ç±»å™¨ç±»å‹: {classifier_type}ã€‚è¯·é€‰æ‹© 'linear' æˆ– 'mlp'ã€‚")

    def forward(self, encoded_features):
        return self.classifier(encoded_features)

# --- 3. ä¿®æ”¹åçš„è®­ç»ƒå’Œè¯„ä¼°å‡½æ•° ---

def train_epoch_cached(model, data_loader, loss_fn, optimizer, device, scheduler):
    """ç¼“å­˜æ¨¡å¼çš„è®­ç»ƒè½®æ¬¡"""
    model.train()
    total_loss = 0
    for batch in tqdm(data_loader, desc="è®­ç»ƒä¸­", leave=False):
        encoded_features = batch['encoded_features'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()
        outputs = model(encoded_features)
        loss = loss_fn(outputs, labels)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()
        total_loss += loss.item()
    return total_loss / len(data_loader)

def evaluate_model_cached(model, data_loader, loss_fn, device, id_to_label: dict):
    """ç¼“å­˜æ¨¡å¼çš„æ¨¡å‹è¯„ä¼°"""
    model.eval()
    total_loss = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for batch in tqdm(data_loader, desc="è¯„ä¼°ä¸­", leave=False):
            encoded_features = batch['encoded_features'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(encoded_features)
            loss = loss_fn(outputs, labels)
            total_loss += loss.item()

            preds = torch.argmax(outputs, dim=1).cpu().numpy()
            all_preds.extend(preds)
            all_labels.extend(labels.cpu().numpy())

    # è®¡ç®—æŒ‡æ ‡ï¼ˆä¸åŸå‡½æ•°ç›¸åŒï¼‰
    avg_loss = total_loss / len(data_loader)
    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)
    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)
    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)

    class_ids = sorted(id_to_label.keys())
    target_names = [f"class_{id_to_label[cid]}" for cid in class_ids]
    report_dict = classification_report(
        all_labels,
        all_preds,
        labels=class_ids,
        target_names=target_names,
        output_dict=True,
        zero_division=0
    )

    per_class_metrics = {}
    for name in target_names:
        if name in report_dict:
            metrics = report_dict[name].copy()
            metrics.pop('support', None)
            per_class_metrics[name] = metrics

    return {
        "loss": avg_loss,
        "accuracy": accuracy,
        "f1_score": f1,
        "precision": precision,
        "recall": recall,
        "per_class_metrics": per_class_metrics
    }

# --- 3. æ•°æ®é›†å’Œæ¨¡å‹å®šä¹‰ ---

class SupervisedTextDataset(Dataset):
    """ç”¨äºæœ‰ç›‘ç£æ–‡æœ¬åˆ†ç±»çš„PyTorchæ•°æ®é›†ã€‚"""
    def __init__(self, texts: list, labels: list, tokenizer, label_to_id: dict, max_len: int = 256):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.label_to_id = label_to_id

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        
        # ä½¿ç”¨ä¸é¢„è®­ç»ƒæ—¶ç›¸åŒçš„åˆ†è¯å™¨
        # æ£€æŸ¥åˆ†è¯å™¨ç±»å‹ï¼Œå› ä¸º TextCNNTokenizer å’Œ HF Tokenizer çš„å‚æ•°ä¸åŒ
        if isinstance(self.tokenizer, TextCNNTokenizer):
            encoding = self.tokenizer(
                text,
                padding=True,
                truncation=True,
                return_tensors='pt',
                max_length=self.max_len
            )
        else: # å‡è®¾æ˜¯ HuggingFace Tokenizer
            encoding = self.tokenizer(
                text,
                add_special_tokens=True,
                max_length=self.max_len,
                return_token_type_ids=False,
                padding='max_length',
                truncation=True,
                return_attention_mask=True,
                return_tensors='pt',
            )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(self.label_to_id[label], dtype=torch.long)
        }

class SupervisedModel(nn.Module):
    """
    åŒ…è£…é¢„è®­ç»ƒçš„ç¼–ç å™¨å’Œä¸€ä¸ªåˆ†ç±»å¤´ã€‚
    """
    def __init__(self, base_encoder: nn.Module, num_labels: int, classifier_type: str = 'linear', mlp_hidden_neurons: int = 384):
        super().__init__()
        self.base_encoder = base_encoder

        # ä»åŸºç¡€ç¼–ç å™¨è·å–éšè—å±‚ç»´åº¦
        if hasattr(base_encoder, 'base_dim'): # é€‚ç”¨äºæˆ‘ä»¬è‡ªå®šä¹‰çš„TextCNNModel
            hidden_size = base_encoder.base_dim
        elif hasattr(base_encoder.config, 'hidden_size'): # é€‚ç”¨äºHuggingFace/ModelScopeæ¨¡å‹
            hidden_size = base_encoder.config.hidden_size
        else:
            # å°è¯•ä»æ¨¡å‹è¾“å‡ºè·å–ç»´åº¦ï¼ˆå¦‚æœæ¨¡å‹å·²åŠ è½½å‚æ•°ï¼‰
            try:
                # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿè¾“å…¥æ¥æ¨æ–­ç»´åº¦
                dummy_input = {'input_ids': torch.randint(0, 100, (1, 10)), 'attention_mask': torch.ones(1, 10)}
                dummy_output = base_encoder(**dummy_input)

                # è°ƒè¯•ï¼šæ‰“å°è¾“å‡ºçš„æ‰€æœ‰é”®
                print(f"è°ƒè¯•: æ¨¡å‹è¾“å‡ºé”®: {dummy_output.keys()}")

                # ModelScope æ¨¡å‹é€šå¸¸è¿”å›å­—å…¸ï¼Œå°è¯•ä¸åŒçš„é”®
                if 'last_hidden_state' in dummy_output:
                    hidden_size = dummy_output['last_hidden_state'].shape[-1]
                elif 'hidden_states' in dummy_output:
                    hidden_size = dummy_output['hidden_states'].shape[-1]
                else:
                    raise KeyError("åœ¨æ¨¡å‹è¾“å‡ºä¸­æ‰¾ä¸åˆ° 'last_hidden_state' æˆ– 'hidden_states'ã€‚")

            except Exception as e:
                raise ValueError(f"æ— æ³•è‡ªåŠ¨ç¡®å®šåŸºç¡€ç¼–ç å™¨çš„è¾“å‡ºç»´åº¦: {e}")

        print(f"åˆ†ç±»å™¨æ¥æ”¶çš„éšè—å±‚ç»´åº¦: {hidden_size}")

        if classifier_type == 'linear':
            self.classifier = nn.Linear(hidden_size, num_labels)
        elif classifier_type == 'mlp':
            # ä¸¤å±‚MLPåˆ†ç±»å™¨ï¼šæŒ‡å®šä¸­é—´å±‚ç¥ç»å…ƒæ•°é‡
            self.classifier = nn.Sequential(
                nn.Linear(hidden_size, mlp_hidden_neurons),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(mlp_hidden_neurons, num_labels)
            )
            print(f"MLPåˆ†ç±»å™¨ç»“æ„: 2å±‚ï¼Œç»´åº¦å˜åŒ–: {hidden_size} -> {mlp_hidden_neurons} -> {num_labels}")
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ†ç±»å™¨ç±»å‹: {classifier_type}ã€‚è¯·é€‰æ‹© 'linear' æˆ– 'mlp'ã€‚")

    def forward(self, input_ids, attention_mask):
        # åŸºç¡€ç¼–ç å™¨è¾“å‡º
        # ModelScope æ¨¡å‹éœ€è¦å…³é”®å­—å‚æ•°
        base_output = self.base_encoder(input_ids=input_ids, attention_mask=attention_mask)
        
        # æ ¹æ®åŸºç¡€ç¼–ç å™¨çš„è¾“å‡ºç±»å‹æå–ç‰¹å¾
        # ModelScope æ¨¡å‹é€šå¸¸è¿”å›ä¸€ä¸ªå­—å…¸
        if isinstance(base_output, dict):
            if 'last_hidden_state' in base_output:
                # [CLS] token representation
                features = base_output['last_hidden_state'][:, 0, :]
            elif 'hidden_states' in base_output:
                # æœ‰äº›æ¨¡å‹å¯èƒ½åªè¿”å› hidden_states
                features = base_output['hidden_states'][:, 0, :]
            elif 'pooler_output' in base_output and base_output['pooler_output'] is not None:
                features = base_output['pooler_output']
            else:
                # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼ŒæŠ›å‡ºé”™è¯¯
                raise KeyError(f"åœ¨æ¨¡å‹è¾“å‡ºå­—å…¸ä¸­æ‰¾ä¸åˆ°å¯ç”¨çš„ç‰¹å¾é”®ã€‚å¯ç”¨é”®: {base_output.keys()}")
        elif hasattr(base_output, 'pooler_output') and base_output.pooler_output is not None:
            features = base_output.pooler_output
        elif hasattr(base_output, 'last_hidden_state'):
            features = base_output.last_hidden_state[:, 0, :]
        else:
            features = base_output

        # --- ä¿®æ­£ï¼šç¡®ä¿ features æ˜¯ float32 ---
        if features.dtype != torch.float32:
            features = features.to(torch.float32)
        # --------------------------------------

        logits = self.classifier(features)
        return logits

    def freeze_encoder(self):
        """å†»ç»“åŸºç¡€ç¼–ç å™¨çš„æ‰€æœ‰å‚æ•°ã€‚"""
        print("ğŸ§Š æ­£åœ¨å†»ç»“åŸºç¡€ç¼–ç å™¨çš„å‚æ•°...")
        for param in self.base_encoder.parameters():
            param.requires_grad = False
        print("âœ… åŸºç¡€ç¼–ç å™¨å·²å†»ç»“ã€‚")

    def unfreeze_encoder(self):
        """è§£å†»åŸºç¡€ç¼–ç å™¨çš„æ‰€æœ‰å‚æ•°ã€‚"""
        print("ğŸ”¥ æ­£åœ¨è§£å†»åŸºç¡€ç¼–ç å™¨çš„å‚æ•°...")
        for param in self.base_encoder.parameters():
            param.requires_grad = True
        print("âœ… åŸºç¡€ç¼–ç å™¨å·²è§£å†»ã€‚")


# --- 2. è¾…åŠ©å‡½æ•° ---

def set_seed(seed_value=42):
    """ä¸ºæ‰€æœ‰ç›¸å…³åº“è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å¯å¤ç°æ€§ã€‚"""
    random.seed(seed_value)
    np.random.seed(seed_value)
    torch.manual_seed(seed_value)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed_value)

def load_data(train_path, test_path):
    """åŠ è½½è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ã€‚"""
    try:
        df_train = pd.read_csv(train_path)
        df_test = pd.read_csv(test_path)
        print(f"åŠ è½½æ•°æ®: è®­ç»ƒé›† {len(df_train)} è¡Œ, æµ‹è¯•é›† {len(df_test)} è¡Œã€‚")
        return df_train, df_test
    except FileNotFoundError as e:
        print(f"é”™è¯¯: æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ° - {e}")
        return None, None

def load_pretrained_encoder(checkpoint_path: str):
    """
    ä»å¯¹æ¯”å­¦ä¹ é˜¶æ®µçš„checkpointåŠ è½½åŸºç¡€æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œæˆ–ç›´æ¥ä»ModelScope HubåŠ è½½ã€‚
    """
    if os.path.exists(checkpoint_path):
        print(f"æ­£åœ¨ä»æœ¬åœ°checkpoint {checkpoint_path} åŠ è½½...")
        checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'), weights_only=False)

        model_type = checkpoint.get('training_model_type', 'hf')
        model_identifier = checkpoint.get('training_model_identifier_or_path')
        proj_config = checkpoint.get('projection_head_config')
        use_peft = checkpoint.get('use_peft', False)
        peft_config = checkpoint.get('peft_config', None)

        temp_encoder = None
        if model_type == 'textcnn':
            textcnn_conf = checkpoint['textcnn_config']
            vocab_data = checkpoint['vocab']
            temp_encoder = ContrastiveEncoder(
                model_type='textcnn', vocab=vocab_data, textcnn_config=textcnn_conf,
                projection_hidden_dim=proj_config['hidden_dim'],
                projection_output_dim=proj_config['output_dim'],
                projection_dropout_rate=proj_config['dropout_rate']
            )
        elif model_type in ['hf', 'modelscope']:
            temp_encoder = ContrastiveEncoder(
                model_type='modelscope', model_name_or_path=model_identifier,
                projection_hidden_dim=proj_config['hidden_dim'],
                projection_output_dim=proj_config['output_dim'],
                projection_dropout_rate=proj_config['dropout_rate']
            )
            # --- LoRA/PEFTæ¨¡å‹ç‰¹æ®Šå¤„ç† ---
            if use_peft and peft_config is not None:
                print("æ£€æµ‹åˆ°PEFT/LoRAæ¨¡å‹ï¼Œæ­£åœ¨åº”ç”¨LoRAç»“æ„...")
                lora_config = LoraConfig(**peft_config)
                temp_encoder.base_model = get_peft_model(temp_encoder.base_model, lora_config)
                print("LoRAç»“æ„å·²åº”ç”¨ã€‚")
        else:
            print(f"é”™è¯¯: Checkpointä¸­æœªçŸ¥çš„æ¨¡å‹ç±»å‹ '{model_type}'")
            return None, None, None

        # åŠ è½½æƒé‡æ—¶å…è®¸strict=Falseï¼Œå…¼å®¹LoRAæƒé‡
        temp_encoder.load_state_dict(checkpoint['contrastive_encoder_state_dict'], strict=False)
        print(f"âœ… Checkpoint åŠ è½½æˆåŠŸã€‚æ¨¡å‹ç±»å‹: {model_type.upper()}")

        return temp_encoder.base_model, temp_encoder.tokenizer, model_type
    
    # å¦‚æœä¸æ˜¯æœ¬åœ°æ–‡ä»¶ï¼Œåˆ™å°è¯•ä» ModelScope Hub åŠ è½½
    else:
        print(f"æœªæ‰¾åˆ°æœ¬åœ°æ–‡ä»¶ï¼Œå°è¯•ä» ModelScope Hub åŠ è½½æ¨¡å‹: {checkpoint_path}")
        try:
            # --- å…³é”®ä¿®æ”¹: ä½¿ç”¨ AutoModel å’Œ AutoTokenizer ---
            base_model = AutoModel.from_pretrained(checkpoint_path, trust_remote_code=True)
            tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)
            print(f"âœ… æˆåŠŸä» ModelScope Hub åŠ è½½åŸºç¡€æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚")
            return base_model, tokenizer, 'ms' # è¿”å› 'ms' ä½œä¸ºæ¨¡å‹ç±»å‹
        except Exception as e:
            print(f"é”™è¯¯: æ— æ³•ä» ModelScope Hub åŠ è½½æ¨¡å‹ '{checkpoint_path}': {e}")
            return None, None, None


# --- 3. è®­ç»ƒå’Œè¯„ä¼°é€»è¾‘ ---

def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler):
    """æ‰§è¡Œä¸€ä¸ªè®­ç»ƒè½®æ¬¡ã€‚"""
    model.train()
    total_loss = 0
    for batch in tqdm(data_loader, desc="è®­ç»ƒä¸­", leave=False):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        loss = loss_fn(outputs, labels)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()
        total_loss += loss.item()
    return total_loss / len(data_loader)

def evaluate_model(model, data_loader, loss_fn, device, id_to_label: dict):
    """åœ¨éªŒè¯é›†æˆ–æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹ã€‚"""
    model.eval()
    total_loss = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for batch in tqdm(data_loader, desc="è¯„ä¼°ä¸­", leave=False):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            loss = loss_fn(outputs, labels)
            total_loss += loss.item()

            preds = torch.argmax(outputs, dim=1).cpu().numpy()
            all_preds.extend(preds)
            all_labels.extend(labels.cpu().numpy())

    # --- è®¡ç®—æ•´ä½“æŒ‡æ ‡ ---
    avg_loss = total_loss / len(data_loader)
    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)
    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)
    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)
    
    # --- è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡ ---
    class_ids = sorted(id_to_label.keys())
    # ä½¿ç”¨åŸå§‹æ ‡ç­¾ä½œä¸ºæŠ¥å‘Šä¸­çš„åç§°
    target_names = [f"class_{id_to_label[cid]}" for cid in class_ids]
    report_dict = classification_report(
        all_labels, 
        all_preds, 
        labels=class_ids,
        target_names=target_names,
        output_dict=True, 
        zero_division=0
    )
    # æå–æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡ï¼Œå¹¶ç§»é™¤ 'support'
    per_class_metrics = {}
    for name in target_names:
        if name in report_dict:
            metrics = report_dict[name].copy()
            metrics.pop('support', None)
            per_class_metrics[name] = metrics

    return {
        "loss": avg_loss,
        "accuracy": accuracy,
        "f1_score": f1,
        "precision": precision,
        "recall": recall,
        "per_class_metrics": per_class_metrics
    }

# --- 4. è¶…å‚æ•°æœç´¢é…ç½® ---

# ç»Ÿä¸€çš„å®éªŒé…ç½®å­—å…¸
CONFIG = {
    # å®éªŒå…ƒä¿¡æ¯
    'experiment_meta': {
        'description': 'linear_experiment_balanceddata',  # å®éªŒæè¿°æ ‡è¯†ç¬¦
        'experiment_name': 'çº¿æ€§åˆ†ç±»å™¨åœ¨6ä¸ªæ ‡ç­¾å‡è¡¡æ•°æ®é›†',   # å®éªŒçš„ä¸­æ–‡åç§°
        'purpose': 'å¯¹æ¯”LoRAå¾®è°ƒåçš„BERTä¸åŸå§‹BERTåœ¨ä¸åŒæ•°æ®é‡ä¸‹çš„æ€§èƒ½è¡¨ç°ï¼ˆå‡è¡¡æ•°æ®é›†ï¼‰',  # å®éªŒç›®çš„
        'notes': 'ä½¿ç”¨linear probeï¼Œæµ‹è¯•5ä¸ªä¸åŒæ•°æ®æ¯”ä¾‹',  # å®éªŒå¤‡æ³¨
    },

    # æ•°æ®é…ç½®
    'data': {
        'train_data_path': 'data/sup_train_data/balanced_trainset.csv',
        'test_data_path': 'data/sup_train_data/balanced_testset.csv',
        'validation_split': 0.2,  # éªŒè¯é›†æ¯”ä¾‹ï¼ˆä»…åœ¨ä½¿ç”¨æ¯”ä¾‹åˆ†å‰²æ—¶æœ‰æ•ˆï¼‰
        'excluded_labels': [],   # è¦è¿‡æ»¤çš„æ ‡ç­¾
        # æ–°å¢ï¼šå›ºå®šæ•°é‡åˆ†å‰²é…ç½®
        'use_fixed_split': True,    # æ˜¯å¦ä½¿ç”¨å›ºå®šæ•°é‡åˆ†å‰²è€Œéæ¯”ä¾‹åˆ†å‰²
        'train_samples_per_label': 500,  # æ¯ä¸ªæ ‡ç­¾çš„è®­ç»ƒæ ·æœ¬æ•°
        'val_samples_per_label': 200,    # æ¯ä¸ªæ ‡ç­¾çš„éªŒè¯æ ·æœ¬æ•°
    },

    # æ¨¡å‹é…ç½®
    'models': {
        'lora_bert_base_chinese_cl': 'model/google-bert_bert-base-chinese/best_contrastive_model.pth',
        # 'TextCNN_CL_bert': 'model/my_custom_textcnn_v3_bert_pruning_paircl/best_contrastive_model.pth',
        'Bert_base_chinese_nocl': 'google-bert/bert-base-chinese',
    },

    # è¶…å‚æ•°æœç´¢ç©ºé—´
    'hyperparameters': {
        'epochs': [50,100],                    # è®­ç»ƒè½®æ•°
        'batch_size': [16,32,64,128],              # æ‰¹æ¬¡å¤§å°
        'learning_rate': [1e-3,1e-4], # å­¦ä¹ ç‡
        'data_fractions': [1.0, 0.5, 0.2, 0.1, 0.05, 0.02],  # æ•°æ®ä½¿ç”¨æ¯”ä¾‹
        'seeds': [42, 123, 456, 789, 101, 202, 303, 404, 505, 606],             # éšæœºç§å­
        'classifier_types': ['linear'], # åˆ†ç±»å™¨ç±»å‹
        'mlp_hidden_neurons': [384],  # MLPéšè—å±‚ç¥ç»å…ƒæ•°é‡
        'freeze_encoder': [True],     # æ˜¯å¦å†»ç»“ç¼–ç å™¨
    },

    # å®éªŒæ§åˆ¶
    'experiment': {
        'base_output_dir': 'sup_result_hyperparams',  # åŸºç¡€è¾“å‡ºç›®å½•
        'save_individual_results': True,
        'aggregate_results': True,
        'save_experiment_info': True,  # ä¿å­˜å®éªŒä¿¡æ¯
    },

    # æ€§èƒ½ä¼˜åŒ–
    'optimization': {
        'use_encoder_cache': True,       # å†»ç»“ç¼–ç å™¨æ—¶æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        'cache_dir': 'encoder_cache',    # ç¼“å­˜ç›®å½•
        'cache_batch_size': 64,          # ç¼“å­˜æ—¶çš„æ‰¹æ¬¡å¤§å°
    }
}

def generate_hyperparameter_combinations(config):
    """ç”Ÿæˆæ‰€æœ‰è¶…å‚æ•°ç»„åˆ"""
    hyperparams = config['hyperparameters']

    # åˆ›å»ºè¶…å‚æ•°ç»„åˆ
    combinations = []
    for (model_name, checkpoint_path), epochs, batch_size, lr, fraction, seed, classifier_type, mlp_hidden_neurons, freeze in itertools.product(
        config['models'].items(),
        hyperparams['epochs'],
        hyperparams['batch_size'],
        hyperparams['learning_rate'],
        hyperparams['data_fractions'],
        hyperparams['seeds'],
        hyperparams['classifier_types'],
        hyperparams['mlp_hidden_neurons'],
        hyperparams['freeze_encoder']
    ):
        # åªæœ‰å½“classifier_type='mlp'æ—¶ï¼Œmlp_hidden_neuronså‚æ•°æ‰æœ‰æ„ä¹‰
        # å½“classifier_type='linear'æ—¶ï¼Œè·³è¿‡ä¸åŒç¥ç»å…ƒæ•°é‡çš„ç»„åˆ
        if classifier_type == 'linear' and mlp_hidden_neurons != hyperparams['mlp_hidden_neurons'][0]:
            continue

        combinations.append({
            'model_name': model_name,
            'checkpoint_path': checkpoint_path,
            'epochs': epochs,
            'batch_size': batch_size,
            'learning_rate': lr,
            'data_fraction': fraction,
            'seed': seed,
            'classifier_type': classifier_type,
            'mlp_hidden_neurons': mlp_hidden_neurons,
            'freeze_encoder': freeze,
        })

    return combinations

def run_single_experiment(config, hyperparams):
    """è¿è¡Œå•æ¬¡å®éªŒ"""

    print(f"\n--- å®éªŒé…ç½® ---")
    print(f"æ¨¡å‹: {hyperparams['model_name']}")
    print(f"åˆ†ç±»å™¨: {hyperparams['classifier_type']}")
    if hyperparams['classifier_type'] == 'mlp':
        print(f"MLPéšè—å±‚ç¥ç»å…ƒ: {hyperparams['mlp_hidden_neurons']}")
    print(f"å†»ç»“ç¼–ç å™¨: {hyperparams['freeze_encoder']}")
    print(f"æ•°æ®æ¯”ä¾‹: {hyperparams['data_fraction']*100}%")
    print(f"å­¦ä¹ ç‡: {hyperparams['learning_rate']}")
    print(f"æ‰¹æ¬¡å¤§å°: {hyperparams['batch_size']}")
    print(f"è®­ç»ƒè½®æ•°: {hyperparams['epochs']}")
    print(f"éšæœºç§å­: {hyperparams['seed']}")

    # ç¼“å­˜ä¼˜åŒ–æç¤º
    use_cache = config['optimization']['use_encoder_cache'] and hyperparams['freeze_encoder']
    if use_cache:
        print("ğŸš€ ç¼“å­˜ä¼˜åŒ–: å¯ç”¨")
    else:
        print("ğŸŒ ç¼“å­˜ä¼˜åŒ–: ç¦ç”¨ (ç¼–ç å™¨æœªå†»ç»“æˆ–ç¼“å­˜åŠŸèƒ½å…³é—­)")

    # åŠ è½½æ•°æ®
    df_train_full, df_test = load_data(config['data']['train_data_path'], config['data']['test_data_path'])
    if df_train_full is None:
        return None

    # è¿‡æ»¤æ ‡ç­¾
    for label in config['data']['excluded_labels']:
        df_train_full = df_train_full[df_train_full['label'] != label].reset_index(drop=True)
        df_test = df_test[df_test['label'] != label].reset_index(drop=True)

    # ğŸ”§ å…³é”®ä¿®æ”¹ï¼šæ ¹æ®é…ç½®é€‰æ‹©æ•°æ®åˆ†å‰²æ–¹å¼
    if config['data']['use_fixed_split']:
        print(f"ğŸ“Š ä½¿ç”¨å›ºå®šæ•°é‡åˆ†å‰²: æ¯ä¸ªæ ‡ç­¾ {config['data']['train_samples_per_label']} è®­ç»ƒ + {config['data']['val_samples_per_label']} éªŒè¯...")

        # æŒ‰æ ‡ç­¾åˆ†ç»„å¹¶å›ºå®šé‡‡æ ·
        df_train_list = []
        df_val_list = []

        for label in df_train_full['label'].unique():
            label_data = df_train_full[df_train_full['label'] == label].reset_index(drop=True)

            # æ£€æŸ¥æ¯ä¸ªæ ‡ç­¾çš„æ•°æ®é‡æ˜¯å¦è¶³å¤Ÿ
            required_total = config['data']['train_samples_per_label'] + config['data']['val_samples_per_label']
            if len(label_data) < required_total:
                print(f"âš ï¸  è­¦å‘Š: æ ‡ç­¾ '{label}' åªæœ‰ {len(label_data)} æ¡æ•°æ®ï¼Œéœ€è¦ {required_total} æ¡")
                print(f"   å°†ä½¿ç”¨æ‰€æœ‰å¯ç”¨æ•°æ®ï¼ŒæŒ‰åŸæ¯”ä¾‹åˆ†å‰²...")
                # å¦‚æœæ•°æ®ä¸å¤Ÿï¼ŒæŒ‰åŸæ¯”ä¾‹åˆ†å‰²
                label_train, label_val = train_test_split(
                    label_data,
                    test_size=config['data']['validation_split'],
                    random_state=42
                )
            else:
                # å›ºå®šç§å­éšæœºé‡‡æ ·
                label_data_shuffled = label_data.sample(n=len(label_data), random_state=42).reset_index(drop=True)

                # æŒ‰å›ºå®šæ•°é‡åˆ†å‰²
                label_train = label_data_shuffled[:config['data']['train_samples_per_label']]
                label_val = label_data_shuffled[config['data']['train_samples_per_label']:config['data']['train_samples_per_label'] + config['data']['val_samples_per_label']]

            df_train_list.append(label_train)
            df_val_list.append(label_val)
            print(f"   æ ‡ç­¾ '{label}': {len(label_train)} è®­ç»ƒ, {len(label_val)} éªŒè¯")

        df_train_prelim = pd.concat(df_train_list, ignore_index=True)
        df_val = pd.concat(df_val_list, ignore_index=True)

        print(f"âœ… å›ºå®šæ•°é‡åˆ†å‰²å®Œæˆ: è®­ç»ƒé›† {len(df_train_prelim)} æ¡, éªŒè¯é›† {len(df_val)} æ¡")
    else:
        print("ğŸ“Š ä½¿ç”¨æ¯”ä¾‹åˆ†å‰²ï¼Œç¡®ä¿å®éªŒé—´æ•°æ®ä¸€è‡´æ€§...")
        df_train_prelim, df_val = train_test_split(
            df_train_full,
            test_size=config['data']['validation_split'],
            random_state=42,  # å›ºå®šç§å­ï¼æ‰€æœ‰å®éªŒä½¿ç”¨ç›¸åŒçš„æ•°æ®åˆ†å‰²
            stratify=df_train_full['label']
        )

    # æ•°æ®é‡‡æ ·ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if hyperparams['data_fraction'] < 1.0:
        df_train = df_train_prelim.groupby('label', group_keys=False).apply(
            lambda x: x.sample(frac=hyperparams['data_fraction'], random_state=hyperparams['seed'])
        ).reset_index(drop=True)
    else:
        df_train = df_train_prelim.reset_index(drop=True)

    # ğŸ”§ å…³é”®ä¿®æ”¹ï¼šåœ¨æ•°æ®åˆ†å‰²åè®¾ç½®å®éªŒéšæœºç§å­
    print(f"ğŸ² è®¾ç½®å®éªŒéšæœºç§å­ {hyperparams['seed']} (å½±å“æ¨¡å‹åˆå§‹åŒ–å’Œè®­ç»ƒè¿‡ç¨‹)...")
    set_seed(hyperparams['seed'])

    # ç”Ÿæˆæ ‡ç­¾æ˜ å°„
    unique_labels = sorted(df_train_full['label'].unique())
    label_to_id = {label: i for i, label in enumerate(unique_labels)}
    id_to_label = {i: label for label, i in label_to_id.items()}
    num_labels = len(unique_labels)

    # åŠ è½½é¢„è®­ç»ƒç¼–ç å™¨
    base_encoder, tokenizer, _ = load_pretrained_encoder(hyperparams['checkpoint_path'])
    if base_encoder is None:
        return None

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # ç¡®ä¿ç¼–ç å™¨åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
    base_encoder = base_encoder.to(device)

    # æ ¹æ®æ˜¯å¦ä½¿ç”¨ç¼“å­˜é€‰æ‹©ä¸åŒçš„å¤„ç†è·¯å¾„
    if use_cache:
        print("\nğŸ”„ ä½¿ç”¨ç¼“å­˜ä¼˜åŒ–æ¨¡å¼...")

        # ä¸ºæ•°æ®é›†ç”Ÿæˆç¼“å­˜
        cache_dir = config['optimization']['cache_dir']

        # ğŸ”§ ä¿®å¤ï¼šåŸºäºå®Œæ•´æ•°æ®é›†ç”Ÿæˆç¼“å­˜é”®ï¼Œé¿å…é‡å¤ç¼“å­˜
        train_cache_key = generate_cache_key(hyperparams['model_name'], df_train_prelim['content'].tolist(), 1.0)  # åŸºäºå®Œæ•´è®­ç»ƒé›†
        val_cache_key = generate_cache_key(hyperparams['model_name'], df_val['content'].tolist(), 1.0)  # éªŒè¯é›†æ€»æ˜¯100%
        test_cache_key = generate_cache_key(hyperparams['model_name'], df_test['content'].tolist(), 1.0)  # æµ‹è¯•é›†æ€»æ˜¯100%

        # åŠ è½½æˆ–åˆ›å»ºç¼“å­˜ï¼ˆåŸºäºå®Œæ•´æ•°æ®é›†ï¼‰
        train_features_full = load_or_create_cache(train_cache_key, cache_dir, df_train_prelim['content'].tolist(),
                                                  base_encoder, tokenizer, device, config)
        val_features = load_or_create_cache(val_cache_key, cache_dir, df_val['content'].tolist(),
                                          base_encoder, tokenizer, device, config)
        test_features = load_or_create_cache(test_cache_key, cache_dir, df_test['content'].tolist(),
                                           base_encoder, tokenizer, device, config)

        # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ ¹æ®é‡‡æ ·åçš„è®­ç»ƒé›†é€‰æ‹©å¯¹åº”çš„ç‰¹å¾
        if hyperparams['data_fraction'] < 1.0:
            # è·å–é‡‡æ ·åè®­ç»ƒé›†åœ¨åŸå§‹è®­ç»ƒé›†ä¸­çš„ç´¢å¼•
            train_indices = df_train_prelim.index[df_train_prelim['content'].isin(df_train['content'])].tolist()
            train_features = train_features_full[train_indices]
            print(f"ğŸ“Š ä»å®Œæ•´è®­ç»ƒç‰¹å¾({train_features_full.shape[0]})ä¸­é€‰æ‹©é‡‡æ ·ç‰¹å¾({train_features.shape[0]})")
        else:
            train_features = train_features_full

        # åˆ›å»ºç¼“å­˜æ•°æ®é›†
        train_dataset = EncodedDataset(train_features, df_train['label'].tolist(), label_to_id)
        val_dataset = EncodedDataset(val_features, df_val['label'].tolist(), label_to_id)
        test_dataset = EncodedDataset(test_features, df_test['label'].tolist(), label_to_id)

        train_loader = DataLoader(train_dataset, batch_size=hyperparams['batch_size'], shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=hyperparams['batch_size'])
        test_loader = DataLoader(test_dataset, batch_size=hyperparams['batch_size'])

        # æ„å»ºç¼“å­˜æ¨¡å¼çš„ç›‘ç£æ¨¡å‹
        encoder_output_dim = train_features.shape[1]
        model = CachedSupervisedModel(
            encoder_output_dim=encoder_output_dim,
            num_labels=num_labels,
            classifier_type=hyperparams['classifier_type'],
            mlp_hidden_neurons=hyperparams['mlp_hidden_neurons'] if hyperparams['classifier_type'] == 'mlp' else 384
        ).to(device)

        # è®¾ç½®ä¼˜åŒ–å™¨
        optimizer = AdamW(model.parameters(), lr=hyperparams['learning_rate'])

        # å‡†å¤‡è®­ç»ƒ
        loss_fn = nn.CrossEntropyLoss()
        total_steps = len(train_loader) * hyperparams['epochs']
        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)

        best_val_f1 = -1
        best_model_state = None

        # è®­ç»ƒå¾ªç¯ï¼ˆç¼“å­˜æ¨¡å¼ï¼‰
        for epoch in range(hyperparams['epochs']):
            print(f"  Epoch {epoch + 1}/{hyperparams['epochs']}")
            train_loss = train_epoch_cached(model, train_loader, loss_fn, optimizer, device, scheduler)
            val_metrics = evaluate_model_cached(model, val_loader, loss_fn, device, id_to_label)
            print(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f} | éªŒè¯æŸå¤±: {val_metrics['loss']:.4f} | éªŒè¯F1: {val_metrics['f1_score']:.4f}")

            if val_metrics['f1_score'] > best_val_f1:
                best_val_f1 = val_metrics['f1_score']
                best_model_state = model.state_dict()
                print(f"  ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯F1åˆ†æ•°: {best_val_f1:.4f}")

        # æµ‹è¯•é›†è¯„ä¼°ï¼ˆç¼“å­˜æ¨¡å¼ï¼‰
        if best_model_state:
            model.load_state_dict(best_model_state)
        print("ğŸ§ª ä½¿ç”¨æœ€ä½³æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
        test_metrics = evaluate_model_cached(model, test_loader, loss_fn, device, id_to_label)

    else:
        print("\nğŸŒ ä½¿ç”¨æ ‡å‡†æ¨¡å¼...")

        # æ ‡å‡†æ¨¡å¼ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        train_dataset = SupervisedTextDataset(df_train['content'].tolist(), df_train['label'].tolist(), tokenizer, label_to_id)
        val_dataset = SupervisedTextDataset(df_val['content'].tolist(), df_val['label'].tolist(), tokenizer, label_to_id)
        test_dataset = SupervisedTextDataset(df_test['content'].tolist(), df_test['label'].tolist(), tokenizer, label_to_id)

        train_loader = DataLoader(train_dataset, batch_size=hyperparams['batch_size'], shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=hyperparams['batch_size'])
        test_loader = DataLoader(test_dataset, batch_size=hyperparams['batch_size'])

        # æ„å»ºç›‘ç£æ¨¡å‹
        model = SupervisedModel(
            base_encoder=base_encoder,
            num_labels=num_labels,
            classifier_type=hyperparams['classifier_type'],
            mlp_hidden_neurons=hyperparams['mlp_hidden_neurons'] if hyperparams['classifier_type'] == 'mlp' else 384
        ).to(device)

        # è®¾ç½®ä¼˜åŒ–å™¨
        if hyperparams['freeze_encoder']:
            model.freeze_encoder()
            optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=hyperparams['learning_rate'])
        else:
            model.unfreeze_encoder()
            optimizer = AdamW(model.parameters(), lr=hyperparams['learning_rate'])

        # å‡†å¤‡è®­ç»ƒ
        loss_fn = nn.CrossEntropyLoss()
        total_steps = len(train_loader) * hyperparams['epochs']
        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)

        best_val_f1 = -1
        best_model_state = None

        # è®­ç»ƒå¾ªç¯
        for epoch in range(hyperparams['epochs']):
            print(f"  Epoch {epoch + 1}/{hyperparams['epochs']}")
            train_loss = train_epoch(model, train_loader, loss_fn, optimizer, device, scheduler)
            val_metrics = evaluate_model(model, val_loader, loss_fn, device, id_to_label)
            print(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f} | éªŒè¯æŸå¤±: {val_metrics['loss']:.4f} | éªŒè¯F1: {val_metrics['f1_score']:.4f}")

            if val_metrics['f1_score'] > best_val_f1:
                best_val_f1 = val_metrics['f1_score']
                best_model_state = model.state_dict()
                print(f"  ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯F1åˆ†æ•°: {best_val_f1:.4f}")

        # æµ‹è¯•é›†è¯„ä¼°
        if best_model_state:
            model.load_state_dict(best_model_state)
        print("ğŸ§ª ä½¿ç”¨æœ€ä½³æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
        test_metrics = evaluate_model(model, test_loader, loss_fn, device, id_to_label)

    print(f"  æµ‹è¯•é›†ç»“æœ -> æŸå¤±: {test_metrics['loss']:.4f}, å‡†ç¡®ç‡: {test_metrics['accuracy']:.4f}, F1åˆ†æ•°: {test_metrics['f1_score']:.4f}")

    # æ·»åŠ è¶…å‚æ•°ä¿¡æ¯åˆ°ç»“æœä¸­
    result = {
        'hyperparameters': hyperparams,
        'metrics': test_metrics,
        'best_val_f1': best_val_f1,
        'used_cache': use_cache
    }

    return result

def extract_best_results_by_model_and_fraction(results):
    """æå–æ¯ä¸ªæ¨¡å‹åœ¨æ¯ä¸ªæ•°æ®æ¯”ä¾‹ä¸‹çš„æœ€ä¼˜ç»“æœ"""
    best_results = {}

    for result in results:
        if result is None:
            continue

        hyperparams = result['hyperparameters']
        metrics = result['metrics']

        model_name = hyperparams['model_name']
        data_fraction = hyperparams['data_fraction']
        f1_score = metrics['f1_score']

        key = (model_name, data_fraction)

        if key not in best_results or f1_score > best_results[key]['metrics']['f1_score']:
            best_results[key] = result

    return best_results

def generate_model_comparison_analysis(best_results):
    """ç”Ÿæˆæ¨¡å‹å¯¹æ¯”åˆ†æ"""
    # æŒ‰æ¨¡å‹å’Œæ•°æ®æ¯”ä¾‹ç»„ç»‡ç»“æœ
    comparison_data = {}
    model_names = set()
    data_fractions = set()

    for (model_name, data_fraction), result in best_results.items():
        model_names.add(model_name)
        data_fractions.add(data_fraction)

        if model_name not in comparison_data:
            comparison_data[model_name] = {}

        comparison_data[model_name][data_fraction] = {
            'metrics': result['metrics'],
            'hyperparams': result['hyperparameters'],
            'best_val_f1': result['best_val_f1']
        }

    model_names = sorted(model_names)
    data_fractions = sorted(data_fractions, reverse=True)

    # ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼æ•°æ®
    comparison_table = {
        'models': model_names,
        'data_fractions': data_fractions,
        'results': {}
    }

    for fraction in data_fractions:
        comparison_table['results'][fraction] = {}
        for model in model_names:
            if model in comparison_data and fraction in comparison_data[model]:
                model_result = comparison_data[model][fraction]
                comparison_table['results'][fraction][model] = {
                    'accuracy': round(model_result['metrics']['accuracy'], 4),
                    'f1_score': round(model_result['metrics']['f1_score'], 4),
                    'precision': round(model_result['metrics']['precision'], 4),
                    'recall': round(model_result['metrics']['recall'], 4),
                    'best_hyperparams': {
                        'learning_rate': model_result['hyperparams']['learning_rate'],
                        'batch_size': model_result['hyperparams']['batch_size'],
                        'epochs': model_result['hyperparams']['epochs'],
                        'seed': model_result['hyperparams']['seed']
                    }
                }
            else:
                comparison_table['results'][fraction][model] = None

    return comparison_table, comparison_data

def save_experiment_results(results, config):
    """ä¿å­˜å®éªŒç»“æœ - é‡æ–°ç»„ç»‡çš„æ–‡ä»¶å¤¹ç»“æ„"""
    # åˆ›å»ºå®éªŒç‰¹å®šçš„è¾“å‡ºç›®å½•
    experiment_id = config['experiment_meta']['description']
    output_dir = os.path.join(config['experiment']['base_output_dir'], experiment_id)
    os.makedirs(output_dir, exist_ok=True)

    # åˆ›å»ºä¸¤ä¸ªå­æ–‡ä»¶å¤¹
    best_results_dir = os.path.join(output_dir, 'best_results_comparison')
    detailed_results_dir = os.path.join(output_dir, 'detailed_results')
    os.makedirs(best_results_dir, exist_ok=True)
    os.makedirs(detailed_results_dir, exist_ok=True)

    # ä¿å­˜å®éªŒä¿¡æ¯åˆ°ä¸»ç›®å½•
    if config['experiment']['save_experiment_info']:
        experiment_info = {
            'experiment_meta': config['experiment_meta'],
            'data_config': config['data'],
            'models': config['models'],
            'hyperparameters': config['hyperparameters'],
            'total_experiments': len(results),
            'successful_experiments': sum(1 for r in results if r is not None),
            'timestamp': pd.Timestamp.now().isoformat()
        }

        info_filepath = os.path.join(output_dir, 'experiment_info.json')
        with open(info_filepath, 'w', encoding='utf-8') as f:
            json.dump(experiment_info, f, ensure_ascii=False, indent=4)
        print(f"ğŸ“‹ å®éªŒä¿¡æ¯å·²ä¿å­˜åˆ°: {info_filepath}")

    # ä¿å­˜è¯¦ç»†ç»“æœåˆ° detailed_results æ–‡ä»¶å¤¹
    if config['experiment']['save_individual_results']:
        print("ğŸ’¾ ä¿å­˜è¯¦ç»†å®éªŒç»“æœ...")
        for i, result in enumerate(results):
            if result is None:
                continue
            hyperparams = result['hyperparameters']
            filename = (
                f"{hyperparams['model_name']}_{hyperparams['classifier_type']}"
            )
            if hyperparams['classifier_type'] == 'mlp':
                filename += f"_{hyperparams['mlp_hidden_neurons']}neurons"
            filename += (
                f"_freeze{hyperparams['freeze_encoder']}_"
                f"ep{hyperparams['epochs']}_bs{hyperparams['batch_size']}_"
                f"lr{hyperparams['learning_rate']}_"
                f"frac{int(hyperparams['data_fraction']*100)}_"
                f"seed{hyperparams['seed']}.json"
            )
            filepath = os.path.join(detailed_results_dir, filename)
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=4)

        print(f"ğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {detailed_results_dir}")

    # æå–æœ€ä¼˜ç»“æœå¹¶ç”Ÿæˆå¯¹æ¯”åˆ†æ
    print("ğŸ† æå–æœ€ä¼˜ç»“æœå¹¶ç”Ÿæˆå¯¹æ¯”åˆ†æ...")
    best_results = extract_best_results_by_model_and_fraction(results)
    comparison_table, comparison_data = generate_model_comparison_analysis(best_results)

    # ä¿å­˜æ¨¡å‹å¯¹æ¯”ç»“æœåˆ° best_results_comparison æ–‡ä»¶å¤¹
    model_comparison_path = os.path.join(best_results_dir, 'model_comparison_by_data_fraction.json')
    with open(model_comparison_path, 'w', encoding='utf-8') as f:
        json.dump(comparison_table, f, ensure_ascii=False, indent=4)
    print(f"ğŸ“Š æ¨¡å‹å¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: {model_comparison_path}")

    # ä¿å­˜æœ€ä¼˜è¶…å‚æ•°é…ç½®
    best_hyperparams = {}
    for (model_name, data_fraction), result in best_results.items():
        if model_name not in best_hyperparams:
            best_hyperparams[model_name] = {}
        best_hyperparams[model_name][data_fraction] = {
            'hyperparameters': result['hyperparameters'],
            'performance': {
                'f1_score': result['metrics']['f1_score'],
                'accuracy': result['metrics']['accuracy'],
                'precision': result['metrics']['precision'],
                'recall': result['metrics']['recall']
            },
            'validation_f1': result['best_val_f1']
        }

    best_hyperparams_path = os.path.join(best_results_dir, 'best_hyperparams_by_model.json')
    with open(best_hyperparams_path, 'w', encoding='utf-8') as f:
        json.dump(best_hyperparams, f, ensure_ascii=False, indent=4)
    print(f"âš™ï¸  æœ€ä¼˜è¶…å‚æ•°å·²ä¿å­˜åˆ°: {best_hyperparams_path}")

    # ç”Ÿæˆæ€§èƒ½åˆ†ææŠ¥å‘Š
    performance_analysis = {
        'summary': {
            'total_model_data_fraction_combinations': len(best_results),
            'models_tested': list(set(model for model, _ in best_results.keys())),
            'data_fractions_tested': sorted(list(set(fraction for _, fraction in best_results.keys())), reverse=True)
        },
        'best_overall_performance': {},
        'performance_trends': {}
    }

    # æ‰¾å‡ºæ¯ä¸ªæŒ‡æ ‡çš„å…¨å±€æœ€ä¼˜ç»“æœ
    for metric in ['accuracy', 'f1_score', 'precision', 'recall']:
        best_result = max(best_results.values(), key=lambda x: x['metrics'][metric])
        performance_analysis['best_overall_performance'][metric] = {
            'value': best_result['metrics'][metric],
            'model': best_result['hyperparameters']['model_name'],
            'data_fraction': best_result['hyperparameters']['data_fraction'],
            'hyperparameters': best_result['hyperparameters']
        }

    # åˆ†ææ€§èƒ½è¶‹åŠ¿
    for model_name in performance_analysis['summary']['models_tested']:
        model_results = [(fraction, result) for (model, fraction), result in best_results.items() if model == model_name]
        model_results.sort(key=lambda x: x[0], reverse=True)  # æŒ‰æ•°æ®æ¯”ä¾‹é™åºæ’åˆ—

        performance_analysis['performance_trends'][model_name] = {
            'f1_scores_by_fraction': [(fraction, result['metrics']['f1_score']) for fraction, result in model_results],
            'accuracy_by_fraction': [(fraction, result['metrics']['accuracy']) for fraction, result in model_results]
        }

    performance_analysis_path = os.path.join(best_results_dir, 'performance_analysis.json')
    with open(performance_analysis_path, 'w', encoding='utf-8') as f:
        json.dump(performance_analysis, f, ensure_ascii=False, indent=4)
    print(f"ğŸ“ˆ æ€§èƒ½åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {performance_analysis_path}")

    # ä¼ ç»Ÿçš„èšåˆç»“æœåˆ†æ - ä¿å­˜åˆ°è¯¦ç»†ç»“æœæ–‡ä»¶å¤¹
    if config['experiment']['aggregate_results']:
        print("ğŸ“Š ç”Ÿæˆä¼ ç»Ÿèšåˆåˆ†æ...")
        aggregate_results = {}
        for result in results:
            if result is None:
                continue

            hyperparams = result['hyperparameters']
            metrics = result['metrics']

            # åˆ›å»ºé…ç½®é”®
            config_key = (
                hyperparams['model_name'],
                hyperparams['classifier_type'],
                hyperparams['mlp_hidden_neurons'] if hyperparams['classifier_type'] == 'mlp' else 0,
                hyperparams['freeze_encoder'],
                hyperparams['epochs'],
                hyperparams['batch_size'],
                hyperparams['learning_rate'],
                hyperparams['data_fraction']
            )

            if config_key not in aggregate_results:
                aggregate_results[config_key] = []

            aggregate_results[config_key].append({
                'seed': hyperparams['seed'],
                'accuracy': metrics['accuracy'],
                'f1_score': metrics['f1_score'],
                'precision': metrics['precision'],
                'recall': metrics['recall']
            })

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        summary_results = {
            'experiment_meta': config['experiment_meta'],  # æ·»åŠ å®éªŒå…ƒä¿¡æ¯
            'results': {}
        }

        for config_key, runs in aggregate_results.items():
            if len(runs) > 0:
                df_runs = pd.DataFrame(runs)
                summary_results['results'][str(config_key)] = {
                    'config': {
                        'model_name': config_key[0],
                        'classifier_type': config_key[1],
                        'mlp_hidden_neurons': config_key[2],
                        'freeze_encoder': config_key[3],
                        'epochs': config_key[4],
                        'batch_size': config_key[5],
                        'learning_rate': config_key[6],
                        'data_fraction': config_key[7]
                    },
                    'mean_metrics': df_runs.mean().to_dict(),
                    'std_metrics': df_runs.std().to_dict(),
                    'num_runs': len(runs),
                    'all_runs': runs
                }

        # ä¿å­˜èšåˆç»“æœåˆ°è¯¦ç»†ç»“æœæ–‡ä»¶å¤¹
        summary_filepath = os.path.join(detailed_results_dir, 'hyperparameter_search_summary.json')
        with open(summary_filepath, 'w', encoding='utf-8') as f:
            json.dump(summary_results, f, ensure_ascii=False, indent=4)
        print(f"ğŸ“‹ ä¼ ç»Ÿèšåˆç»“æœå·²ä¿å­˜åˆ°: {summary_filepath}")

    print(f"\nâœ… æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
    print(f"   ğŸ“ æœ€ä¼˜ç»“æœå¯¹æ¯”: {best_results_dir}")
    print(f"   ğŸ“ è¯¦ç»†å®éªŒç»“æœ: {detailed_results_dir}")

    return output_dir


# --- 6. ä¸»å‡½æ•° ---

if __name__ == '__main__':
    print("ğŸš€ å¼€å§‹è¶…å‚æ•°æœç´¢å®éªŒ...")
    print(f"ğŸ“ å®éªŒåç§°: {CONFIG['experiment_meta']['experiment_name']}")
    print(f"ğŸ¯ å®éªŒç›®çš„: {CONFIG['experiment_meta']['purpose']}")
    print(f"ğŸ“„ å®éªŒå¤‡æ³¨: {CONFIG['experiment_meta']['notes']}")

    # ç”Ÿæˆæ‰€æœ‰è¶…å‚æ•°ç»„åˆ
    combinations = generate_hyperparameter_combinations(CONFIG)
    print(f"ğŸ“Š æ€»å…±éœ€è¦è¿è¡Œ {len(combinations)} ä¸ªå®éªŒé…ç½®")

    # è¿è¡Œæ‰€æœ‰å®éªŒ
    all_results = []
    for i, hyperparams in enumerate(combinations):
        print(f"\n{'='*80}")
        print(f"å®éªŒè¿›åº¦: {i+1}/{len(combinations)}")
        print(f"{'='*80}")

        result = run_single_experiment(CONFIG, hyperparams)
        all_results.append(result)

        # å¯é€‰ï¼šæ¯å®Œæˆå‡ ä¸ªå®éªŒå°±ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ
        if (i + 1) % 10 == 0:
            print(f"\nğŸ’¾ ä¿å­˜ä¸­é—´ç»“æœ... (å·²å®Œæˆ {i+1}/{len(combinations)} ä¸ªå®éªŒ)")
            experiment_dir = save_experiment_results(all_results, CONFIG)

    # ä¿å­˜æœ€ç»ˆç»“æœ
    print(f"\nğŸ’¾ ä¿å­˜æœ€ç»ˆå®éªŒç»“æœ...")
    experiment_dir = save_experiment_results(all_results, CONFIG)

    print(f"\nğŸ‰ æ‰€æœ‰è¶…å‚æ•°æœç´¢å®éªŒå·²å®Œæˆï¼")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {experiment_dir}")
    print(f"ğŸ“‹ å®éªŒæè¿°: {CONFIG['experiment_meta']['description']}")

# --- å…¶ä»–å®éªŒé…ç½®ç¤ºä¾‹ ---

# ä½ å¯ä»¥å¤åˆ¶ä»¥ä¸‹é…ç½®ç¤ºä¾‹ï¼Œä¿®æ”¹experiment_metaéƒ¨åˆ†ï¼Œè¿›è¡Œä¸åŒçš„å®éªŒå¯¹æ¯”

EXPERIMENT_CONFIGS = {
    "learning_rate_comparison": {
        'experiment_meta': {
            'description': 'learning_rate_comparison',
            'experiment_name': 'å­¦ä¹ ç‡å¯¹æ¯”å®éªŒ',
            'purpose': 'æµ‹è¯•ä¸åŒå­¦ä¹ ç‡å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“',
            'notes': 'å›ºå®šå…¶ä»–å‚æ•°ï¼Œå¯¹æ¯”1e-3, 2e-3, 5e-3ä¸‰ç§å­¦ä¹ ç‡',
        },
        'hyperparameters': {
            'epochs': [20],
            'batch_size': [32],
            'learning_rate': [1e-3, 2e-3, 5e-3],
            'data_fractions': [1.0],
            'seeds': [42, 123, 456],
            'classifier_types': ['linear'],
            'mlp_layers': [1],  # linearåˆ†ç±»å™¨æ—¶MLPå±‚æ•°æ— æ„ä¹‰
            'freeze_encoder': [True],
        },
    },

    "data_efficiency": {
        'experiment_meta': {
            'description': 'data_efficiency',
            'experiment_name': 'æ•°æ®æ•ˆç‡åˆ†æ',
            'purpose': 'åˆ†ææ¨¡å‹åœ¨ä¸åŒæ•°æ®é‡ä¸‹çš„å­¦ä¹ æ•ˆç‡',
            'notes': 'å›ºå®šæœ€ä¼˜å‚æ•°ï¼Œæµ‹è¯•æ•°æ®ç¨€ç¼ºåœºæ™¯ä¸‹çš„æ€§èƒ½è¡°å‡',
        },
        'hyperparameters': {
            'epochs': [50],
            'batch_size': [32],
            'learning_rate': [1e-3],
            'data_fractions': [1.0, 0.8, 0.6, 0.4, 0.2, 0.1, 0.05, 0.01],
            'seeds': [42, 123, 456, 789, 101],
            'classifier_types': ['linear', 'mlp'],
            'mlp_layers': [1, 2, 3],  # æµ‹è¯•ä¸åŒMLPå±‚æ•°
            'freeze_encoder': [True],
        },
    },

    "architecture_comparison": {
        'experiment_meta': {
            'description': 'architecture_comparison',
            'experiment_name': 'æ¶æ„å¯¹æ¯”å®éªŒ',
            'purpose': 'å¯¹æ¯”Linear Probeå’ŒMLPåˆ†ç±»å™¨çš„æ€§èƒ½å·®å¼‚',
            'notes': 'åœ¨ç›¸åŒæ¡ä»¶ä¸‹æµ‹è¯•ä¸¤ç§åˆ†ç±»å™¨æ¶æ„',
        },
        'hyperparameters': {
            'epochs': [30],
            'batch_size': [16, 32],
            'learning_rate': [1e-3, 2e-3],
            'data_fractions': [1.0, 0.5, 0.2],
            'seeds': [42, 123, 456],
            'classifier_types': ['linear', 'mlp'],
            'mlp_layers': [1, 2, 3],  # å¯¹æ¯”ä¸åŒMLPå±‚æ•°çš„æ•ˆæœ
            'freeze_encoder': [True, False],
        },
    }
}

# ä½¿ç”¨æ–¹æ³•ï¼š
# 1. å°†ä¸Šè¿°ä»»æ„é…ç½®å¤åˆ¶åˆ°ä¸»CONFIGä¸­
# 2. ä¿®æ”¹experiment_metaå­—æ®µæ¥æè¿°ä½ çš„å®éªŒ
# 3. è¿è¡Œè„šæœ¬å³å¯