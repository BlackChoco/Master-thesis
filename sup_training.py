import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
import os
import json
import random
import itertools
from tqdm import tqdm
from torch.optim import AdamW
from transformers import get_linear_schedule_with_warmup
# --- å…³é”®ä¿®æ”¹: å¯¼å…¥ AutoModel å’Œ AutoTokenizer ---
from modelscope import AutoModel, AutoTokenizer
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report
from sklearn.model_selection import train_test_split # ç”¨äºä»è®­ç»ƒé›†ä¸­åˆ’åˆ†éªŒè¯é›†
from peft import get_peft_model, LoraConfig


# ä»è§£è€¦åçš„æ¨¡å—å¯¼å…¥å¿…è¦çš„ç±»
from cl_base_model import ContrastiveEncoder, TextCNNModel, TextCNNTokenizer

# --- 1. æ•°æ®é›†å’Œæ¨¡å‹å®šä¹‰ ---

class SupervisedTextDataset(Dataset):
    """ç”¨äºæœ‰ç›‘ç£æ–‡æœ¬åˆ†ç±»çš„PyTorchæ•°æ®é›†ã€‚"""
    def __init__(self, texts: list, labels: list, tokenizer, label_to_id: dict, max_len: int = 256):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.label_to_id = label_to_id

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        
        # ä½¿ç”¨ä¸é¢„è®­ç»ƒæ—¶ç›¸åŒçš„åˆ†è¯å™¨
        # æ£€æŸ¥åˆ†è¯å™¨ç±»å‹ï¼Œå› ä¸º TextCNNTokenizer å’Œ HF Tokenizer çš„å‚æ•°ä¸åŒ
        if isinstance(self.tokenizer, TextCNNTokenizer):
            encoding = self.tokenizer(
                text,
                padding=True,
                truncation=True,
                return_tensors='pt',
                max_length=self.max_len
            )
        else: # å‡è®¾æ˜¯ HuggingFace Tokenizer
            encoding = self.tokenizer(
                text,
                add_special_tokens=True,
                max_length=self.max_len,
                return_token_type_ids=False,
                padding='max_length',
                truncation=True,
                return_attention_mask=True,
                return_tensors='pt',
            )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(self.label_to_id[label], dtype=torch.long)
        }

class SupervisedModel(nn.Module):
    """
    åŒ…è£…é¢„è®­ç»ƒçš„ç¼–ç å™¨å’Œä¸€ä¸ªåˆ†ç±»å¤´ã€‚
    """
    def __init__(self, base_encoder: nn.Module, num_labels: int, classifier_type: str = 'linear', mlp_layers: int = 2):
        super().__init__()
        self.base_encoder = base_encoder

        # ä»åŸºç¡€ç¼–ç å™¨è·å–éšè—å±‚ç»´åº¦
        if hasattr(base_encoder, 'base_dim'): # é€‚ç”¨äºæˆ‘ä»¬è‡ªå®šä¹‰çš„TextCNNModel
            hidden_size = base_encoder.base_dim
        elif hasattr(base_encoder.config, 'hidden_size'): # é€‚ç”¨äºHuggingFace/ModelScopeæ¨¡å‹
            hidden_size = base_encoder.config.hidden_size
        else:
            # å°è¯•ä»æ¨¡å‹è¾“å‡ºè·å–ç»´åº¦ï¼ˆå¦‚æœæ¨¡å‹å·²åŠ è½½å‚æ•°ï¼‰
            try:
                # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿè¾“å…¥æ¥æ¨æ–­ç»´åº¦
                dummy_input = {'input_ids': torch.randint(0, 100, (1, 10)), 'attention_mask': torch.ones(1, 10)}
                dummy_output = base_encoder(**dummy_input)

                # è°ƒè¯•ï¼šæ‰“å°è¾“å‡ºçš„æ‰€æœ‰é”®
                print(f"è°ƒè¯•: æ¨¡å‹è¾“å‡ºé”®: {dummy_output.keys()}")

                # ModelScope æ¨¡å‹é€šå¸¸è¿”å›å­—å…¸ï¼Œå°è¯•ä¸åŒçš„é”®
                if 'last_hidden_state' in dummy_output:
                    hidden_size = dummy_output['last_hidden_state'].shape[-1]
                elif 'hidden_states' in dummy_output:
                    hidden_size = dummy_output['hidden_states'].shape[-1]
                else:
                    raise KeyError("åœ¨æ¨¡å‹è¾“å‡ºä¸­æ‰¾ä¸åˆ° 'last_hidden_state' æˆ– 'hidden_states'ã€‚")

            except Exception as e:
                raise ValueError(f"æ— æ³•è‡ªåŠ¨ç¡®å®šåŸºç¡€ç¼–ç å™¨çš„è¾“å‡ºç»´åº¦: {e}")

        print(f"åˆ†ç±»å™¨æ¥æ”¶çš„éšè—å±‚ç»´åº¦: {hidden_size}")

        if classifier_type == 'linear':
            self.classifier = nn.Linear(hidden_size, num_labels)
        elif classifier_type == 'mlp':
            # åŠ¨æ€æ„å»ºMLPåˆ†ç±»å™¨
            layers = []
            current_dim = hidden_size

            # æ·»åŠ éšè—å±‚
            for i in range(mlp_layers - 1):
                next_dim = current_dim // 2
                layers.extend([
                    nn.Linear(current_dim, next_dim),
                    nn.ReLU(),
                    nn.Dropout(0.1)
                ])
                current_dim = next_dim

            # æ·»åŠ è¾“å‡ºå±‚
            layers.append(nn.Linear(current_dim, num_labels))

            self.classifier = nn.Sequential(*layers)
            print(f"MLPåˆ†ç±»å™¨ç»“æ„: {mlp_layers}å±‚ï¼Œç»´åº¦å˜åŒ–: {hidden_size} -> ... -> {num_labels}")
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ†ç±»å™¨ç±»å‹: {classifier_type}ã€‚è¯·é€‰æ‹© 'linear' æˆ– 'mlp'ã€‚")

    def forward(self, input_ids, attention_mask):
        # åŸºç¡€ç¼–ç å™¨è¾“å‡º
        # ModelScope æ¨¡å‹éœ€è¦å…³é”®å­—å‚æ•°
        base_output = self.base_encoder(input_ids=input_ids, attention_mask=attention_mask)
        
        # æ ¹æ®åŸºç¡€ç¼–ç å™¨çš„è¾“å‡ºç±»å‹æå–ç‰¹å¾
        # ModelScope æ¨¡å‹é€šå¸¸è¿”å›ä¸€ä¸ªå­—å…¸
        if isinstance(base_output, dict):
            if 'last_hidden_state' in base_output:
                # [CLS] token representation
                features = base_output['last_hidden_state'][:, 0, :]
            elif 'hidden_states' in base_output:
                # æœ‰äº›æ¨¡å‹å¯èƒ½åªè¿”å› hidden_states
                features = base_output['hidden_states'][:, 0, :]
            elif 'pooler_output' in base_output and base_output['pooler_output'] is not None:
                features = base_output['pooler_output']
            else:
                # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼ŒæŠ›å‡ºé”™è¯¯
                raise KeyError(f"åœ¨æ¨¡å‹è¾“å‡ºå­—å…¸ä¸­æ‰¾ä¸åˆ°å¯ç”¨çš„ç‰¹å¾é”®ã€‚å¯ç”¨é”®: {base_output.keys()}")
        elif hasattr(base_output, 'pooler_output') and base_output.pooler_output is not None:
            features = base_output.pooler_output
        elif hasattr(base_output, 'last_hidden_state'):
            features = base_output.last_hidden_state[:, 0, :]
        else:
            features = base_output

        # --- ä¿®æ­£ï¼šç¡®ä¿ features æ˜¯ float32 ---
        if features.dtype != torch.float32:
            features = features.to(torch.float32)
        # --------------------------------------

        logits = self.classifier(features)
        return logits

    def freeze_encoder(self):
        """å†»ç»“åŸºç¡€ç¼–ç å™¨çš„æ‰€æœ‰å‚æ•°ã€‚"""
        print("ğŸ§Š æ­£åœ¨å†»ç»“åŸºç¡€ç¼–ç å™¨çš„å‚æ•°...")
        for param in self.base_encoder.parameters():
            param.requires_grad = False
        print("âœ… åŸºç¡€ç¼–ç å™¨å·²å†»ç»“ã€‚")

    def unfreeze_encoder(self):
        """è§£å†»åŸºç¡€ç¼–ç å™¨çš„æ‰€æœ‰å‚æ•°ã€‚"""
        print("ğŸ”¥ æ­£åœ¨è§£å†»åŸºç¡€ç¼–ç å™¨çš„å‚æ•°...")
        for param in self.base_encoder.parameters():
            param.requires_grad = True
        print("âœ… åŸºç¡€ç¼–ç å™¨å·²è§£å†»ã€‚")


# --- 2. è¾…åŠ©å‡½æ•° ---

def set_seed(seed_value=42):
    """ä¸ºæ‰€æœ‰ç›¸å…³åº“è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å¯å¤ç°æ€§ã€‚"""
    random.seed(seed_value)
    np.random.seed(seed_value)
    torch.manual_seed(seed_value)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed_value)

def load_data(train_path, test_path):
    """åŠ è½½è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ã€‚"""
    try:
        df_train = pd.read_csv(train_path)
        df_test = pd.read_csv(test_path)
        print(f"åŠ è½½æ•°æ®: è®­ç»ƒé›† {len(df_train)} è¡Œ, æµ‹è¯•é›† {len(df_test)} è¡Œã€‚")
        return df_train, df_test
    except FileNotFoundError as e:
        print(f"é”™è¯¯: æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ° - {e}")
        return None, None

def load_pretrained_encoder(checkpoint_path: str):
    """
    ä»å¯¹æ¯”å­¦ä¹ é˜¶æ®µçš„checkpointåŠ è½½åŸºç¡€æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œæˆ–ç›´æ¥ä»ModelScope HubåŠ è½½ã€‚
    """
    if os.path.exists(checkpoint_path):
        print(f"æ­£åœ¨ä»æœ¬åœ°checkpoint {checkpoint_path} åŠ è½½...")
        checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'), weights_only=False)

        model_type = checkpoint.get('training_model_type', 'hf')
        model_identifier = checkpoint.get('training_model_identifier_or_path')
        proj_config = checkpoint.get('projection_head_config')
        use_peft = checkpoint.get('use_peft', False)
        peft_config = checkpoint.get('peft_config', None)

        temp_encoder = None
        if model_type == 'textcnn':
            textcnn_conf = checkpoint['textcnn_config']
            vocab_data = checkpoint['vocab']
            temp_encoder = ContrastiveEncoder(
                model_type='textcnn', vocab=vocab_data, textcnn_config=textcnn_conf,
                projection_hidden_dim=proj_config['hidden_dim'],
                projection_output_dim=proj_config['output_dim'],
                projection_dropout_rate=proj_config['dropout_rate']
            )
        elif model_type in ['hf', 'modelscope']:
            temp_encoder = ContrastiveEncoder(
                model_type='modelscope', model_name_or_path=model_identifier,
                projection_hidden_dim=proj_config['hidden_dim'],
                projection_output_dim=proj_config['output_dim'],
                projection_dropout_rate=proj_config['dropout_rate']
            )
            # --- LoRA/PEFTæ¨¡å‹ç‰¹æ®Šå¤„ç† ---
            if use_peft and peft_config is not None:
                print("æ£€æµ‹åˆ°PEFT/LoRAæ¨¡å‹ï¼Œæ­£åœ¨åº”ç”¨LoRAç»“æ„...")
                lora_config = LoraConfig(**peft_config)
                temp_encoder.base_model = get_peft_model(temp_encoder.base_model, lora_config)
                print("LoRAç»“æ„å·²åº”ç”¨ã€‚")
        else:
            print(f"é”™è¯¯: Checkpointä¸­æœªçŸ¥çš„æ¨¡å‹ç±»å‹ '{model_type}'")
            return None, None, None

        # åŠ è½½æƒé‡æ—¶å…è®¸strict=Falseï¼Œå…¼å®¹LoRAæƒé‡
        temp_encoder.load_state_dict(checkpoint['contrastive_encoder_state_dict'], strict=False)
        print(f"âœ… Checkpoint åŠ è½½æˆåŠŸã€‚æ¨¡å‹ç±»å‹: {model_type.upper()}")

        return temp_encoder.base_model, temp_encoder.tokenizer, model_type
    
    # å¦‚æœä¸æ˜¯æœ¬åœ°æ–‡ä»¶ï¼Œåˆ™å°è¯•ä» ModelScope Hub åŠ è½½
    else:
        print(f"æœªæ‰¾åˆ°æœ¬åœ°æ–‡ä»¶ï¼Œå°è¯•ä» ModelScope Hub åŠ è½½æ¨¡å‹: {checkpoint_path}")
        try:
            # --- å…³é”®ä¿®æ”¹: ä½¿ç”¨ AutoModel å’Œ AutoTokenizer ---
            base_model = AutoModel.from_pretrained(checkpoint_path, trust_remote_code=True)
            tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)
            print(f"âœ… æˆåŠŸä» ModelScope Hub åŠ è½½åŸºç¡€æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚")
            return base_model, tokenizer, 'ms' # è¿”å› 'ms' ä½œä¸ºæ¨¡å‹ç±»å‹
        except Exception as e:
            print(f"é”™è¯¯: æ— æ³•ä» ModelScope Hub åŠ è½½æ¨¡å‹ '{checkpoint_path}': {e}")
            return None, None, None


# --- 3. è®­ç»ƒå’Œè¯„ä¼°é€»è¾‘ ---

def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler):
    """æ‰§è¡Œä¸€ä¸ªè®­ç»ƒè½®æ¬¡ã€‚"""
    model.train()
    total_loss = 0
    for batch in tqdm(data_loader, desc="è®­ç»ƒä¸­", leave=False):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        loss = loss_fn(outputs, labels)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()
        total_loss += loss.item()
    return total_loss / len(data_loader)

def evaluate_model(model, data_loader, loss_fn, device, id_to_label: dict):
    """åœ¨éªŒè¯é›†æˆ–æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹ã€‚"""
    model.eval()
    total_loss = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for batch in tqdm(data_loader, desc="è¯„ä¼°ä¸­", leave=False):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            loss = loss_fn(outputs, labels)
            total_loss += loss.item()

            preds = torch.argmax(outputs, dim=1).cpu().numpy()
            all_preds.extend(preds)
            all_labels.extend(labels.cpu().numpy())

    # --- è®¡ç®—æ•´ä½“æŒ‡æ ‡ ---
    avg_loss = total_loss / len(data_loader)
    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)
    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)
    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)
    
    # --- è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡ ---
    class_ids = sorted(id_to_label.keys())
    # ä½¿ç”¨åŸå§‹æ ‡ç­¾ä½œä¸ºæŠ¥å‘Šä¸­çš„åç§°
    target_names = [f"class_{id_to_label[cid]}" for cid in class_ids]
    report_dict = classification_report(
        all_labels, 
        all_preds, 
        labels=class_ids,
        target_names=target_names,
        output_dict=True, 
        zero_division=0
    )
    # æå–æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡ï¼Œå¹¶ç§»é™¤ 'support'
    per_class_metrics = {}
    for name in target_names:
        if name in report_dict:
            metrics = report_dict[name].copy()
            metrics.pop('support', None)
            per_class_metrics[name] = metrics

    return {
        "loss": avg_loss,
        "accuracy": accuracy,
        "f1_score": f1,
        "precision": precision,
        "recall": recall,
        "per_class_metrics": per_class_metrics
    }

# --- 4. è¶…å‚æ•°æœç´¢é…ç½® ---

# ç»Ÿä¸€çš„å®éªŒé…ç½®å­—å…¸
CONFIG = {
    # å®éªŒå…ƒä¿¡æ¯
    'experiment_meta': {
        'description': 'baseline_comparison',  # å®éªŒæè¿°æ ‡è¯†ç¬¦
        'experiment_name': 'Baselineå¯¹æ¯”å®éªŒ',   # å®éªŒçš„ä¸­æ–‡åç§°
        'purpose': 'å¯¹æ¯”LoRAå¾®è°ƒåçš„BERTä¸åŸå§‹BERTåœ¨ä¸åŒæ•°æ®é‡ä¸‹çš„æ€§èƒ½è¡¨ç°',  # å®éªŒç›®çš„
        'notes': 'ä½¿ç”¨linear probeå’ŒMLPåˆ†ç±»å™¨ï¼Œæµ‹è¯•5ä¸ªä¸åŒæ•°æ®æ¯”ä¾‹',  # å®éªŒå¤‡æ³¨
    },

    # æ•°æ®é…ç½®
    'data': {
        'train_data_path': 'data/sup_train_data/trainset.csv',
        'test_data_path': 'data/sup_train_data/testset.csv',
        'validation_split': 0.2,  # éªŒè¯é›†æ¯”ä¾‹
        'excluded_labels': [5],   # è¦è¿‡æ»¤çš„æ ‡ç­¾
    },

    # æ¨¡å‹é…ç½®
    'models': {
        'lora_bert_base_chinese_cl': 'model/google-bert_bert-base-chinese/best_contrastive_model.pth',
        # 'TextCNN_CL_bert': 'model/my_custom_textcnn_v3_bert_pruning_paircl/best_contrastive_model.pth',
        'Bert_base_chinese_nocl': 'google-bert/bert-base-chinese',
    },

    # è¶…å‚æ•°æœç´¢ç©ºé—´
    'hyperparameters': {
        'epochs': [50],                    # è®­ç»ƒè½®æ•°
        'batch_size': [32],              # æ‰¹æ¬¡å¤§å°
        'learning_rate': [1e-3], # å­¦ä¹ ç‡
        'data_fractions': [1.0, 0.5, 0.2, 0.1, 0.05],  # æ•°æ®ä½¿ç”¨æ¯”ä¾‹
        'seeds': [42, 123, 456, 789, 101],             # éšæœºç§å­
        'classifier_types': ['linear'], # åˆ†ç±»å™¨ç±»å‹
        'mlp_layers': [1, 2, 3],          # MLPå±‚æ•° (ä»…åœ¨classifier_type='mlp'æ—¶ç”Ÿæ•ˆ)
        'freeze_encoder': [True],     # æ˜¯å¦å†»ç»“ç¼–ç å™¨
    },

    # å®éªŒæ§åˆ¶
    'experiment': {
        'base_output_dir': 'sup_result_hyperparams',  # åŸºç¡€è¾“å‡ºç›®å½•
        'save_individual_results': True,
        'aggregate_results': True,
        'save_experiment_info': True,  # ä¿å­˜å®éªŒä¿¡æ¯
    }
}

def generate_hyperparameter_combinations(config):
    """ç”Ÿæˆæ‰€æœ‰è¶…å‚æ•°ç»„åˆ"""
    hyperparams = config['hyperparameters']

    # åˆ›å»ºè¶…å‚æ•°ç»„åˆ
    combinations = []
    for (model_name, checkpoint_path), epochs, batch_size, lr, fraction, seed, classifier_type, mlp_layers, freeze in itertools.product(
        config['models'].items(),
        hyperparams['epochs'],
        hyperparams['batch_size'],
        hyperparams['learning_rate'],
        hyperparams['data_fractions'],
        hyperparams['seeds'],
        hyperparams['classifier_types'],
        hyperparams['mlp_layers'],
        hyperparams['freeze_encoder']
    ):
        # åªæœ‰å½“classifier_type='mlp'æ—¶ï¼Œmlp_layerså‚æ•°æ‰æœ‰æ„ä¹‰
        # å½“classifier_type='linear'æ—¶ï¼Œè·³è¿‡mlp_layers > 1çš„ç»„åˆ
        if classifier_type == 'linear' and mlp_layers > 1:
            continue

        combinations.append({
            'model_name': model_name,
            'checkpoint_path': checkpoint_path,
            'epochs': epochs,
            'batch_size': batch_size,
            'learning_rate': lr,
            'data_fraction': fraction,
            'seed': seed,
            'classifier_type': classifier_type,
            'mlp_layers': mlp_layers,
            'freeze_encoder': freeze,
        })

    return combinations

def run_single_experiment(config, hyperparams):
    """è¿è¡Œå•æ¬¡å®éªŒ"""

    # è®¾ç½®éšæœºç§å­
    set_seed(hyperparams['seed'])
    print(f"\n--- å®éªŒé…ç½® ---")
    print(f"æ¨¡å‹: {hyperparams['model_name']}")
    print(f"åˆ†ç±»å™¨: {hyperparams['classifier_type']}")
    if hyperparams['classifier_type'] == 'mlp':
        print(f"MLPå±‚æ•°: {hyperparams['mlp_layers']}")
    print(f"å†»ç»“ç¼–ç å™¨: {hyperparams['freeze_encoder']}")
    print(f"æ•°æ®æ¯”ä¾‹: {hyperparams['data_fraction']*100}%")
    print(f"å­¦ä¹ ç‡: {hyperparams['learning_rate']}")
    print(f"æ‰¹æ¬¡å¤§å°: {hyperparams['batch_size']}")
    print(f"è®­ç»ƒè½®æ•°: {hyperparams['epochs']}")
    print(f"éšæœºç§å­: {hyperparams['seed']}")

    # åŠ è½½æ•°æ®
    df_train_full, df_test = load_data(config['data']['train_data_path'], config['data']['test_data_path'])
    if df_train_full is None:
        return None

    # è¿‡æ»¤æ ‡ç­¾
    for label in config['data']['excluded_labels']:
        df_train_full = df_train_full[df_train_full['label'] != label].reset_index(drop=True)
        df_test = df_test[df_test['label'] != label].reset_index(drop=True)

    # åˆ’åˆ†éªŒè¯é›†
    df_train_prelim, df_val = train_test_split(
        df_train_full,
        test_size=config['data']['validation_split'],
        random_state=hyperparams['seed'],
        stratify=df_train_full['label']
    )

    # æ•°æ®é‡‡æ ·
    if hyperparams['data_fraction'] < 1.0:
        df_train = df_train_prelim.groupby('label', group_keys=False).apply(
            lambda x: x.sample(frac=hyperparams['data_fraction'], random_state=hyperparams['seed'])
        ).reset_index(drop=True)
    else:
        df_train = df_train_prelim.reset_index(drop=True)

    # ç”Ÿæˆæ ‡ç­¾æ˜ å°„
    unique_labels = sorted(df_train_full['label'].unique())
    label_to_id = {label: i for i, label in enumerate(unique_labels)}
    id_to_label = {i: label for label, i in label_to_id.items()}
    num_labels = len(unique_labels)

    # åŠ è½½é¢„è®­ç»ƒç¼–ç å™¨
    base_encoder, tokenizer, _ = load_pretrained_encoder(hyperparams['checkpoint_path'])
    if base_encoder is None:
        return None

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # åˆ›å»ºæ•°æ®é›†
    train_dataset = SupervisedTextDataset(df_train['content'].tolist(), df_train['label'].tolist(), tokenizer, label_to_id)
    val_dataset = SupervisedTextDataset(df_val['content'].tolist(), df_val['label'].tolist(), tokenizer, label_to_id)
    test_dataset = SupervisedTextDataset(df_test['content'].tolist(), df_test['label'].tolist(), tokenizer, label_to_id)

    train_loader = DataLoader(train_dataset, batch_size=hyperparams['batch_size'], shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=hyperparams['batch_size'])
    test_loader = DataLoader(test_dataset, batch_size=hyperparams['batch_size'])

    # æ„å»ºç›‘ç£æ¨¡å‹
    model = SupervisedModel(
        base_encoder=base_encoder,
        num_labels=num_labels,
        classifier_type=hyperparams['classifier_type'],
        mlp_layers=hyperparams['mlp_layers'] if hyperparams['classifier_type'] == 'mlp' else 1
    ).to(device)

    # è®¾ç½®ä¼˜åŒ–å™¨
    if hyperparams['freeze_encoder']:
        model.freeze_encoder()
        optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=hyperparams['learning_rate'])
    else:
        model.unfreeze_encoder()
        optimizer = AdamW(model.parameters(), lr=hyperparams['learning_rate'])

    # å‡†å¤‡è®­ç»ƒ
    loss_fn = nn.CrossEntropyLoss()
    total_steps = len(train_loader) * hyperparams['epochs']
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)

    best_val_f1 = -1
    best_model_state = None

    # è®­ç»ƒå¾ªç¯
    for epoch in range(hyperparams['epochs']):
        print(f"  Epoch {epoch + 1}/{hyperparams['epochs']}")
        train_loss = train_epoch(model, train_loader, loss_fn, optimizer, device, scheduler)
        val_metrics = evaluate_model(model, val_loader, loss_fn, device, id_to_label)
        print(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f} | éªŒè¯æŸå¤±: {val_metrics['loss']:.4f} | éªŒè¯F1: {val_metrics['f1_score']:.4f}")

        if val_metrics['f1_score'] > best_val_f1:
            best_val_f1 = val_metrics['f1_score']
            best_model_state = model.state_dict()
            print(f"  ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯F1åˆ†æ•°: {best_val_f1:.4f}")

    # æµ‹è¯•é›†è¯„ä¼°
    if best_model_state:
        model.load_state_dict(best_model_state)
    print("ğŸ§ª ä½¿ç”¨æœ€ä½³æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
    test_metrics = evaluate_model(model, test_loader, loss_fn, device, id_to_label)
    print(f"  æµ‹è¯•é›†ç»“æœ -> æŸå¤±: {test_metrics['loss']:.4f}, å‡†ç¡®ç‡: {test_metrics['accuracy']:.4f}, F1åˆ†æ•°: {test_metrics['f1_score']:.4f}")

    # æ·»åŠ è¶…å‚æ•°ä¿¡æ¯åˆ°ç»“æœä¸­
    result = {
        'hyperparameters': hyperparams,
        'metrics': test_metrics,
        'best_val_f1': best_val_f1
    }

    return result

def save_experiment_results(results, config):
    """ä¿å­˜å®éªŒç»“æœ"""
    # åˆ›å»ºå®éªŒç‰¹å®šçš„è¾“å‡ºç›®å½•
    experiment_id = config['experiment_meta']['description']
    output_dir = os.path.join(config['experiment']['base_output_dir'], experiment_id)
    os.makedirs(output_dir, exist_ok=True)

    # ä¿å­˜å®éªŒä¿¡æ¯
    if config['experiment']['save_experiment_info']:
        experiment_info = {
            'experiment_meta': config['experiment_meta'],
            'data_config': config['data'],
            'models': config['models'],
            'hyperparameters': config['hyperparameters'],
            'total_experiments': len(results),
            'successful_experiments': sum(1 for r in results if r is not None),
            'timestamp': pd.Timestamp.now().isoformat()
        }

        info_filepath = os.path.join(output_dir, 'experiment_info.json')
        with open(info_filepath, 'w', encoding='utf-8') as f:
            json.dump(experiment_info, f, ensure_ascii=False, indent=4)
        print(f"ğŸ“‹ å®éªŒä¿¡æ¯å·²ä¿å­˜åˆ°: {info_filepath}")

    if config['experiment']['save_individual_results']:
        # ä¿å­˜æ¯ä¸ªå®éªŒçš„è¯¦ç»†ç»“æœ
        for i, result in enumerate(results):
            if result is None:
                continue
            hyperparams = result['hyperparameters']
            filename = (
                f"{hyperparams['model_name']}_{hyperparams['classifier_type']}"
            )
            if hyperparams['classifier_type'] == 'mlp':
                filename += f"_{hyperparams['mlp_layers']}layers"
            filename += (
                f"_freeze{hyperparams['freeze_encoder']}_"
                f"ep{hyperparams['epochs']}_bs{hyperparams['batch_size']}_"
                f"lr{hyperparams['learning_rate']}_"
                f"frac{int(hyperparams['data_fraction']*100)}_"
                f"seed{hyperparams['seed']}.json"
            )
            filepath = os.path.join(output_dir, filename)
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=4)

    if config['experiment']['aggregate_results']:
        # èšåˆç»“æœåˆ†æ
        aggregate_results = {}
        for result in results:
            if result is None:
                continue

            hyperparams = result['hyperparameters']
            metrics = result['metrics']

            # åˆ›å»ºé…ç½®é”®
            config_key = (
                hyperparams['model_name'],
                hyperparams['classifier_type'],
                hyperparams['mlp_layers'] if hyperparams['classifier_type'] == 'mlp' else 1,
                hyperparams['freeze_encoder'],
                hyperparams['epochs'],
                hyperparams['batch_size'],
                hyperparams['learning_rate'],
                hyperparams['data_fraction']
            )

            if config_key not in aggregate_results:
                aggregate_results[config_key] = []

            aggregate_results[config_key].append({
                'seed': hyperparams['seed'],
                'accuracy': metrics['accuracy'],
                'f1_score': metrics['f1_score'],
                'precision': metrics['precision'],
                'recall': metrics['recall']
            })

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        summary_results = {
            'experiment_meta': config['experiment_meta'],  # æ·»åŠ å®éªŒå…ƒä¿¡æ¯
            'results': {}
        }

        for config_key, runs in aggregate_results.items():
            if len(runs) > 0:
                df_runs = pd.DataFrame(runs)
                summary_results['results'][str(config_key)] = {
                    'config': {
                        'model_name': config_key[0],
                        'classifier_type': config_key[1],
                        'mlp_layers': config_key[2],
                        'freeze_encoder': config_key[3],
                        'epochs': config_key[4],
                        'batch_size': config_key[5],
                        'learning_rate': config_key[6],
                        'data_fraction': config_key[7]
                    },
                    'mean_metrics': df_runs.mean().to_dict(),
                    'std_metrics': df_runs.std().to_dict(),
                    'num_runs': len(runs),
                    'all_runs': runs
                }

        # ä¿å­˜èšåˆç»“æœ
        summary_filepath = os.path.join(output_dir, 'hyperparameter_search_summary.json')
        with open(summary_filepath, 'w', encoding='utf-8') as f:
            json.dump(summary_results, f, ensure_ascii=False, indent=4)

        print(f"\nâœ… èšåˆç»“æœå·²ä¿å­˜åˆ°: {summary_filepath}")

    return output_dir


# --- 6. ä¸»å‡½æ•° ---

if __name__ == '__main__':
    print("ğŸš€ å¼€å§‹è¶…å‚æ•°æœç´¢å®éªŒ...")
    print(f"ğŸ“ å®éªŒåç§°: {CONFIG['experiment_meta']['experiment_name']}")
    print(f"ğŸ¯ å®éªŒç›®çš„: {CONFIG['experiment_meta']['purpose']}")
    print(f"ğŸ“„ å®éªŒå¤‡æ³¨: {CONFIG['experiment_meta']['notes']}")

    # ç”Ÿæˆæ‰€æœ‰è¶…å‚æ•°ç»„åˆ
    combinations = generate_hyperparameter_combinations(CONFIG)
    print(f"ğŸ“Š æ€»å…±éœ€è¦è¿è¡Œ {len(combinations)} ä¸ªå®éªŒé…ç½®")

    # è¿è¡Œæ‰€æœ‰å®éªŒ
    all_results = []
    for i, hyperparams in enumerate(combinations):
        print(f"\n{'='*80}")
        print(f"å®éªŒè¿›åº¦: {i+1}/{len(combinations)}")
        print(f"{'='*80}")

        result = run_single_experiment(CONFIG, hyperparams)
        all_results.append(result)

        # å¯é€‰ï¼šæ¯å®Œæˆå‡ ä¸ªå®éªŒå°±ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ
        if (i + 1) % 10 == 0:
            print(f"\nğŸ’¾ ä¿å­˜ä¸­é—´ç»“æœ... (å·²å®Œæˆ {i+1}/{len(combinations)} ä¸ªå®éªŒ)")
            experiment_dir = save_experiment_results(all_results, CONFIG)

    # ä¿å­˜æœ€ç»ˆç»“æœ
    print(f"\nğŸ’¾ ä¿å­˜æœ€ç»ˆå®éªŒç»“æœ...")
    experiment_dir = save_experiment_results(all_results, CONFIG)

    print(f"\nğŸ‰ æ‰€æœ‰è¶…å‚æ•°æœç´¢å®éªŒå·²å®Œæˆï¼")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {experiment_dir}")
    print(f"ğŸ“‹ å®éªŒæè¿°: {CONFIG['experiment_meta']['description']}")

# --- å…¶ä»–å®éªŒé…ç½®ç¤ºä¾‹ ---

# ä½ å¯ä»¥å¤åˆ¶ä»¥ä¸‹é…ç½®ç¤ºä¾‹ï¼Œä¿®æ”¹experiment_metaéƒ¨åˆ†ï¼Œè¿›è¡Œä¸åŒçš„å®éªŒå¯¹æ¯”

EXPERIMENT_CONFIGS = {
    "learning_rate_comparison": {
        'experiment_meta': {
            'description': 'learning_rate_comparison',
            'experiment_name': 'å­¦ä¹ ç‡å¯¹æ¯”å®éªŒ',
            'purpose': 'æµ‹è¯•ä¸åŒå­¦ä¹ ç‡å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“',
            'notes': 'å›ºå®šå…¶ä»–å‚æ•°ï¼Œå¯¹æ¯”1e-3, 2e-3, 5e-3ä¸‰ç§å­¦ä¹ ç‡',
        },
        'hyperparameters': {
            'epochs': [20],
            'batch_size': [32],
            'learning_rate': [1e-3, 2e-3, 5e-3],
            'data_fractions': [1.0],
            'seeds': [42, 123, 456],
            'classifier_types': ['linear'],
            'mlp_layers': [1],  # linearåˆ†ç±»å™¨æ—¶MLPå±‚æ•°æ— æ„ä¹‰
            'freeze_encoder': [True],
        },
    },

    "data_efficiency": {
        'experiment_meta': {
            'description': 'data_efficiency',
            'experiment_name': 'æ•°æ®æ•ˆç‡åˆ†æ',
            'purpose': 'åˆ†ææ¨¡å‹åœ¨ä¸åŒæ•°æ®é‡ä¸‹çš„å­¦ä¹ æ•ˆç‡',
            'notes': 'å›ºå®šæœ€ä¼˜å‚æ•°ï¼Œæµ‹è¯•æ•°æ®ç¨€ç¼ºåœºæ™¯ä¸‹çš„æ€§èƒ½è¡°å‡',
        },
        'hyperparameters': {
            'epochs': [50],
            'batch_size': [32],
            'learning_rate': [1e-3],
            'data_fractions': [1.0, 0.8, 0.6, 0.4, 0.2, 0.1, 0.05, 0.01],
            'seeds': [42, 123, 456, 789, 101],
            'classifier_types': ['linear', 'mlp'],
            'mlp_layers': [1, 2, 3],  # æµ‹è¯•ä¸åŒMLPå±‚æ•°
            'freeze_encoder': [True],
        },
    },

    "architecture_comparison": {
        'experiment_meta': {
            'description': 'architecture_comparison',
            'experiment_name': 'æ¶æ„å¯¹æ¯”å®éªŒ',
            'purpose': 'å¯¹æ¯”Linear Probeå’ŒMLPåˆ†ç±»å™¨çš„æ€§èƒ½å·®å¼‚',
            'notes': 'åœ¨ç›¸åŒæ¡ä»¶ä¸‹æµ‹è¯•ä¸¤ç§åˆ†ç±»å™¨æ¶æ„',
        },
        'hyperparameters': {
            'epochs': [30],
            'batch_size': [16, 32],
            'learning_rate': [1e-3, 2e-3],
            'data_fractions': [1.0, 0.5, 0.2],
            'seeds': [42, 123, 456],
            'classifier_types': ['linear', 'mlp'],
            'mlp_layers': [1, 2, 3],  # å¯¹æ¯”ä¸åŒMLPå±‚æ•°çš„æ•ˆæœ
            'freeze_encoder': [True, False],
        },
    }
}

# ä½¿ç”¨æ–¹æ³•ï¼š
# 1. å°†ä¸Šè¿°ä»»æ„é…ç½®å¤åˆ¶åˆ°ä¸»CONFIGä¸­
# 2. ä¿®æ”¹experiment_metaå­—æ®µæ¥æè¿°ä½ çš„å®éªŒ
# 3. è¿è¡Œè„šæœ¬å³å¯