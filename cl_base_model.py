import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Dict, Optional, Union
import jieba
import json
import os
from modelscope import AutoModel, AutoTokenizer
from peft import get_peft_model, LoraConfig


def load_model_from_modelscope(model_name_or_path: str, trust_remote_code: bool = True, **kwargs):
    """
    ä¼˜å…ˆä»ModelScopeåŠ è½½æ¨¡å‹ï¼Œå¤±è´¥æ—¶æç¤ºç”¨æˆ·
    """
    try:
        print(f"ğŸ” æ­£åœ¨ä»ModelScopeåŠ è½½æ¨¡å‹: {model_name_or_path}")
        model = AutoModel.from_pretrained(
            model_name_or_path, 
            trust_remote_code=trust_remote_code,
            **kwargs
        )
        print(f"âœ… æˆåŠŸä»ModelScopeåŠ è½½æ¨¡å‹: {model_name_or_path}")
        return model
    except Exception as e:
        print(f"âŒ ä»ModelScopeåŠ è½½æ¨¡å‹å¤±è´¥: {model_name_or_path}")
        print(f"é”™è¯¯ä¿¡æ¯: {e}")
        print("è¯·æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®ï¼Œæˆ–ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸ã€‚")
        raise e


def load_tokenizer_from_modelscope(model_name_or_path: str, trust_remote_code: bool = True, **kwargs):
    """
    ä¼˜å…ˆä»ModelScopeåŠ è½½åˆ†è¯å™¨ï¼Œå¤±è´¥æ—¶æç¤ºç”¨æˆ·
    """
    try:
        print(f"ğŸ” æ­£åœ¨ä»ModelScopeåŠ è½½åˆ†è¯å™¨: {model_name_or_path}")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name_or_path, 
            trust_remote_code=trust_remote_code,
            **kwargs
        )
        print(f"âœ… æˆåŠŸä»ModelScopeåŠ è½½åˆ†è¯å™¨: {model_name_or_path}")
        return tokenizer
    except Exception as e:
        print(f"âŒ ä»ModelScopeåŠ è½½åˆ†è¯å™¨å¤±è´¥: {model_name_or_path}")
        print(f"é”™è¯¯ä¿¡æ¯: {e}")
        print("è¯·æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®ï¼Œæˆ–ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸ã€‚")
        raise e


class TextCNNTokenizer:
    """
    ä½¿ç”¨é¢„æ„å»ºè¯æ±‡è¡¨å’Œjiebaçš„TextCNNæ¨¡å‹åˆ†è¯å™¨ã€‚
    """
    def __init__(self, word_to_idx: Dict[str, int], max_length: int = 128):
        self.word_to_idx = word_to_idx
        self.max_length = max_length
        self.pad_token = "<pad>"
        self.unk_token = "<unk>"
        self.pad_token_id = word_to_idx.get(self.pad_token, 0)
        self.unk_token_id = word_to_idx.get(self.unk_token, 1)

    def tokenize(self, text: str) -> List[str]:
        if not isinstance(text, str):
            text = str(text)
        return jieba.lcut(text.strip())

    def convert_tokens_to_ids(self, tokens: List[str]) -> List[int]:
        return [self.word_to_idx.get(token, self.unk_token_id) for token in tokens]

    def _pad_truncate_single(self, token_ids: List[int], max_len: int) -> tuple[List[int], List[int]]:
        attention_mask = [1] * len(token_ids)
        if len(token_ids) < max_len:
            padding_len = max_len - len(token_ids)
            token_ids.extend([self.pad_token_id] * padding_len)
            attention_mask.extend([0] * padding_len)
        elif len(token_ids) > max_len:
            token_ids = token_ids[:max_len]
            attention_mask = attention_mask[:max_len]
        return token_ids, attention_mask

    def __call__(self, texts: Union[str, List[str]], padding: Union[bool, str] = True, 
                 truncation: Union[bool, str] = True, return_tensors: Optional[str] = None, 
                 max_length: Optional[int] = None) -> Dict[str, Union[List[List[int]], torch.Tensor]]:
        if isinstance(texts, str):
            texts = [texts]
        
        # ç¡®ä¿æ‰€æœ‰è¾“å…¥éƒ½æ˜¯å­—ç¬¦ä¸²
        processed_texts = []
        for i, text_input in enumerate(texts):
            if not isinstance(text_input, str):
                processed_texts.append(str(text_input))
            else:
                processed_texts.append(text_input)
        texts = processed_texts
        
        effective_max_length = max_length if max_length is not None else self.max_length

        batch_input_ids = []
        batch_attention_masks = []

        for text in texts:
            tokens = self.tokenize(text)
            token_ids = self.convert_tokens_to_ids(tokens)
            
            # æ ¹æ® truncation å’Œ padding å‚æ•°å¤„ç†
            if truncation:
                token_ids = token_ids[:effective_max_length]
            
            # åªæœ‰åœ¨ padding ä¸º True æ—¶æ‰è¿›è¡Œå¡«å……
            if padding:
                current_len = len(token_ids)
                attention_mask = [1] * current_len
                if current_len < effective_max_length:
                    pad_len = effective_max_length - current_len
                    token_ids.extend([self.pad_token_id] * pad_len)
                    attention_mask.extend([0] * pad_len)
                elif current_len > effective_max_length:
                    token_ids = token_ids[:effective_max_length]
                    attention_mask = [1] * effective_max_length

                padded_ids = token_ids
            else:
                padded_ids = token_ids 
                attention_mask = [1] * len(token_ids)

            batch_input_ids.append(padded_ids)
            batch_attention_masks.append(attention_mask)

        if return_tensors == 'pt':
            if not padding:
                max_len_in_batch = 0
                if batch_input_ids:
                    max_len_in_batch = max(len(ids) for ids in batch_input_ids) if batch_input_ids else 0
                
                if max_len_in_batch > 0:
                    for i in range(len(batch_input_ids)):
                        ids = batch_input_ids[i]
                        mask = batch_attention_masks[i]
                        len_diff = max_len_in_batch - len(ids)
                        if len_diff > 0:
                            batch_input_ids[i] = ids + [self.pad_token_id] * len_diff
                            batch_attention_masks[i] = mask + [0] * len_diff
            try:
                batch_input_ids_tensor = torch.tensor(batch_input_ids, dtype=torch.long)
                batch_attention_masks_tensor = torch.tensor(batch_attention_masks, dtype=torch.long)
            except RuntimeError as e:
                print(f"å°†åˆ—è¡¨è½¬æ¢ä¸ºå¼ é‡æ—¶å‡ºé”™ã€‚æ£€æŸ¥åºåˆ—é•¿åº¦æ˜¯å¦ä¸€è‡´ã€‚é”™è¯¯: {e}")
                print(f"æ‰¹å¤„ç†è¾“å…¥IDé•¿åº¦: {[len(ids) for ids in batch_input_ids]}")
                raise e

            return {
                'input_ids': batch_input_ids_tensor,
                'attention_mask': batch_attention_masks_tensor
            }
        else:
            return {
                'input_ids': batch_input_ids,
                'attention_mask': batch_attention_masks
            }


class TextCNNModel(torch.nn.Module):
    """
    ç”¨äºå¥å­åµŒå…¥çš„TextCNNæ¨¡å‹ã€‚
    """
    def __init__(self, vocab_size: int, embedding_dim: int, num_filters: int, 
                 filter_sizes: List[int], output_dim_for_encoder: int, dropout_rate: float = 0.1):
        super().__init__()
        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.convs = torch.nn.ModuleList([
            torch.nn.Conv1d(in_channels=embedding_dim, 
                            out_channels=num_filters, 
                            kernel_size=K) 
            for K in filter_sizes
        ])
        self.dropout = torch.nn.Dropout(dropout_rate)
        self.fc = torch.nn.Linear(len(filter_sizes) * num_filters, output_dim_for_encoder)
        self.base_dim = output_dim_for_encoder

    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # input_ids: [batch_size, seq_len]
        embedded = self.embedding(input_ids)  # [batch_size, seq_len, embedding_dim]
        
        # (å¯é€‰) åº”ç”¨ attention_mask åˆ°åµŒå…¥å±‚ï¼Œå°†å¡«å……ä½ç½®çš„åµŒå…¥ç½®é›¶
        if attention_mask is not None:
            embedded = embedded * attention_mask.unsqueeze(-1).float()

        embedded = embedded.permute(0, 2, 1)  # [batch_size, embedding_dim, seq_len]
        
        conved = [F.relu(conv(embedded)) for conv in self.convs]
        # conved[i]: [batch_size, num_filters, seq_len - filter_sizes[i] + 1]
        
        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]
        # pooled[i]: [batch_size, num_filters]
        
        cat = self.dropout(torch.cat(pooled, dim=1)) # [batch_size, num_filters * len(filter_sizes)]
        output = self.fc(cat) # [batch_size, output_dim_for_encoder]
        return output


class ContrastiveEncoder(torch.nn.Module):
    """
    ğŸ”§ ä¿®æ”¹åçš„å¯¹æ¯”ç¼–ç å™¨ï¼šæ”¯æŒModelScope AutoModelå’Œè‡ªå®šä¹‰TextCNNã€‚
    """
    def __init__(self, model_type: str,
                 model_name_or_path: Optional[str] = None,
                 vocab: Optional[Dict[str, int]] = None,
                 textcnn_config: Optional[Dict] = None,
                 projection_hidden_dim: int = 512, 
                 projection_output_dim: int = 256, 
                 projection_dropout_rate: float = 0.1):
        super().__init__()
        self.model_type = model_type.lower()
        self.tokenizer = None
        self.base_model = None
        self.base_dim = 0

        if self.model_type == 'modelscope':
            if model_name_or_path is None:
                raise ValueError("å¯¹äº 'modelscope' æ¨¡å‹ç±»å‹ï¼Œmodel_name_or_path æ˜¯å¿…éœ€çš„")
            
            # ä½¿ç”¨ModelScopeåŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
            self.tokenizer = load_tokenizer_from_modelscope(model_name_or_path)
            self.base_model = load_model_from_modelscope(
                model_name_or_path,
                torch_dtype=torch.float32
            )
            
            if hasattr(self.base_model.config, 'hidden_size'):
                 self.base_dim = self.base_model.config.hidden_size
            elif hasattr(self.base_model.config, 'd_model'):
                 self.base_dim = self.base_model.config.d_model
            else:
                try:
                    dummy_input = self.tokenizer("test", return_tensors="pt")
                    if 'token_type_ids' in dummy_input and not any(p.name == 'token_type_ids' for p in self.base_model.forward.__code__.co_varnames):
                        del dummy_input['token_type_ids']
                    dummy_output = self.base_model(**dummy_input)
                    if hasattr(dummy_output, 'last_hidden_state'):
                        self.base_dim = dummy_output.last_hidden_state.shape[-1]
                    elif isinstance(dummy_output, torch.Tensor):
                         self.base_dim = dummy_output.shape[-1]
                    else:
                        raise ValueError(f"æ— æ³•è‡ªåŠ¨ç¡®å®šModelScopeæ¨¡å‹ {model_name_or_path} çš„åŸºç¡€ç»´åº¦ã€‚è¯·æ£€æŸ¥æ¨¡å‹é…ç½®ã€‚")
                except Exception as e:
                     raise ValueError(f"å°è¯•ç¡®å®šModelScopeæ¨¡å‹ {model_name_or_path} åŸºç¡€ç»´åº¦æ—¶å‡ºé”™: {e}")

            print(f"ğŸ—ï¸ ModelScope ContrastiveEncoder åˆå§‹åŒ–å®Œæˆ:")
            print(f"   åŸºç¡€æ¨¡å‹: {model_name_or_path}")

        elif self.model_type == 'textcnn':
            if vocab is None or textcnn_config is None:
                raise ValueError("å¯¹äº 'textcnn' æ¨¡å‹ç±»å‹ï¼Œvocab å’Œ textcnn_config æ˜¯å¿…éœ€çš„")
            
            _max_len = textcnn_config.get('max_seq_length', 128)
            self.tokenizer = TextCNNTokenizer(vocab, max_length=_max_len)
            self.base_model = TextCNNModel(
                vocab_size=len(vocab),
                embedding_dim=textcnn_config['embedding_dim'],
                num_filters=textcnn_config['num_filters'],
                filter_sizes=textcnn_config['filter_sizes'],
                output_dim_for_encoder=textcnn_config['textcnn_output_dim'],
                dropout_rate=textcnn_config.get('model_dropout_rate', 0.1)
            )
            self.base_dim = textcnn_config['textcnn_output_dim']
            print(f"ğŸ—ï¸ TextCNN ContrastiveEncoder åˆå§‹åŒ–å®Œæˆ:")
            print(f"   TextCNN è¯æ±‡è¡¨å¤§å°: {len(vocab)}")
            print(f"   TextCNN é…ç½®: {textcnn_config}")
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model_type}ã€‚è¯·é€‰æ‹© 'modelscope' æˆ– 'textcnn'ã€‚")

        # æŠ•å½±å¤´ (ä¸¤ç§æ¨¡å‹ç±»å‹é€šç”¨)
        self.projection_head = torch.nn.Sequential(
            torch.nn.Linear(self.base_dim, projection_hidden_dim),
            torch.nn.ReLU(),
            torch.nn.Dropout(projection_dropout_rate),
            torch.nn.Linear(projection_hidden_dim, projection_output_dim),
            torch.nn.LayerNorm(projection_output_dim)
        ).float()
        
        print(f"   åŸºç¡€æ¨¡å‹è¾“å‡ºç»´åº¦ (åˆ°æŠ•å½±å¤´): {self.base_dim}")
        print(f"   æŠ•å½±å¤´è¾“å…¥ç»´åº¦: {self.base_dim}")
        print(f"   æŠ•å½±å¤´éšè—å±‚ç»´åº¦: {projection_hidden_dim}")
        print(f"   æŠ•å½±å¤´è¾“å‡ºç»´åº¦ (æœ€ç»ˆåµŒå…¥): {projection_output_dim}")
        if self.base_model:
             print(f"   åŸºç¡€æ¨¡å‹æ•°æ®ç±»å‹: {next(self.base_model.parameters()).dtype}")
        print(f"   æŠ•å½±å¤´æ•°æ®ç±»å‹: {next(self.projection_head.parameters()).dtype}")

    def _ensure_float32(self, tensor):
        if tensor.dtype in [torch.bfloat16, torch.float16]:
            return tensor.float()
        return tensor

    def _ensure_list_of_strings(self, texts: Union[str, List]) -> List[str]:
        if isinstance(texts, str):
            texts = [texts]
        
        processed_texts = []
        for text_input in texts:
            if not isinstance(text_input, str):
                processed_texts.append(str(text_input) if text_input is not None else "")
            else:
                processed_texts.append(text_input if text_input else "")
        return processed_texts

    def get_base_embeddings(self, texts: Union[str, List]):
        """
        è·å–åŸºç¡€æ¨¡å‹çš„åµŒå…¥ï¼ˆåœ¨æŠ•å½±å¤´ä¹‹å‰ï¼‰ã€‚
        """
        processed_texts = self._ensure_list_of_strings(texts)
        if not processed_texts:
            return torch.empty(0, self.base_dim, device=next(self.parameters()).device)

        device = next(self.parameters()).device
        
        max_len = 512
        if self.model_type == 'textcnn':
            max_len = self.tokenizer.max_length
        elif hasattr(self.tokenizer, 'model_max_length'):
            max_len = self.tokenizer.model_max_length

        inputs = self.tokenizer(
            processed_texts, 
            padding=True, 
            truncation=True, 
            return_tensors='pt', 
            max_length=max_len
        )
        inputs = {key: value.to(device) for key, value in inputs.items()}

        outputs = self.base_model(**inputs)

        if self.model_type == 'modelscope':
            if hasattr(outputs, 'last_hidden_state'):
                last_hidden = self._ensure_float32(outputs.last_hidden_state)
                attention_mask = inputs['attention_mask']
                mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()
                sum_embeddings = torch.sum(last_hidden * mask_expanded, 1)
                sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)
                base_embeddings = sum_embeddings / sum_mask
            elif hasattr(outputs, 'pooler_output'):
                base_embeddings = self._ensure_float32(outputs.pooler_output)
            elif isinstance(outputs, torch.Tensor):
                base_embeddings = self._ensure_float32(outputs)
            else:
                raise ValueError("æ— æ³•ä»ModelScopeæ¨¡å‹è¾“å‡ºç¡®å®šåŸºç¡€åµŒå…¥ã€‚")
        elif self.model_type == 'textcnn':
            base_embeddings = self._ensure_float32(outputs)
        else:
            raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {self.model_type}")

        return base_embeddings

    def forward(self, texts: Union[str, List]):
        """
        å®šä¹‰æ¨¡å‹çš„å®Œæ•´å‰å‘ä¼ æ’­ï¼šåŸºç¡€æ¨¡å‹ -> æŠ•å½±å¤´ã€‚
        """
        base_embeddings = self.get_base_embeddings(texts)
        
        if base_embeddings.nelement() == 0:
            proj_output_dim = self.projection_head[-2].out_features
            return torch.empty(0, proj_output_dim, device=base_embeddings.device, dtype=torch.float)
            
        projected_embeddings = self.projection_head(base_embeddings)
        return projected_embeddings

    def save_base_model(self, path: str):
        """ä¿å­˜åŸºç¡€æ¨¡å‹ (ModelScope æˆ– TextCNN) åŠå…¶åˆ†è¯å™¨/è¯æ±‡è¡¨ã€‚"""
        os.makedirs(path, exist_ok=True)
        if self.model_type == 'modelscope':
            self.base_model.save_pretrained(path)
            self.tokenizer.save_pretrained(path)
            print(f"ğŸ’¾ ModelScope åŸºç¡€æ¨¡å‹å’Œåˆ†è¯å™¨å·²ä¿å­˜åˆ°: {path}")
        elif self.model_type == 'textcnn':
            model_save_path = os.path.join(path, "textcnn_model.pth")
            vocab_save_path = os.path.join(path, "textcnn_vocab.json")
            tokenizer_config_path = os.path.join(path, "textcnn_tokenizer_config.json")

            torch.save(self.base_model.state_dict(), model_save_path)
            with open(vocab_save_path, 'w', encoding='utf-8') as f:
                json.dump(self.tokenizer.word_to_idx, f, ensure_ascii=False, indent=4)
            
            tokenizer_config = {'max_length': self.tokenizer.max_length}
            with open(tokenizer_config_path, 'w', encoding='utf-8') as f:
                json.dump(tokenizer_config, f, ensure_ascii=False, indent=4)

            print(f"ğŸ’¾ TextCNN åŸºç¡€æ¨¡å‹ state_dict å·²ä¿å­˜åˆ°: {model_save_path}")
            print(f"ğŸ’¾ TextCNN è¯æ±‡è¡¨å·²ä¿å­˜åˆ°: {vocab_save_path}")
            print(f"ğŸ’¾ TextCNN åˆ†è¯å™¨é…ç½®å·²ä¿å­˜åˆ°: {tokenizer_config_path}")
        else:
            print(f"âš ï¸ æœªçŸ¥æ¨¡å‹ç±»å‹ '{self.model_type}'ï¼Œæ— æ³•ä¿å­˜åŸºç¡€æ¨¡å‹ã€‚")