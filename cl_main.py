import pandas as pd
from Tree_data_model import PostStorage
from cl_training import DynamicContrastiveTrainer

def main_training_pipeline():
    print("ğŸš€ å¼€å§‹è®­ç»ƒæµç¨‹...")
    # 1. å‡†å¤‡æ•°æ®
    try:
        comment_df = pd.read_csv('data/cl_data/train_comments_filtered.csv', encoding='utf-8')
        post_df = pd.read_csv('data/cl_data/train_posts_filtered.csv', encoding='utf-8')
    except FileNotFoundError:
        print("é”™è¯¯: comments_data.csv æˆ– contents_data.csv æœªæ‰¾åˆ°ã€‚è¯·ç¡®ä¿æ•°æ®æ–‡ä»¶å­˜åœ¨ã€‚")
        return      
    

    # ç¡®ä¿ note_id å’Œ comment_id æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œä»¥é¿å…åç»­é—®é¢˜
    comment_df['note_id'] = comment_df['note_id'].astype(str)
    comment_df['comment_id'] = comment_df['comment_id'].astype(str)
    comment_df['parent_comment_id'] = comment_df['parent_comment_id'].astype(str)
    post_df['note_id'] = post_df['note_id'].astype(str)


    storage = PostStorage()
    # ç¡®ä¿å¸–å­å†…å®¹æ˜¯å­—ç¬¦ä¸²ï¼Œå¦‚æœ title ä¸å­˜åœ¨ï¼Œå°è¯• contentï¼Œå¦‚æœéƒ½ä¸ºç©ºï¼Œåˆ™ä¸ºç©ºå­—ç¬¦ä¸²
    for _, row in post_df.iterrows():
        post_content = str(row.get('title', '')) or str(row.get('content', '')) # ä¿è¯æ˜¯å­—ç¬¦ä¸²
        storage.add_post(post_id=str(row['note_id']), post_content=post_content)

    for _, row in comment_df.iterrows():
        post_id_str = str(row['note_id'])
        comment_id_str = str(row['comment_id'])
        content_str = str(row.get('content', '')) # ç¡®ä¿å†…å®¹æ˜¯å­—ç¬¦ä¸²
        parent_id_str = str(row['parent_comment_id']) if str(row['parent_comment_id']) != '0' else post_id_str
        
        try:
            storage.add_comment_to_post(post_id_str, comment_id_str, content_str, parent_id_str)
        except Exception as e:
            print(f"æ’å…¥è¯„è®ºå¤±è´¥: {e}, å¸–å­ID: {post_id_str}, è¯„è®ºID: {comment_id_str}")

    # 2. é€‰æ‹©è®­ç»ƒæ¨¡å‹ç±»å‹å¹¶é…ç½®è®­ç»ƒå™¨
    common_trainer_params = {
        'post_storage': storage,
        'pruning_model_path': "google-bert/bert-base-chinese", # 
        'similarity_threshold': 0.95, # è°ƒæ•´é˜ˆå€¼
        'num_negatives': 8,      # å¢åŠ è´Ÿæ ·æœ¬æ•°é‡
        'batch_size': 2,        # è°ƒæ•´æ‰¹é‡å¤§å°
        'pruning_inference_batch_size': 16, # <--- ä¸ºå‰ªææ¨¡å‹æ¨æ–­è®¾ç½®ä¸€ä¸ªåˆç†çš„æ‰¹å¤§å°
        'base_lr': 5e-6,         # è°ƒæ•´å­¦ä¹ ç‡
        'projection_lr': 5e-5,
        'use_weighted_loss': True,
        'loss_weights': {'dataset1': 1, 'dataset2': 0}, # è°ƒæ•´æƒé‡
        'adaptive_weighting': False, # å¯ç”¨è‡ªé€‚åº”æƒé‡
        'infonce_mode': 'bidirectional', # åŒå‘å¯¹æ¯”
        'projection_head_config': {'hidden_dim': 768, 'output_dim': 384, 'dropout_rate': 0.15}, # è°ƒæ•´æŠ•å½±å¤´
        'min_subtree_size_ds1': 2, 'max_samples_per_post_ds1': None,
        'min_subtree_size_ds2': 100000, 'max_samples_per_subtree_ds2': None,

        # --- æ–°å¢PEFTé…ç½® ---
        'use_peft': True,  # è®¾ç½®ä¸º True æ¥å¯ç”¨ LoRA
        'peft_config': {
            'r': 8,              # LoRAçš„ç§©ï¼Œè¶Šå°å‚æ•°è¶Šå°‘ï¼Œå¸¸ç”¨8, 16, 32
            'lora_alpha': 16,    # LoRAçš„ç¼©æ”¾å› å­ï¼Œé€šå¸¸æ˜¯rçš„ä¸¤å€
            'target_modules': ["query", "key", "value"], # å¯¹æ³¨æ„åŠ›çš„Q,K,Våº”ç”¨
            'lora_dropout': 0.1, # LoRAå±‚çš„dropoutç‡
            'bias': "none",      # "none", "all", "lora_only"
        },
        
        # --- ğŸ¯ æ–°å¢ï¼šæ­£è´Ÿæ ·æœ¬å¯¹æ„é€ ç­–ç•¥é€‰æ‹© ---
        'positive_pair_strategy': 'simcse_dropout',  # å¯é€‰: 'comment_reply', 'simcse_dropout', 'hybrid'
        'simcse_temperature': 0.05,  # SimCSEçš„æ¸©åº¦å‚æ•°
        'simcse_remove_duplicates': True,  # æ–°å¢ï¼šæ˜¯å¦å»é‡ç›¸åŒæ–‡æœ¬
        'hybrid_ratio': 0.5,  # æ··åˆç­–ç•¥ä¸­SimCSE vs è¯„è®ºå›å¤çš„æ¯”ä¾‹ (0.0-1.0)
        # ---------------------------------------
    }

    # æ ¹æ®é€‰æ‹©çš„ç­–ç•¥æ‰“å°ä¿¡æ¯
    strategy = common_trainer_params['positive_pair_strategy']
    print(f"\nğŸ¯ æ­£æ ·æœ¬å¯¹æ„é€ ç­–ç•¥: {strategy}")
    if strategy == 'comment_reply':
        print("   - ä½¿ç”¨è¯„è®º-å›å¤å…³ç³»æ„é€ æ­£æ ·æœ¬å¯¹")
        print("   - åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦è¿‡æ»¤")
    elif strategy == 'simcse_dropout':
        print("   - ä½¿ç”¨SimCSE dropoutç­–ç•¥æ„é€ æ­£æ ·æœ¬å¯¹")
        print("   - å……åˆ†åˆ©ç”¨æ‰€æœ‰æ–‡æœ¬ï¼šçˆ¶è¯„è®º+å­è¯„è®º")
        print("   - åŒä¸€æ–‡æœ¬é€šè¿‡ä¸åŒdropoutç”Ÿæˆæ­£æ ·æœ¬å¯¹")
        print(f"   - æ¸©åº¦å‚æ•°: {common_trainer_params['simcse_temperature']}")
        print(f"   - å»é‡è®¾ç½®: {common_trainer_params['simcse_remove_duplicates']}")
    elif strategy == 'hybrid':
        print("   - ä½¿ç”¨æ··åˆç­–ç•¥æ„é€ æ­£æ ·æœ¬å¯¹")
        print(f"   - SimCSEæ¯”ä¾‹: {common_trainer_params['hybrid_ratio']}")
        print(f"   - è¯„è®ºå›å¤æ¯”ä¾‹: {1 - common_trainer_params['hybrid_ratio']}")
    print()

    # ğŸ¯ é€‰é¡¹ 1: ModelScope æ¨¡å‹
    print("\n--- é…ç½® ModelScope æ¨¡å‹è®­ç»ƒ ---")
    trainer = DynamicContrastiveTrainer(
        training_model_type='modelscope',
        # ä½¿ç”¨å¦ä¸€ä¸ªModelScopeæ¨¡å‹ä½œä¸ºè®­ç»ƒç›®æ ‡
        training_model_identifier_or_path="google-bert/bert-base-chinese",
        **common_trainer_params
    )

    # # ğŸ¯ é€‰é¡¹ 2: è‡ªå®šä¹‰ TextCNN
    # print("\n--- é…ç½® TextCNN è®­ç»ƒ ---")
    # textcnn_specific_config = {
    #     'embedding_dim': 300,       
    #     'num_filters': 128,         
    #     'filter_sizes': [2, 3, 4],  
    #     'model_dropout_rate': 0.1,  
    #     'max_seq_length': 200,      # TextCNNåˆ†è¯å™¨çš„æœ€å¤§åºåˆ—é•¿åº¦
    #     'textcnn_output_dim': 768,  # TextCNNè¾“å‡ºç»´åº¦ (ä¸æŠ•å½±å¤´è¾“å‡ºåŒ¹é…æˆ–ä½œä¸ºå…¶è¾“å…¥)
    #     'min_vocab_freq': 1         # è¯æ±‡è¡¨æœ€å°è¯é¢‘
    # }
    
    # ç¡®ä¿ TextCNN çš„è¾“å‡ºç»´åº¦ä¸æŠ•å½±å¤´çš„è¾“å…¥ç»´åº¦åŒ¹é…
    # common_trainer_params['projection_head_config']['hidden_dim'] å¯ä»¥åŸºäº textcnn_output_dim
    # æˆ–è€… textcnn_output_dim ç›´æ¥ä½œä¸ºæŠ•å½±å¤´çš„è¾“å…¥
    # è¿™é‡Œå‡è®¾ textcnn_output_dim æ˜¯æŠ•å½±å¤´çš„è¾“å…¥ï¼Œæ‰€ä»¥ base_dim ä¼šæ˜¯ textcnn_output_dim

    # trainer = DynamicContrastiveTrainer(
    #     training_model_type='textcnn',
    #     training_model_identifier_or_path="model/my_custom_textcnn_v4_no_pruning_paircl", # è‡ªå®šä¹‰æ¨¡å‹æ ‡è¯†ç¬¦
    #     textcnn_config=textcnn_specific_config,
    #     **common_trainer_params
    # )
    
    # 3. å¼€å§‹è®­ç»ƒ
    print("\n--- å¼€å§‹è®­ç»ƒ ---")
    trainer.train(
        num_epochs=1, # ä¸ºäº†å¿«é€Ÿæµ‹è¯•ï¼Œå‡å°‘äº†epochï¼ŒåŸä¸º100
        rebuild_frequency=2,  # ä¸ºäº†å¿«é€Ÿæµ‹è¯•ï¼Œå‡å°‘äº†é¢‘ç‡ï¼ŒåŸä¸º200
        scheduler_patience=7, # åŸä¸º2
        min_improvement=1e-5
    )
    
    print("ğŸ‰ è®­ç»ƒæµç¨‹å®Œæˆ!")
    
    # æ˜¾ç¤ºå®éªŒä¿å­˜ä¿¡æ¯
    model_name = trainer.training_model_identifier_or_path.replace('/', '_').replace('-', '_')
    strategy_name = trainer.positive_pair_strategy
    if trainer.positive_pair_strategy == 'hybrid':
        strategy_name = f"hybrid_ratio{trainer.hybrid_ratio}"
    similarity_str = str(trainer.similarity_threshold).replace('.', 'p')
    experiment_folder = f"{model_name}_{strategy_name}_sim{similarity_str}"
    
    print(f"ğŸ’¾ å®éªŒç»“æœå·²ä¿å­˜åˆ°: model/{experiment_folder}/")
    print(f"ğŸ“Š å®éªŒé…ç½®:")
    print(f"   - æ¨¡å‹: {trainer.training_model_identifier_or_path}")
    print(f"   - ç­–ç•¥: {trainer.positive_pair_strategy}")
    print(f"   - ç›¸ä¼¼åº¦é˜ˆå€¼: {trainer.similarity_threshold}")
    print(f"   - æ–‡ä»¶å¤¹: {experiment_folder}")
    print(f"ğŸ”§ åŸºç¡€æ¨¡å‹ä½äº: model/{experiment_folder}/trained_{trainer.training_model_type}_embedding_model/")


if __name__ == "__main__":
    main_training_pipeline()